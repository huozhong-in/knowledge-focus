# å†…ç½®è§†è§‰æ¨¡å‹(MLX-VLM)å®ç°è¿›åº¦

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

ä¸º Knowledge Focus æ·»åŠ å†…ç½®çš„ MLX-VLM è§†è§‰æ¨¡å‹æ”¯æŒï¼Œä½¿ç”¨ Apple MLX æ¡†æ¶åœ¨æœ¬åœ°è¿è¡Œå°å‹è§†è§‰è¯­è¨€æ¨¡å‹ï¼Œæ— éœ€ä¾èµ– ollama/lm-studio ç­‰å¤–éƒ¨å·¥å…·ï¼Œ**çœŸæ­£å®ç°"å¼€ç®±å³ç”¨"çš„éšç§ä¿æŠ¤ä½“éªŒ**ã€‚

**ç›®æ ‡æ¨¡å‹**: Qwen3-VL-4B-Instruct-3bit (2.6GB)  
**è¿è¡Œæ–¹å¼**: é›†æˆåˆ°ä¸» FastAPI æœåŠ¡å™¨  
**æ¥å£æ ‡å‡†**: OpenAI Compatible API (`/v1/chat/completions`)  
**åº”ç”¨åœºæ™¯**: å››ç§æ ¸å¿ƒèƒ½åŠ›ï¼ˆVISION/TEXT/STRUCTURED_OUTPUT/TOOL_USEï¼‰  
**äº§å“å®šä½**: å¼ºéšç§ä¿æŠ¤ï¼Œä¸ä¸‹è½½æˆåŠŸä¸å…è®¸è¿›å…¥App

---

## ğŸ¯ æ ¸å¿ƒè®¾è®¡å†³ç­–ï¼ˆ2025-10-18 æ›´æ–°ï¼‰

### 1. æ¶æ„è®¾è®¡

- âœ… **å•è¿›ç¨‹æ¶æ„**: MLX-VLM é›†æˆåˆ°ä¸» FastAPI è¿›ç¨‹ï¼Œé€šè¿‡ `/v1/chat/completions` ç«¯ç‚¹æä¾›æœåŠ¡
- âœ… **OpenAI å…¼å®¹**: å®Œå…¨å…¼å®¹ OpenAI Chat Completion API æ ¼å¼ï¼ˆæ”¯æŒ streamingï¼‰
- âœ… **æŒ‰éœ€åŠ è½½**: é¦–æ¬¡è¯·æ±‚æ—¶è‡ªåŠ¨åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨ `asyncio.Lock` é˜²æ­¢å¹¶å‘åŠ è½½
- âœ… **ä¼˜å…ˆçº§é˜Ÿåˆ—**: å®ç° `asyncio.PriorityQueue`ï¼Œç¡®ä¿ç”¨æˆ·ä¼šè¯è¯·æ±‚ä¼˜å…ˆäºæ‰¹é‡ä»»åŠ¡
- âœ… **æ™ºèƒ½å¸è½½**: å½“å››é¡¹èƒ½åŠ›å…¨éƒ¨åˆ‡æ¢åˆ°å…¶ä»–æ¨¡å‹æ—¶ï¼Œè‡ªåŠ¨å¸è½½é‡Šæ”¾å†…å­˜

### 2. æ•°æ®åº“è®¾è®¡

- âœ… **Provider è®°å½•**: å·²åœ¨ `db_mgr.py:643-652` é¢„ç½® `[Builtin]` provider
  - `provider_type`: "openai"
  - `source_type`: "builtin"
  - `base_url`: "http://127.0.0.1:60315/v1"  ï¼ˆæ³¨ï¼šä¸ä¸»APIå…±äº«ç«¯å£ï¼‰
- âœ… **Model Configuration**: å·²åœ¨ `db_mgr.py:782-792` é¢„ç½®æ¨¡å‹é…ç½®
  - `model_identifier`: "mlx-community/Qwen3-VL-4B-Instruct-3bit"
  - `display_name`: "Qwen3-VL 4B (3-bit)"
  - `capabilities_json`: ["vision", "text", "structured_output", "tool_use"]
- âœ… **èƒ½åŠ›ç»‘å®š**: å·²åœ¨ `db_mgr.py:800-820` åˆå§‹åŒ–æ—¶è‡ªåŠ¨ç»‘å®šå››é¡¹èƒ½åŠ›
  - `CapabilityAssignment` è¡¨ä¸­é¢„ç½®å››æ¡è®°å½•
  - ç”¨æˆ·åç»­å¯æ‰‹åŠ¨åˆ‡æ¢åˆ°å…¶ä»–æ¨¡å‹

### 3. å¯åŠ¨æµç¨‹ï¼ˆä¼˜åŒ–è®¾è®¡ 2025-10-20ï¼‰â­

**æ ¸å¿ƒæ”¹è¿›**ï¼š
- âœ… **å¹¶è¡Œå¯åŠ¨**: uv ç¯å¢ƒåˆå§‹åŒ–ä¸ Splash ç•Œé¢åŒæ—¶è¿›è¡Œï¼Œæ— éœ€ç­‰å¾…
- âœ… **æƒé™å»¶å**: å°†æƒé™æ£€æŸ¥ç§»åˆ°æ¨¡å‹ä¸‹è½½åï¼Œé¿å…é‡å¯ä¸­æ–­ uv
- âœ… **ç®€æ´äº¤äº’**: æ­£å¸¸æµç¨‹åªæ˜¾ç¤ºé˜¶æ®µæç¤ºï¼Œå¼‚å¸¸æ—¶æ™ºèƒ½å±•å¼€è¯¦ç»†æ—¥å¿—
- âœ… **æ™ºèƒ½å®¹é”™**: è¶…æ—¶æ£€æµ‹ + é•œåƒåˆ‡æ¢å»ºè®® + æ¸…æ™°é”™è¯¯æŒ‡å¼•

**ä¼˜åŒ–åçš„å¯åŠ¨æµç¨‹**ï¼š
```
App å¯åŠ¨
  â†“ (å¹¶è¡Œè¿›è¡Œ)
  â”œâ”€ Tauri sidecar å¯åŠ¨ uv sync (30-90sï¼Œé¦–æ¬¡éœ€ä¸‹è½½ä¾èµ–)
  â””â”€ æ˜¾ç¤º Splash ç•Œé¢
  â†“
[é˜¶æ®µ1] Python ç¯å¢ƒåˆå§‹åŒ–
  æ˜¾ç¤º: "Initializing Python environment..."
  è¶…æ—¶: 30s â†’ æ˜¾ç¤ºæ—¥å¿— + ç¯å¢ƒå˜é‡é‡å¯æç¤º
  â†“ [uv sync å®Œæˆ]
[é˜¶æ®µ2] API æœåŠ¡å™¨å¯åŠ¨
  æ˜¾ç¤º: "Starting API server..."
  è¶…æ—¶: 90s (é¦–æ¬¡å¯åŠ¨éœ€ç¼–è¯‘ __pycache__)
  â†“ [FastAPI å°±ç»ª]
[é˜¶æ®µ3] å†…ç½®æ¨¡å‹æ£€æŸ¥ä¸ä¸‹è½½
  3a) æ£€æŸ¥: "Checking builtin model..."
  3b) å·²ä¸‹è½½ â†’ è·³åˆ°é˜¶æ®µ4
  3c) ä¸‹è½½ä¸­: è¿›åº¦æ¡ (0-100%) + é•œåƒé€‰æ‹©å™¨(ä»…erroræ˜¾ç¤º)
  3d) å¤±è´¥: æ˜¾ç¤ºé”™è¯¯ + é•œåƒåˆ‡æ¢ + é‡è¯•æŒ‰é’®
  â†“ [ä¸‹è½½æˆåŠŸ]
[é˜¶æ®µ4] ç£ç›˜è®¿é—®æƒé™æ£€æŸ¥ (æ–°ä½ç½®ï¼)
  æ˜¾ç¤º: "Checking disk access permission..."
  å¤±è´¥: æ˜¾ç¤ºè¯·æ±‚æƒé™æŒ‰é’® + é‡å¯æç¤º
  â†“ [æƒé™é€šè¿‡]
[é˜¶æ®µ5] åç«¯æ–‡ä»¶æ‰«æå¯åŠ¨
  æ˜¾ç¤º: "Starting file scanning..."
  è°ƒç”¨: start_backend_scanning()
  â†“
è¿›å…¥ä¸»ç•Œé¢ âœ¨
```

**å…³é”®è®¾è®¡å†³ç­–**ï¼š
1. **æƒé™å»¶åçš„åˆç†æ€§**ï¼š
   - å®Œå…¨ç£ç›˜è®¿é—®æƒé™åªå½±å“æ–‡ä»¶æ‰«æåŠŸèƒ½
   - ä¸å½±å“ï¼šPython ç¯å¢ƒã€API å¯åŠ¨ã€æ¨¡å‹ä¸‹è½½
   - å»¶åé¿å…é‡å¯æ—¶ä¸­æ–­ uvï¼Œæå‡ç¨³å®šæ€§

2. **è¶…æ—¶æ—¶é—´è®¾ç½®**ï¼š
   - uv sync: 30s (æ­£å¸¸) / 90s (ç½‘ç»œæ…¢)
   - API å¯åŠ¨: 90s (é¦–æ¬¡å¯åŠ¨éœ€ç¼–è¯‘ Python å­—èŠ‚ç )
   - æ¨¡å‹ä¸‹è½½: æ— å›ºå®šè¶…æ—¶ (å¤§æ–‡ä»¶ï¼Œæ˜¾ç¤ºè¿›åº¦å³å¯)

3. **æ—¥å¿—æ˜¾ç¤ºç­–ç•¥**ï¼š
   - é»˜è®¤ï¼šåªæ˜¾ç¤ºé˜¶æ®µæ€§æç¤º + loading åŠ¨ç”»
   - è¶…æ—¶ï¼šè‡ªåŠ¨å±•å¼€è¯¦ç»†æ—¥å¿— + è§£å†³æ–¹æ¡ˆ
   - å¯é€‰ï¼š"æŸ¥çœ‹è¯¦ç»†æ—¥å¿—"æŒ‰é’®ï¼ˆæŠ˜å /å±•å¼€ï¼‰

### 4. æ¨¡å‹ç”Ÿå‘½å‘¨æœŸç®¡ç†

#### 4.1 åŠ è½½ç­–ç•¥ï¼ˆLazy Loadingï¼‰
- **è§¦å‘æ—¶æœº**: é¦–æ¬¡æ”¶åˆ° `/v1/chat/completions` è¯·æ±‚æ—¶
- **åŠ è½½ä½ç½®**: `MLXVLMModelManager.ensure_loaded()`
- **å¹¶å‘ä¿æŠ¤**: ä½¿ç”¨ `asyncio.Lock` ç¡®ä¿åªåŠ è½½ä¸€æ¬¡
- **åŠ è½½æµç¨‹**:
  ```python
  async with self._lock:
      if model already loaded:
          return
      model, processor = load(model_path, trust_remote_code=True)
      self._model_cache = {"model": model, "processor": processor, ...}
      start queue processor
  ```

#### 4.2 å¸è½½ç­–ç•¥ï¼ˆSmart Unloadingï¼‰
- **è§¦å‘æ—¶æœº**: ç”¨æˆ·åœ¨åœºæ™¯é…ç½®ä¸­åˆ‡æ¢èƒ½åŠ›ç»‘å®šå
- **æ£€æŸ¥é€»è¾‘**: 
  1. æŸ¥è¯¢ `CapabilityAssignment` è¡¨
  2. æ£€æŸ¥ VISION/TEXT/STRUCTURED_OUTPUT/TOOL_USE å››é¡¹èƒ½åŠ›
  3. å¦‚æœ**å…¨éƒ¨å››é¡¹**éƒ½ä¸å†ç»‘å®šåˆ°å†…ç½®æ¨¡å‹ â†’ å¸è½½
- **å¸è½½æ“ä½œ**:
  ```python
  self._model_cache.clear()
  gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
  ```

### 5. è¯·æ±‚ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼ˆæ–°å¢ï¼‰â­

**è®¾è®¡ç›®æ ‡**: é˜²æ­¢æ‰¹é‡æ‰“æ ‡ç­¾ä»»åŠ¡é˜»å¡ç”¨æˆ·ä¼šè¯

#### 5.1 ä¼˜å…ˆçº§å®šä¹‰
```python
class RequestPriority(IntEnum):
    HIGH = 1    # ä¼šè¯ç•Œé¢è¯·æ±‚ï¼ˆç”¨æˆ·ä¸»åŠ¨å‘èµ·ï¼‰
    LOW = 10    # æ‰¹é‡ä»»åŠ¡è¯·æ±‚ï¼ˆåå°è‡ªåŠ¨ï¼‰
```

#### 5.2 é˜Ÿåˆ—å®ç°
- **é˜Ÿåˆ—ç±»å‹**: `asyncio.PriorityQueue`
- **å…¥é˜Ÿæ–¹æ³•**: `enqueue_request(request, model_path, priority)`
- **å¤„ç†å™¨**: åå°ä»»åŠ¡å¾ªç¯å¤„ç†é˜Ÿåˆ—ï¼Œä¼˜å…ˆå¤„ç† HIGH ä¼˜å…ˆçº§è¯·æ±‚
- **è¶…æ—¶ç­–ç•¥**: é˜Ÿåˆ—ç©ºé—² 60 ç§’åè‡ªåŠ¨åœæ­¢å¤„ç†å™¨ï¼ˆèŠ‚çœèµ„æºï¼‰

#### 5.3 API é›†æˆ
```python
@router.post("/v1/chat/completions")
async def openai_chat_completions(
    request: dict,
    priority: int = Query(default=10)  # 1=HIGH, 10=LOW
):
    response = await manager.enqueue_request(
        openai_request, 
        model_path,
        RequestPriority(priority)
    )
    return response
```

### 6. ä¸‹è½½æœºåˆ¶

#### 6.1 å¤šé•œåƒæ”¯æŒ
- **é•œåƒåˆ—è¡¨**:
  - `https://huggingface.co` (å…¨çƒ)
  - `https://hf-mirror.com` (ä¸­å›½é•œåƒ)
- **ç”¨æˆ·é€‰æ‹©**: Splash é¡µé¢æä¾›ä¸‹æ‹‰é€‰æ‹©
- **è‡ªåŠ¨é‡è¯•**: å•ä¸ªé•œåƒå¤±è´¥åä¸è‡ªåŠ¨åˆ‡æ¢ï¼Œç”±ç”¨æˆ·æ‰‹åŠ¨é€‰æ‹©å¹¶é‡è¯•

#### 6.2 è¿›åº¦æ¨é€ï¼ˆBridge Eventsï¼‰
- **äº‹ä»¶åç§°**: `builtin-model-download-progress`
- **Payload æ ¼å¼**:
  ```json
  {
    "model_id": "qwen3-vl-4b",
    "progress": 45,           // 0-100
    "status": "downloading",  // downloading | completed | failed
    "message": "Downloading... 1.2GB / 2.6GB",
    "speed_mbps": 5.2         // å¯é€‰
  }
  ```
- **èŠ‚æµç­–ç•¥**: æ¯ç§’æœ€å¤šæ¨é€ 1 æ¬¡è¿›åº¦äº‹ä»¶

#### 6.3 æ–­ç‚¹ç»­ä¼ 
- **åŸç”Ÿæ”¯æŒ**: `huggingface_hub.snapshot_download()` è‡ªå¸¦æ–­ç‚¹ç»­ä¼ 
- **ç¼“å­˜ä½ç½®**: `{base_dir}/builtin_models/models--mlx-community--Qwen3-VL-4B-Instruct-3bit/`

### 7. ç®€åŒ–çš„æ¶æ„ï¼ˆç›¸æ¯”åŸæ–¹æ¡ˆï¼‰

**å·²åˆ é™¤çš„å¤æ‚é€»è¾‘**:
- âŒ MLX Server å­è¿›ç¨‹ç®¡ç†ï¼ˆç«¯å£ 60316ï¼‰
- âŒ æœåŠ¡å™¨å¯åŠ¨/åœæ­¢/å¥åº·æ£€æŸ¥ API
- âŒ æ¨¡å‹é…ç½®é¡µçš„ Builtin Tab
- âŒ useBuiltinModels Hook å’Œä¸‹è½½ç®¡ç† UI
- âŒ ç®€å•çš„ refcount å¸è½½é€»è¾‘
- âŒ "è·³è¿‡ä¸‹è½½" é™çº§é€‰é¡¹
- âŒ Pin/Unpin UI

**ä¿ç•™çš„æ ¸å¿ƒåŠŸèƒ½**:
- âœ… `/v1/chat/completions` OpenAI å…¼å®¹ç«¯ç‚¹
- âœ… MLXVLMModelManager å•ä¾‹æ¨¡å¼
- âœ… ä¸‹è½½è¿›åº¦ Bridge Events
- âœ… æ•°æ®åº“èƒ½åŠ›ç»‘å®š

---

## ğŸ“ æŠ€æœ¯å®ç°ç»†èŠ‚

### 1. æ–‡ä»¶ç»“æ„

```
api/
â”œâ”€â”€ builtin_openai_compat.py       # OpenAI å…¼å®¹å±‚ + ä¼˜å…ˆçº§é˜Ÿåˆ—
â”‚   â”œâ”€â”€ MLXVLMModelManager         # æ¨¡å‹ç®¡ç†ï¼ˆå•ä¾‹ï¼‰
â”‚   â”‚   â”œâ”€â”€ ensure_loaded()        # æŒ‰éœ€åŠ è½½ + å¹¶å‘ä¿æŠ¤
â”‚   â”‚   â”œâ”€â”€ unload_model()         # å¸è½½æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ check_and_unload_if_unused()  # æ™ºèƒ½å¸è½½æ£€æŸ¥
â”‚   â”‚   â”œâ”€â”€ enqueue_request()      # å…¥é˜Ÿè¯·æ±‚
â”‚   â”‚   â””â”€â”€ _process_queue()       # é˜Ÿåˆ—å¤„ç†å™¨
â”‚   â”œâ”€â”€ RequestPriority            # ä¼˜å…ˆçº§æšä¸¾
â”‚   â””â”€â”€ OpenAI æ•°æ®æ¨¡å‹
â”œâ”€â”€ models_builtin.py              # æ¨¡å‹ä¸‹è½½ç®¡ç†
â”‚   â”œâ”€â”€ download_model_with_events()  # å¼‚æ­¥ä¸‹è½½ + äº‹ä»¶æ¨é€
â”‚   â”œâ”€â”€ is_model_downloaded()      # æ£€æŸ¥ä¸‹è½½çŠ¶æ€
â”‚   â””â”€â”€ get_model_path()           # è·å–æ¨¡å‹è·¯å¾„
â””â”€â”€ models_api.py                  # API è·¯ç”±
    â”œâ”€â”€ POST /models/builtin/initialize      # Splash è°ƒç”¨
    â”œâ”€â”€ GET  /models/builtin/download-status # çŠ¶æ€æŸ¥è¯¢
    â””â”€â”€ POST /v1/chat/completions            # OpenAI å…¼å®¹ç«¯ç‚¹

tauri-app/src/
â””â”€â”€ splash.tsx                     # å¯åŠ¨é¡µ + æ¨¡å‹ä¸‹è½½ UI
    â”œâ”€â”€ modelStage: checking/downloading/ready/error
    â”œâ”€â”€ è¿›åº¦æ¡ç»„ä»¶
    â”œâ”€â”€ é•œåƒåˆ‡æ¢ä¸‹æ‹‰æ¡†
    â””â”€â”€ é‡è¯•æŒ‰é’®
```

### 2. å…³é”®ä»£ç ç‰‡æ®µ

#### 2.1 Splash é¡µé¢çŠ¶æ€æœº
```tsx
type ModelStage = 'checking' | 'downloading' | 'ready' | 'error';

// çŠ¶æ€è½¬æ¢:
// checking â†’ downloading â†’ ready â†’ è¿›å…¥ä¸»ç•Œé¢
//         â†“               â†“
//         error â† â”€ â”€ â”€ â”€ â”˜
//           â†“ [é‡è¯•]
//         checking
```

#### 2.2 ä¼˜å…ˆçº§é˜Ÿåˆ—å¤„ç†
```python
# é«˜ä¼˜å…ˆçº§è¯·æ±‚ï¼ˆä¼šè¯ï¼‰
await manager.enqueue_request(req, path, RequestPriority.HIGH)

# ä½ä¼˜å…ˆçº§è¯·æ±‚ï¼ˆæ‰¹é‡ï¼‰
await manager.enqueue_request(req, path, RequestPriority.LOW)

# é˜Ÿåˆ—è‡ªåŠ¨æŒ‰ä¼˜å…ˆçº§æ’åºï¼ŒHIGH å…ˆå¤„ç†
```

#### 2.3 æ™ºèƒ½å¸è½½æ£€æŸ¥
```python
# åœ¨åœºæ™¯é…ç½® API ä¸­è°ƒç”¨
@router.post("/models/capabilities/{capability}/assign")
async def assign_capability_to_model(...):
    # æ›´æ–°ç»‘å®š
    update_assignment(...)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦å¸è½½
    vlm_manager = get_vlm_manager()
    await vlm_manager.check_and_unload_if_unused(engine)
```

---

## ğŸ“ å®æ–½è®¡åˆ’

### Phase 1: åç«¯æ ¸å¿ƒåŠŸèƒ½ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰

#### Task 2.1: Bridge Events é›†æˆ
- [x] ä¿®æ”¹ `models_builtin.py`
  - æ–°å¢ `download_model_with_events()` å¼‚æ­¥æ–¹æ³•
  - é›†æˆ `bridge_events.push_bridge_event()`
  - æ”¯æŒé•œåƒå‚æ•° (`mirror: str`)
- [x] æ–°å¢ API ç«¯ç‚¹ï¼ˆ`models_api.py`ï¼‰
  - `POST /models/builtin/initialize`
  - `GET /models/builtin/download-status`

#### Task 2.2: æŒ‰éœ€åŠ è½½ä¸å¹¶å‘ä¿æŠ¤
- [x] ä¿®æ”¹ `builtin_openai_compat.py`
  - åœ¨ `MLXVLMModelManager` ä¸­æ·»åŠ  `asyncio.Lock`
  - å®ç° `ensure_loaded()` æ–¹æ³•
  - åœ¨ `/v1/chat/completions` è¯·æ±‚å…¥å£è°ƒç”¨

#### Task 2.3: ä¼˜å…ˆçº§é˜Ÿåˆ—
- [x] ä¿®æ”¹ `builtin_openai_compat.py`
  - æ·»åŠ  `RequestPriority` æšä¸¾
  - å®ç° `asyncio.PriorityQueue`
  - å®ç° `enqueue_request()` å’Œ `_process_queue()`
- [x] ä¿®æ”¹ `/v1/chat/completions` API
  - æ·»åŠ  `priority` æŸ¥è¯¢å‚æ•°
  - æ”¹ä¸ºè°ƒç”¨ `enqueue_request()`

#### Task 2.4: æ™ºèƒ½å¸è½½
- [ ] ä¿®æ”¹ `builtin_openai_compat.py`
  - å®ç° `check_and_unload_if_unused()`
  - å®ç° `unload_model()`
- [ ] ä¿®æ”¹åœºæ™¯é…ç½® API
  - åœ¨èƒ½åŠ›ç»‘å®šå˜æ›´åè°ƒç”¨å¸è½½æ£€æŸ¥

### Phase 2: å‰ç«¯é›†æˆ

#### Task 1.1: Splash é¡µé¢æ”¹é€ ï¼ˆä¼˜åŒ–ç‰ˆ 2025-10-20ï¼‰

**Phase 1: å¿«é€Ÿæ”¹è¿›ï¼ˆå·²å®ŒæˆåŸºç¡€å®ç°ï¼Œå¾…ä¼˜åŒ–ï¼‰** âš¡
- [x] æ·»åŠ æ¨¡å‹ä¸‹è½½ç›¸å…³çŠ¶æ€ç®¡ç†
  - `modelStage`: checking/downloading/ready/error
  - `downloadProgress`: 0-100
  - `downloadMessage`: ä¸‹è½½è¯¦æƒ…
  - `selectedMirror`: huggingface/hf-mirror
- [x] é›†æˆ bridge events ç›‘å¬
  - `model-download-progress`: æ›´æ–°è¿›åº¦
  - `model-download-completed`: è®¾ç½® ready
  - `model-download-failed`: æ˜¾ç¤ºé”™è¯¯
- [x] æ·»åŠ ä¸‹è½½è¿›åº¦ UI ç»„ä»¶
  - è¿›åº¦æ¡ï¼ˆè“è‰²ï¼Œç™¾åˆ†æ¯”æ˜¾ç¤ºï¼‰
  - é”™è¯¯é¢æ¿ï¼ˆçº¢è‰²ï¼Œé”™è¯¯ä¿¡æ¯ï¼‰
  - é‡è¯•æŒ‰é’®

**Phase 2: ä½“éªŒä¼˜åŒ–ï¼ˆè¿›è¡Œä¸­ï¼‰** âœ¨
- [ ] **è°ƒæ•´æƒé™æ£€æŸ¥ä½ç½®**ï¼ˆ30åˆ†é’Ÿï¼‰
  - å°†æƒé™æ£€æŸ¥ä»åˆå§‹åŒ–ç§»åˆ°æ¨¡å‹ä¸‹è½½æˆåŠŸå
  - ä¿®æ”¹ useEffect ä¾èµ–é¡ºåº
  - ç¡®ä¿ä¸å½±å“ uv å¯åŠ¨æµç¨‹
  
- [ ] **ä¼˜åŒ–é•œåƒé€‰æ‹©å™¨æ˜¾ç¤º**ï¼ˆ10åˆ†é’Ÿï¼‰
  - ä¸‹è½½ä¸­ï¼ˆdownloadingï¼‰éšè—é•œåƒé€‰æ‹©å™¨
  - ä»…åœ¨ error çŠ¶æ€æ˜¾ç¤º
  - æ·»åŠ ç¦ç”¨çŠ¶æ€æç¤º
  
- [ ] **æ·»åŠ æ—¥å¿—æŠ˜å åŠŸèƒ½**ï¼ˆ30åˆ†é’Ÿï¼‰
  - æ–°å¢ `showDetailedLogs` çŠ¶æ€
  - æ·»åŠ "æŸ¥çœ‹è¯¦ç»†æ—¥å¿—"æŒ‰é’®
  - é»˜è®¤æŠ˜å ï¼Œè¶…æ—¶æˆ–é”™è¯¯æ—¶è‡ªåŠ¨å±•å¼€
  
- [ ] **ç®€åŒ–é˜¶æ®µæç¤ºä¿¡æ¯**ï¼ˆ20åˆ†é’Ÿï¼‰
  - å®šä¹‰æ¸…æ™°çš„é˜¶æ®µæ¶ˆæ¯æ˜ å°„
  - ç§»é™¤æŠ€æœ¯æœ¯è¯­ï¼Œä½¿ç”¨ç”¨æˆ·å‹å¥½çš„æ–‡æ¡ˆ
  - å„é˜¶æ®µæç¤ºï¼š
    - "Initializing Python environment..."
    - "Starting API server..."
    - "Checking builtin model..."
    - "Downloading model... (XX%)"
    - "Checking disk access permission..."
    - "Starting file scanning..."

- [ ] **æ·»åŠ è¶…æ—¶æ£€æµ‹**ï¼ˆå¯é€‰ï¼Œ1-2å°æ—¶ï¼‰
  - uv è¶…æ—¶æ£€æµ‹ï¼ˆ30sï¼‰
  - API å¯åŠ¨è¶…æ—¶æ£€æµ‹ï¼ˆ90sï¼‰
  - è¶…æ—¶åæ˜¾ç¤ºç¯å¢ƒå˜é‡é‡å¯æç¤º
  - è¶…æ—¶åè‡ªåŠ¨å±•å¼€è¯¦ç»†æ—¥å¿—

**Phase 3: é”™è¯¯å¤„ç†å¢å¼º**ï¼ˆå¯é€‰ï¼‰ ğŸ¯
- [ ] æ·»åŠ ç½‘ç»œè¿æ¥æ£€æµ‹
- [ ] ä¼˜åŒ–é”™è¯¯æç¤ºæ–‡æ¡ˆï¼ˆä¸­è‹±æ–‡ï¼‰
- [ ] æ·»åŠ å¸¸è§é—®é¢˜é“¾æ¥

**å®ç°ç»†èŠ‚**:
- ä½¿ç”¨ `listen()` ç›‘å¬ä¸‰ä¸ª bridge events
- åœ¨ API å°±ç»ªåè°ƒç”¨ `/models/builtin/initialize`
- æ ¹æ®è¿”å›çš„ status ('ready'/'downloading'/'error') è®¾ç½® modelStage
- downloading æ—¶æ˜¾ç¤ºè¿›åº¦æ¡ï¼ˆä¸‹è½½ä¸­éšè—é•œåƒé€‰æ‹©å™¨ï¼‰
- error æ—¶æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯ã€é•œåƒåˆ‡æ¢ã€é‡è¯•æŒ‰é’®
- ready åæ£€æŸ¥æƒé™ï¼Œæƒé™é€šè¿‡åå¯åŠ¨åç«¯æ‰«æ

### Phase 3: ä»£ç æ¸…ç†

#### Task 6.1: åˆ é™¤åºŸå¼ƒä»£ç 
- [x] `models_builtin.py`: åˆ é™¤å­è¿›ç¨‹ç®¡ç†ä»£ç 
- [x] `models_api.py`: åˆ é™¤æ—§çš„ builtin ç®¡ç†ç«¯ç‚¹
- [ ] `settings-ai-models.tsx`: åˆ é™¤ `useBuiltinModels` å’Œ `BuiltinModelsTab`

### Phase 4: æµ‹è¯•ä¸éªŒè¯

#### Task 7.1: ç«¯åˆ°ç«¯æµ‹è¯•
- [ ] å…¨æ–°å®‰è£…æµ‹è¯•ï¼ˆåˆ é™¤ DB + æ¨¡å‹æ–‡ä»¶ï¼‰
- [ ] ä¸‹è½½å¤±è´¥ + é•œåƒåˆ‡æ¢æµ‹è¯•
- [ ] ä¼˜å…ˆçº§é˜Ÿåˆ—æµ‹è¯•ï¼ˆå¹¶å‘ä¼šè¯ + æ‰¹é‡ä»»åŠ¡ï¼‰
- [ ] æ™ºèƒ½å¸è½½æµ‹è¯•ï¼ˆåˆ‡æ¢å››é¡¹èƒ½åŠ›ï¼‰

---

## ğŸ” æ•…éšœæ’æŸ¥æŒ‡å—

### é—®é¢˜ 1: ä¸‹è½½å¡åœ¨ 0% ä¸åŠ¨

**å¯èƒ½åŸå› **:
- ç½‘ç»œè¿æ¥é—®é¢˜
- é•œåƒç«™ç‚¹ä¸å¯è®¿é—®
- huggingface_hub ä¾èµ–æœªå®‰è£…

**æ’æŸ¥æ­¥éª¤**:
1. æ£€æŸ¥ API æ—¥å¿—: `~/Library/Application Support/knowledge-focus.huozhong.in/logs/*.log`
2. æœç´¢å…³é”®å­—: "download" æˆ– "builtin-model"
3. å°è¯•åˆ‡æ¢é•œåƒç«™ç‚¹
4. æ£€æŸ¥ç»ˆç«¯èƒ½å¦è®¿é—®: `curl -I https://huggingface.co`

### é—®é¢˜ 2: ä¸‹è½½å®Œæˆä½†æ— æ³•è¿›å…¥ä¸»ç•Œé¢

**å¯èƒ½åŸå› **:
- æ¨¡å‹æ–‡ä»¶æŸå
- ç¼“å­˜è®°å½•ä¸ä¸€è‡´

**è§£å†³æ–¹æ¡ˆ**:
```bash
# åˆ é™¤æ¨¡å‹å’Œç¼“å­˜
rm -rf ~/Library/Application\ Support/knowledge-focus.huozhong.in/builtin_models/

# é‡å¯ Appï¼Œé‡æ–°ä¸‹è½½
```

### é—®é¢˜ 3: æ¨ç†è¯·æ±‚è¶…æ—¶æˆ–æ— å“åº”

**å¯èƒ½åŸå› **:
- æ¨¡å‹æœªåŠ è½½
- é˜Ÿåˆ—å¤„ç†å™¨æœªå¯åŠ¨
- å†…å­˜ä¸è¶³

**æ’æŸ¥æ­¥éª¤**:
1. æ£€æŸ¥æ—¥å¿—ä¸­æ˜¯å¦æœ‰ "Loading model" æˆ– "Model loaded"
2. æ£€æŸ¥å†…å­˜å ç”¨: `Activity Monitor` â†’ æœç´¢ "Knowledge Focus"
3. æ£€æŸ¥é˜Ÿåˆ—çŠ¶æ€: æ—¥å¿—ä¸­æœç´¢ "Processing request with priority"

### é—®é¢˜ 4: ä¼šè¯è¯·æ±‚ä»ç„¶è¢«æ‰¹é‡ä»»åŠ¡é˜»å¡

**å¯èƒ½åŸå› **:
- å‰ç«¯æœªä¼ é€’ `priority=1` å‚æ•°
- é˜Ÿåˆ—æœªæ­£ç¡®å®ç°ä¼˜å…ˆçº§æ’åº

**éªŒè¯æ–¹æ³•**:
```bash
# æµ‹è¯•é«˜ä¼˜å…ˆçº§è¯·æ±‚
curl -X POST http://127.0.0.1:60315/v1/chat/completions?priority=1 \
  -H "Content-Type: application/json" \
  -d '{"model": "qwen3-vl-4b", "messages": [...]}'
```

---

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

### ç›®æ ‡æŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | è¯´æ˜ |
|------|--------|------|
| æ¨¡å‹åŠ è½½æ—¶é—´ | < 10 ç§’ | é¦–æ¬¡åŠ è½½è€—æ—¶ |
| å•æ¬¡æ¨ç†å»¶è¿Ÿ | < 3 ç§’ | çº¯æ–‡æœ¬å¯¹è¯ |
| å›¾ç‰‡æ¨ç†å»¶è¿Ÿ | < 5 ç§’ | å•å›¾é—®ç­” |
| å†…å­˜å ç”¨ | < 3 GB | æ¨¡å‹åŠ è½½å |
| é˜Ÿåˆ—å¤„ç†å»¶è¿Ÿ | < 100 ms | é«˜ä¼˜å…ˆçº§è¯·æ±‚æ’é˜Ÿæ—¶é—´ |

### ç›‘æ§æ–¹æ³•

```python
# åœ¨æ—¥å¿—ä¸­è®°å½•å…³é”®æŒ‡æ ‡
logger.info(f"Model loaded in {duration:.2f}s")
logger.info(f"Request processed in {duration:.2f}s, priority={priority}")
logger.info(f"Queue size: {queue.qsize()}")
```

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [PRD.md](./PRD.md) - äº§å“éœ€æ±‚æ–‡æ¡£
- [mlx-vlm GitHub](https://github.com/Blaizzy/mlx-vlm) - MLX-VLM å®˜æ–¹æ–‡æ¡£
- [db_mgr.py](../api/db_mgr.py) - æ•°æ®åº“æ¨¡å‹å®šä¹‰
- [models_api.py](../api/models_api.py) - æ¨¡å‹ API è·¯ç”±
- [builtin_openai_compat.py](../api/builtin_openai_compat.py) - OpenAI å…¼å®¹å±‚
- [splash.tsx](../tauri-app/src/splash.tsx) - å¯åŠ¨é¡µé¢

---

## ğŸ“… æ›´æ–°å†å²

- **2025-10-18**: é‡å¤§è®¾è®¡å˜æ›´
  - å°†ä¸‹è½½æµç¨‹ç§»è‡³ Splash é¡µé¢ï¼ˆé˜»å¡å¼ï¼‰
  - åˆ é™¤"è·³è¿‡ä¸‹è½½"é€‰é¡¹ï¼ˆå¼ºåŒ–éšç§ä¿æŠ¤å®šä½ï¼‰
  - æ–°å¢ä¼˜å…ˆçº§é˜Ÿåˆ—æœºåˆ¶
  - ä¼˜åŒ–å¸è½½ç­–ç•¥ï¼ˆåŸºäºå››é¡¹èƒ½åŠ›ç»‘å®šæ£€æŸ¥ï¼‰
  - ç®€åŒ–æ¶æ„ï¼ˆç§»é™¤å­è¿›ç¨‹ç®¡ç†ï¼‰

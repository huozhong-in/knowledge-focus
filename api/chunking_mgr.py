"""
å¤šæ¨¡æ€åˆ†å—ç®¡ç†å™¨ (ChunkingMgr)

è´Ÿè´£ä½¿ç”¨doclingè§£ææ–‡æ¡£ï¼Œå®ç°åˆ†å±‚åˆ†å—ç­–ç•¥ï¼Œç”Ÿæˆçˆ¶å—å’Œå­å—ï¼Œ
å¹¶è°ƒç”¨æ¨¡å‹è¿›è¡Œå›¾ç‰‡æè¿°ç”Ÿæˆå’Œæ–‡æœ¬å‘é‡åŒ–ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä½¿ç”¨doclingè§£æPDF/PPT/DOCXç­‰æ–‡æ¡£
2. åŸºäºdoclingçš„body/groupsç»“æ„å®ç°åˆ†å±‚åˆ†å—
3. è°ƒç”¨visionæ¨¡å‹ç”Ÿæˆå›¾ç‰‡/è¡¨æ ¼æè¿°
4. è°ƒç”¨embeddingæ¨¡å‹è¿›è¡Œå‘é‡åŒ–
5. å­˜å‚¨åˆ°SQLite(å…ƒæ•°æ®)å’ŒLanceDB(å‘é‡)
"""

import os
import json
import hashlib
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from uuid import uuid4

# ç”ŸæˆçŸ­IDçš„å·¥å…·å‡½æ•°ï¼ˆæ›¿ä»£nanoidï¼‰
def generate_vector_id() -> str:
    """ç”Ÿæˆç”¨äºvector_idçš„çŸ­ID"""
    return str(uuid4()).replace('-', '')[:16]

from sqlmodel import Session
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import (
    PictureDescriptionApiOptions,
    PdfPipelineOptions,
    RapidOcrOptions,
)
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling_core.types.doc import (
    DoclingDocument,
    ImageRefMode,
    PictureItem,
    TableItem,
    TextItem,
)
from docling.datamodel.document import ConversionResult
from docling.chunking import HybridChunker
from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer
from transformers import AutoTokenizer

from db_mgr import Document, ParentChunk, ChildChunk
from lancedb_mgr import LanceDBMgr
from models_mgr import ModelsMgr
from model_config_mgr import ModelConfigMgr
from bridge_events import BridgeEventSender

logger = logging.getLogger(__name__)

class ChunkingMgr:
    """å¤šæ¨¡æ€åˆ†å—ç®¡ç†å™¨"""
    
    def __init__(self, session: Session, lancedb_mgr: LanceDBMgr, models_mgr: ModelsMgr):
        """
        åˆå§‹åŒ–åˆ†å—ç®¡ç†å™¨
        
        Args:
            session: SQLModelæ•°æ®åº“ä¼šè¯
            lancedb_mgr: LanceDBå‘é‡æ•°æ®åº“ç®¡ç†å™¨
            models_mgr: æ¨¡å‹ç®¡ç†å™¨ï¼ˆç”¨äºè°ƒç”¨visionå’Œembeddingæ¨¡å‹ï¼‰
        """
        self.session = session
        self.lancedb_mgr = lancedb_mgr
        self.models_mgr = models_mgr
        self.model_config_mgr = ModelConfigMgr(session)
        
        # è·å–æ•°æ®åº“ç›®å½•ä½œä¸ºåŸºç¡€è·¯å¾„
        self._init_base_paths()
        
        # åˆå§‹åŒ–æ¡¥æ¥äº‹ä»¶å‘é€å™¨
        self.bridge_events = BridgeEventSender(source="chunking-manager")
        
        # è·å–å½“å‰è§†è§‰æ¨¡å‹é…ç½®
        self.vision_api_endpoint, self.vision_model_id = self.model_config_mgr.get_vision_model_config()
        
        # è·å–embeddingç»´åº¦é…ç½®
        from config import EMBEDDING_DIMENSIONS
        self.embedding_dimensions = EMBEDDING_DIMENSIONS

        # åˆå§‹åŒ–doclingè½¬æ¢å™¨
        self._init_docling_converter()
        
        # åˆå§‹åŒ–chunker
        self._init_chunker()
        
        logger.info("ChunkingMgr initialized successfully")
    
    def _init_base_paths(self):
        """åˆå§‹åŒ–åŸºç¡€è·¯å¾„ï¼Œä½¿ç”¨æ•°æ®åº“ç›®å½•çš„çˆ¶ç›®å½•"""
        try:
            # å°è¯•ä»LanceDBè·å–è·¯å¾„
            if hasattr(self.lancedb_mgr, 'db') and hasattr(self.lancedb_mgr.db, 'uri'):
                lancedb_path = Path(self.lancedb_mgr.db.uri)
                self.data_base_dir = lancedb_path.parent
            else:
                # ä»SQLiteè¿æ¥è·å–è·¯å¾„
                sqlite_url = str(self.session.bind.url)
                if sqlite_url.startswith('sqlite:///'):
                    sqlite_path = Path(sqlite_url.replace('sqlite:///', ''))
                    self.data_base_dir = sqlite_path.parent
                else:
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨å½“å‰ç›®å½•
                    self.data_base_dir = Path.cwd()
            
            # åˆ›å»ºdoclingç¼“å­˜ç›®å½•
            self.docling_cache_dir = self.data_base_dir / "docling_cache"
            self.docling_cache_dir.mkdir(exist_ok=True)
            
            logger.info(f"Data base directory: {self.data_base_dir}")
            logger.info(f"Docling cache directory: {self.docling_cache_dir}")
            
        except Exception as e:
            logger.error(f"Failed to initialize base paths: {e}")
            # å¤‡ç”¨æ–¹æ¡ˆ
            self.data_base_dir = Path.cwd()
            self.docling_cache_dir = self.data_base_dir / "docling_cache"
            self.docling_cache_dir.mkdir(exist_ok=True)
    
    def _init_docling_converter(self):
        """åˆå§‹åŒ–doclingæ–‡æ¡£è½¬æ¢å™¨ï¼Œå‚è€ƒtest_docling_01.pyçš„é…ç½®"""
        try:
            # é…ç½®PDFå¤„ç†é€‰é¡¹
            pipeline_options = PdfPipelineOptions()
            pipeline_options.generate_picture_images = True
            # pipeline_options.generate_page_images = True
            pipeline_options.images_scale = 2.0  # å›¾ç‰‡åˆ†è¾¨ç‡scale
            pipeline_options.do_picture_description = True
            pipeline_options.enable_remote_services = True  # å¯ç”¨è¿œç¨‹æœåŠ¡ç”¨äºå›¾ç‰‡æè¿°
            
            # é…ç½®å›¾ç‰‡æè¿°APIé€‰é¡¹ - ä½¿ç”¨æœ¬åœ°visionæ¨¡å‹
            pipeline_options.picture_description_options = PictureDescriptionApiOptions(
                url=f"{self.vision_api_endpoint}/chat/completions",
                params=dict(
                    model=self.vision_model_id,
                    seed=42,
                    temperature=0.2,
                    max_completion_tokens=250,
                ),
                prompt="""
You are an assistant tasked with summarizing images for retrieval. 
These summaries will be embedded and used to retrieve the raw image. 
Give a concise summary of the image that is well optimized for retrieval.
""".strip(),
                timeout=180,
            )
            
            # é…ç½®OCRé€‰é¡¹ - ä½¿ç”¨RapidOCR
            # print("Downloading RapidOCR models")
            # download_path = snapshot_download(repo_id="SWHL/RapidOCR") # from HuggingFace
            # # Setup RapidOcrOptions for english detection
            # det_model_path = os.path.join(download_path, "PP-OCRv4", "en_PP-OCRv3_det_infer.onnx")
            # rec_model_path = os.path.join(download_path, "PP-OCRv4", "ch_PP-OCRv4_rec_server_infer.onnx")
            # cls_model_path = os.path.join(download_path, "PP-OCRv3", "ch_ppocr_mobile_v2.0_cls_train.onnx")
            # ocr_options = RapidOcrOptions(
            #     det_model_path=det_model_path,
            #     rec_model_path=rec_model_path,
            #     cls_model_path=cls_model_path,
            # )
            # pipeline_options.do_ocr = True
            # pipeline_options.ocr_options = ocr_options
            
            # åˆ›å»ºæ–‡æ¡£è½¬æ¢å™¨
            self.converter = DocumentConverter(format_options={
                InputFormat.PDF: PdfFormatOption(
                    pipeline_options=pipeline_options,
                )
            })
            
            logger.info("Docling converter initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize docling converter: {e}")
            raise
    
    def _init_chunker(self):
        """åˆå§‹åŒ–DoclingåŸç”Ÿchunkerï¼ŒåŸºäºæœ€ä½³å®è·µé…ç½®"""
        try:
            # å…³é”®è®¾è®¡å†³ç­–ï¼šchunkerçš„tokenizerä¸embeddingæ¨¡å‹è§£è€¦
            # 
            # åŸå› ï¼š
            # 1. HybridChunkerçš„tokenizerä¸»è¦ç”¨äºchunkå¤§å°æ§åˆ¶ï¼Œä¸éœ€è¦ä¸embeddingæ¨¡å‹å®Œå…¨ä¸€è‡´
            # 2. æˆ‘ä»¬é€šè¿‡APIè°ƒç”¨embeddingæœåŠ¡ï¼ˆollama/lm_studioï¼‰ï¼Œæ— æ³•ç›´æ¥ä½¿ç”¨å…¶tokenizer
            # 3. ä½¿ç”¨é€šç”¨tokenizerè¿›è¡Œè¿‘ä¼¼ä¼°ç®—æ›´ç¨³å®šå¯é 
            
            # ä½¿ç”¨é€šç”¨çš„ä¸­è‹±æ–‡å‹å¥½tokenizerä½œä¸ºchunkå¤§å°ä¼°ç®—å™¨
            try:
                # ä¼˜å…ˆå°è¯•ä½¿ç”¨ä¸€ä¸ªé€šç”¨çš„å¤šè¯­è¨€tokenizer
                tokenizer = HuggingFaceTokenizer(
                    tokenizer=AutoTokenizer.from_pretrained("microsoft/multilingual-MiniLM-L12-H384"),
                    max_tokens=512,  # ä¿å®ˆçš„chunkå¤§å°ï¼Œç¡®ä¿embedding APIè°ƒç”¨ç¨³å®š
                )
                logger.info("Using multilingual tokenizer for chunking")
            except Exception as e:
                logger.warning(f"Failed to load multilingual tokenizer: {e}")
                try:
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨BERT tokenizer
                    tokenizer = HuggingFaceTokenizer(
                        tokenizer=AutoTokenizer.from_pretrained("bert-base-uncased"),
                        max_tokens=512,
                    )
                    logger.info("Using BERT tokenizer as fallback for chunking")
                except Exception as e2:
                    logger.error(f"Failed to load any tokenizer: {e2}")
                    raise Exception(f"Cannot initialize any tokenizer. Primary error: {e}, Fallback error: {e2}")
            
            # åˆ›å»ºHybridChunkerå®ä¾‹
            self.chunker = HybridChunker(
                tokenizer=tokenizer,
                merge_peers=True,  # åˆå¹¶ç›¸é‚»çš„åŒç±»chunk
            )
            
            logger.info(f"Chunker initialized with max_tokens={tokenizer.get_max_tokens()}")
            logger.info("Note: Chunker tokenizer is decoupled from embedding model - embeddings generated via API")
            
        except Exception as e:
            logger.error(f"Failed to initialize chunker: {e}")
            raise
    
    def process_document(self, file_path: str) -> bool:
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£çš„å®Œæ•´æµç¨‹
        
        Args:
            file_path: æ–‡æ¡£æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
            
        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"[CHUNKING] Starting document processing: {file_path}")
            self.bridge_events.progress_update("multivector", 0, 100, "å¼€å§‹è§£ææ–‡æ¡£...")
            
            # 1. éªŒè¯æ–‡ä»¶
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"File not found: {file_path}")
            
            # 2. è®¡ç®—æ–‡ä»¶hash
            file_hash = self._calculate_file_hash(file_path)
            
            # 3. æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡ä¸”æ–‡ä»¶æœªå˜æ›´
            existing_doc = self._get_existing_document(file_path, file_hash)
            if existing_doc:
                logger.info(f"[CHUNKING] Document already processed and unchanged: {file_path}")
                self.bridge_events.progress_update("multivector", 100, 100, "æ–‡æ¡£å·²å¤„ç†ï¼Œæ— éœ€é‡å¤å¤„ç†")
                return True
            
            # 4. ä½¿ç”¨doclingè§£ææ–‡æ¡£
            self.bridge_events.progress_update("multivector", 20, 100, "æ­£åœ¨è§£ææ–‡æ¡£ç»“æ„...")
            docling_result = self._parse_with_docling(file_path)
            
            # 5. ä¿å­˜doclingè§£æç»“æœ
            docling_json_path = self._save_docling_result(file_path, docling_result)
            
            # 6. åˆ›å»º/æ›´æ–°Documentè®°å½•
            self.bridge_events.progress_update("multivector", 40, 100, "åˆ›å»ºæ–‡æ¡£è®°å½•...")
            document = self._create_or_update_document(file_path, file_hash, docling_json_path)
            
            # 7. ç”Ÿæˆçˆ¶å—å’Œå­å—
            self.bridge_events.progress_update("multivector", 60, 100, "ç”Ÿæˆå†…å®¹å—...")
            parent_chunks, child_chunks = self._generate_chunks(document.id, docling_result.document)
            
            # 8. å­˜å‚¨åˆ°æ•°æ®åº“
            self.bridge_events.progress_update("multivector", 80, 100, "å­˜å‚¨åˆ°æ•°æ®åº“...")
            self._store_chunks(parent_chunks, child_chunks)
            
            # 8.5. ä¸ºå›¾ç‰‡chunksåˆ›å»ºå›¾æ–‡å…³ç³»å­å—ï¼ˆå…³é”®è®¾è®¡ï¼‰
            self.bridge_events.progress_update("multivector", 85, 100, "åˆ›å»ºå›¾æ–‡å…³ç³»å­å—...")
            all_parent_chunks, all_child_chunks = self._create_image_context_chunks(parent_chunks, child_chunks, document.id)
            
            # å¦‚æœåˆ›å»ºäº†é¢å¤–çš„ä¸Šä¸‹æ–‡å—ï¼Œæ›´æ–°chunkåˆ—è¡¨
            if len(all_parent_chunks) > len(parent_chunks):
                additional_parent_chunks = all_parent_chunks[len(parent_chunks):]
                additional_child_chunks = all_child_chunks[len(child_chunks):]
                
                # å­˜å‚¨é¢å¤–çš„ä¸Šä¸‹æ–‡å—
                self._store_chunks(additional_parent_chunks, additional_child_chunks)
                logger.info(f"Stored {len(additional_parent_chunks)} additional image context chunks")
                
                # æ›´æ–°ç”¨äºå‘é‡åŒ–çš„chunkåˆ—è¡¨
                parent_chunks = all_parent_chunks
                child_chunks = all_child_chunks
            
            # 9. å‘é‡åŒ–å’Œå­˜å‚¨
            self.bridge_events.progress_update("multivector", 90, 100, "å‘é‡åŒ–å’Œå­˜å‚¨...")
            self._vectorize_and_store(parent_chunks, child_chunks)
            
            # 10. æ›´æ–°æ–‡æ¡£çŠ¶æ€
            document.status = "done"
            document.processed_at = datetime.now()
            self.session.add(document)
            self.session.commit()
            
            self.bridge_events.progress_update("multivector", 100, 100, "æ–‡æ¡£å¤„ç†å®Œæˆ")
            logger.info(f"[CHUNKING] Document processing completed: {file_path}")
            
            return True
            
        except Exception as e:
            logger.error(f"[CHUNKING] Document processing failed for {file_path}: {e}", exc_info=True)
            self.bridge_events.error_occurred("chunking_error", f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}", {
                "file_path": file_path,
                "error_type": type(e).__name__
            })
            
            # æ›´æ–°æ–‡æ¡£çŠ¶æ€ä¸ºé”™è¯¯
            try:
                document = self._get_or_create_document_record(file_path, "", "")
                document.status = "error"
                self.session.add(document)
                self.session.commit()
            except:
                pass  # å¿½ç•¥çŠ¶æ€æ›´æ–°é”™è¯¯
            
            return False
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶hashå€¼"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                # å¯¹äºå¤§æ–‡ä»¶ï¼Œåªè¯»å–éƒ¨åˆ†å†…å®¹è®¡ç®—hash
                chunk_size = 8192
                while chunk := f.read(chunk_size):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            logger.error(f"Failed to calculate hash for {file_path}: {e}")
            return ""
    
    def _get_existing_document(self, file_path: str, file_hash: str) -> Optional[Document]:
        """æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç›¸åŒhashçš„æ–‡æ¡£è®°å½•"""
        try:
            from sqlmodel import select
            stmt = select(Document).where(
                Document.file_path == file_path,
                Document.file_hash == file_hash,
                Document.status == "done"
            )
            return self.session.exec(stmt).first()
        except Exception as e:
            logger.error(f"Failed to check existing document: {e}")
            return None
    
    def _parse_with_docling(self, file_path: str) -> ConversionResult:
        """ä½¿ç”¨doclingè§£ææ–‡æ¡£"""
        try:
            logger.info(f"[CHUNKING] Parsing document with docling: {file_path}")
            result = self.converter.convert(source=file_path)
            
            if not result or not result.document:
                raise ValueError("Docling parsing returned empty result")
            
            logger.info(f"[CHUNKING] Docling parsing completed. Document has {len(result.document.pages)} pages")
            return result
            
        except Exception as e:
            logger.error(f"Docling parsing failed for {file_path}: {e}")
            raise
    
    def _save_docling_result(self, file_path: str, result: ConversionResult) -> str:
        """ä¿å­˜doclingè§£æç»“æœåˆ°JSONæ–‡ä»¶"""
        try:
            # ä½¿ç”¨æ•°æ®åº“ç›®å½•çš„docling_cacheå­ç›®å½•
            output_dir = self.docling_cache_dir
            
            # ç”ŸæˆJSONæ–‡ä»¶åï¼ˆç›´æ¥ä½¿ç”¨åŸæ–‡ä»¶åï¼Œä¸æ‹¼æ¥æ—¶é—´æˆ³ï¼‰
            file_stem = Path(file_path).stem
            json_filename = f"{file_stem}.json"
            json_path = output_dir / json_filename
            
            # ä¿å­˜JSON
            result.document.save_as_json(
                filename=json_path,
                indent=2,
                image_mode=ImageRefMode.PLACEHOLDER,
                artifacts_dir=output_dir
            )
            
            logger.info(f"[CHUNKING] Docling result saved to: {json_path}")
            return str(json_path)
            
        except Exception as e:
            logger.error(f"Failed to save docling result: {e}")
            # è¿”å›ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºä¿å­˜å¤±è´¥ï¼Œä½†ä¸å½±å“ä¸»æµç¨‹
            return ""
    
    def _create_or_update_document(self, file_path: str, file_hash: str, docling_json_path: str) -> Document:
        """åˆ›å»ºæˆ–æ›´æ–°Documentè®°å½•"""
        try:
            from sqlmodel import select
            
            # å°è¯•è·å–ç°æœ‰è®°å½•
            stmt = select(Document).where(Document.file_path == file_path)
            document = self.session.exec(stmt).first()
            
            if document:
                # æ›´æ–°ç°æœ‰è®°å½•
                document.file_hash = file_hash
                document.docling_json_path = docling_json_path
                document.status = "processing"
                document.processed_at = datetime.now()
            else:
                # åˆ›å»ºæ–°è®°å½•
                document = Document(
                    file_path=file_path,
                    file_hash=file_hash,
                    docling_json_path=docling_json_path,
                    status="processing",
                    processed_at=datetime.now()
                )
            
            self.session.add(document)
            self.session.commit()
            self.session.refresh(document)
            
            logger.info(f"[CHUNKING] Document record created/updated: ID={document.id}")
            return document
            
        except Exception as e:
            logger.error(f"Failed to create/update document record: {e}")
            raise
    
    def _get_or_create_document_record(self, file_path: str, file_hash: str, docling_json_path: str) -> Document:
        """è·å–æˆ–åˆ›å»ºæ–‡æ¡£è®°å½•ï¼ˆè¾…åŠ©æ–¹æ³•ï¼‰"""
        try:
            return self._create_or_update_document(file_path, file_hash, docling_json_path)
        except:
            # å¦‚æœåˆ›å»ºå¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªä¸´æ—¶å¯¹è±¡
            return Document(
                file_path=file_path,
                file_hash=file_hash,
                docling_json_path=docling_json_path,
                status="error"
            )
    
    def _generate_chunks(self, document_id: int, docling_doc: DoclingDocument) -> Tuple[List[ParentChunk], List[ChildChunk]]:
        """
        ä½¿ç”¨Docling HybridChunkerè¿›è¡Œæ™ºèƒ½åˆ†å—
        
        è®¾è®¡æ€è·¯ï¼š
        1. ä½¿ç”¨Doclingçš„HybridChunkerè¿›è¡Œè¯­ä¹‰æ„ŸçŸ¥çš„åˆ†å—
        2. chunkerçš„tokenizerä»…ç”¨äºchunkå¤§å°æ§åˆ¶ï¼Œä¸embeddingæ¨¡å‹è§£è€¦
        3. å®é™…embeddingç”Ÿæˆé€šè¿‡APIè°ƒç”¨å®Œæˆï¼Œæ”¯æŒQwen3-Embedding-0.6Bç­‰æ¨¡å‹
        4. é‡‡ç”¨çˆ¶å­å—æ¶æ„ï¼Œçˆ¶å—ç”¨äºæ£€ç´¢ï¼Œå­å—ä¿ç•™ç»†èŠ‚
        
        Args:
            document_id: æ–‡æ¡£ID
            docling_doc: Doclingè§£æçš„æ–‡æ¡£å¯¹è±¡
            
        Returns:
            (parent_chunks, child_chunks): çˆ¶å—å’Œå­å—çš„å…ƒç»„
        """
        logger.info(f"[CHUNKING] Generating chunks using HybridChunker for document ID: {document_id}")
        
        all_parent_chunks = []
        all_child_chunks = []
        
        try:
            # ä½¿ç”¨HybridChunkerç”Ÿæˆchunks
            chunk_iter = self.chunker.chunk(dl_doc=docling_doc)
            chunks = list(chunk_iter)
            
            logger.info(f"[CHUNKING] HybridChunker generated {len(chunks)} chunks")
            
            # ä¸ºæ¯ä¸ªchunkåˆ›å»ºçˆ¶å—å’Œå¯¹åº”çš„å­å—
            for i, chunk in enumerate(chunks):
                try:
                    # è·å–åŸå§‹chunkæ–‡æœ¬ - ç”¨äºçˆ¶å—ï¼ˆä¿æŒæ•°æ®çº¯å‡€æ€§ï¼‰
                    raw_content = chunk.text
                    
                    # è·å–ä¸Šä¸‹æ–‡ä¸°å¯Œçš„å†…å®¹ - ä»…ç”¨äºå­å—æ‘˜è¦ç”Ÿæˆ
                    contextualized_content = self.chunker.contextualize(chunk=chunk)
                    
                    # ç¡®å®šchunkç±»å‹
                    chunk_type = self._determine_chunk_type(chunk)
                    
                    logger.debug(f"[CHUNKING] Chunk {i}: type={chunk_type}, raw_length={len(raw_content)}, contextualized_length={len(contextualized_content)}")
                    
                    # å¦‚æœæ˜¯å›¾ç‰‡ç±»å‹ä¸”åŒ…å«æ··åˆå†…å®¹ï¼Œéœ€è¦è¿›è¡Œåˆ†å‰²å¤„ç†
                    if chunk_type == "image" and self._contains_mixed_content(raw_content):
                        logger.debug(f"[CHUNKING] Chunk {i} contains mixed content, splitting...")
                        # åˆ†å‰²æ··åˆå†…å®¹ä¸ºçº¯å›¾ç‰‡æè¿°å’Œçº¯æ–‡æœ¬éƒ¨åˆ†
                        sub_chunks = self._split_mixed_chunk(chunk, document_id, i)
                        all_parent_chunks.extend([sc[0] for sc in sub_chunks])
                        all_child_chunks.extend([sc[1] for sc in sub_chunks])
                    else:
                        # æ­£å¸¸å¤„ç†å•ä¸€ç±»å‹çš„chunk
                        parent_chunk, child_chunk = self._create_chunk_pair(
                            chunk, document_id, i, chunk_type, raw_content, contextualized_content
                        )
                        all_parent_chunks.append(parent_chunk)
                        all_child_chunks.append(child_chunk)
                    
                except Exception as e:
                    logger.error(f"Failed to process chunk {i}: {e}")
                    continue
            
            logger.info(f"[CHUNKING] Successfully generated {len(all_parent_chunks)} parent chunks and {len(all_child_chunks)} child chunks")
            
        except Exception as e:
            logger.error(f"Failed to generate chunks using HybridChunker: {e}")
            raise
        
        return all_parent_chunks, all_child_chunks
    
    def _determine_chunk_type(self, chunk) -> str:
        """
        æ ¹æ®chunkçš„å†…å®¹å’Œå…ƒæ•°æ®ç¡®å®šç±»å‹
        
        æ³¨æ„ï¼šæˆ‘ä»¬é¿å…ä½¿ç”¨"mixed"ç±»å‹ï¼Œä»¥ä¿æŒæ•°æ®çº¯å‡€æ€§
        å¦‚æœchunkåŒ…å«å¤šç§ç±»å‹ï¼Œæˆ‘ä»¬ä¼šåœ¨_generate_chunksä¸­è¿›è¡Œæ‹†åˆ†å¤„ç†
        
        Args:
            chunk: HybridChunkerç”Ÿæˆçš„chunkå¯¹è±¡
            
        Returns:
            chunkç±»å‹å­—ç¬¦ä¸²: "text", "image", "table"
        """
        try:
            # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
            logger.debug(f"[CHUNK_TYPE] Processing chunk with text length: {len(chunk.text)}")
            logger.debug(f"[CHUNK_TYPE] Chunk has meta: {hasattr(chunk, 'meta')}")
            
            if hasattr(chunk.meta, 'doc_items') and chunk.meta.doc_items:
                logger.debug(f"[CHUNK_TYPE] Found {len(chunk.meta.doc_items)} doc_items")
                
                # æ£€æŸ¥doc_itemsä¸­çš„ç±»å‹
                item_types = set()
                for i, item in enumerate(chunk.meta.doc_items):
                    item_label = None
                    if hasattr(item, 'label'):
                        item_label = str(item.label)
                        item_types.add(item_label)
                    logger.debug(f"[CHUNK_TYPE] Item {i}: type={type(item).__name__}, label={item_label}")
                
                logger.debug(f"[CHUNK_TYPE] All item types found: {item_types}")
                
                # ä¼˜å…ˆçº§ï¼šè¡¨æ ¼ > å›¾ç‰‡ > æ–‡æœ¬
                # è¿™æ ·å¯ä»¥ç¡®ä¿ä¸»è¦å†…å®¹ç±»å‹ä¸è¢«å¿½ç•¥
                if any('TABLE' in item_type.upper() for item_type in item_types):
                    logger.debug(f"[CHUNK_TYPE] Determined type: table")
                    return "table"
                elif any('PICTURE' in item_type.upper() or 'IMAGE' in item_type.upper() for item_type in item_types):
                    logger.debug(f"[CHUNK_TYPE] Determined type: image")
                    return "image"
                else:
                    logger.debug(f"[CHUNK_TYPE] Determined type: text (default)")
                    return "text"
            else:
                logger.debug(f"[CHUNK_TYPE] No doc_items found, defaulting to text")
                # å¦‚æœæ²¡æœ‰å…ƒæ•°æ®ï¼Œæ ¹æ®å†…å®¹é•¿åº¦å’Œç‰¹å¾åˆ¤æ–­
                return "text"
        except Exception as e:
            logger.warning(f"Failed to determine chunk type: {e}")
            return "text"
    
    def _contains_mixed_content(self, content: str) -> bool:
        """
        æ£€æµ‹å†…å®¹æ˜¯å¦åŒ…å«å›¾ç‰‡æè¿°å’Œæ–‡æœ¬çš„æ··åˆå†…å®¹
        
        Args:
            content: è¦æ£€æµ‹çš„å†…å®¹
            
        Returns:
            bool: æ˜¯å¦åŒ…å«æ··åˆå†…å®¹
        """
        # æ£€æµ‹æ˜¯å¦åŒ…å«å›¾ç‰‡æè¿°çš„ç‰¹å¾
        # å›¾ç‰‡æè¿°é€šå¸¸ä»¥ç‰¹å®šçš„å¥å¼å¼€å¤´ï¼Œå¹¶ä¸”åŒ…å«å›¾ç‰‡ç›¸å…³çš„è¯æ±‡
        content_lower = content.lower()
        
        # å›¾ç‰‡æè¿°çš„å¸¸è§å¼€å¤´å’Œç‰¹å¾è¯
        image_indicators = [
            "the image illustrates",
            "the image shows",
            "the image displays",
            "the diagram shows",
            "the figure illustrates",
            "the chart displays",
            "this image depicts",
            "the visualization shows"
        ]
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾ç‰‡æè¿°ç‰¹å¾
        has_image_description = any(indicator in content_lower for indicator in image_indicators)
        
        # æ£€æŸ¥æ˜¯å¦è¿˜åŒ…å«å…¶ä»–æ–‡æœ¬å†…å®¹ï¼ˆé€šå¸¸å›¾ç‰‡æè¿°åä¼šæœ‰æ¢è¡Œå’Œå…¶ä»–æ–‡æœ¬ï¼‰
        lines = content.strip().split('\n')
        has_multiple_sections = len(lines) > 3  # å›¾ç‰‡æè¿°é€šå¸¸æ˜¯1-2è¡Œï¼Œå¦‚æœè¶…è¿‡3è¡Œå¯èƒ½åŒ…å«å…¶ä»–å†…å®¹
        
        # ç®€å•çš„ä¸­æ–‡å†…å®¹æ£€æµ‹ï¼ˆå¦‚æœåŒ…å«å¤§é‡ä¸­æ–‡ï¼Œå¯èƒ½æ˜¯åŸæ–‡ï¼‰
        chinese_chars = len([c for c in content if '\u4e00' <= c <= '\u9fff'])
        has_chinese_content = chinese_chars > 50  # å¦‚æœä¸­æ–‡å­—ç¬¦è¶…è¿‡50ä¸ªï¼Œå¯èƒ½åŒ…å«åŸæ–‡
        
        return has_image_description and (has_multiple_sections or has_chinese_content)
    
    def _split_mixed_chunk(self, chunk, document_id: int, chunk_index: int) -> List[Tuple[ParentChunk, ChildChunk]]:
        """
        åˆ†å‰²åŒ…å«æ··åˆå†…å®¹çš„chunkä¸ºå¤šä¸ªçº¯å‡€çš„chunks
        
        Args:
            chunk: åŸå§‹chunkå¯¹è±¡
            document_id: æ–‡æ¡£ID
            chunk_index: chunkç´¢å¼•
            
        Returns:
            List[Tuple[ParentChunk, ChildChunk]]: åˆ†å‰²åçš„chunkå¯¹åˆ—è¡¨
        """
        result_chunks = []
        content = chunk.text.strip()
        
        try:
            # ç®€å•çš„åˆ†å‰²ç­–ç•¥ï¼šé€šè¿‡æ®µè½åˆ†éš”
            paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
            
            if len(paragraphs) < 2:
                # å¦‚æœæ²¡æœ‰æ˜æ˜¾çš„æ®µè½åˆ†éš”ï¼Œå°è¯•æŒ‰å¥å­åˆ†å‰²
                paragraphs = [p.strip() for p in content.split('\n') if p.strip()]
            
            current_section = []
            current_type = None
            
            for para in paragraphs:
                para_lower = para.lower()
                
                # åˆ¤æ–­è¿™ä¸ªæ®µè½æ˜¯å›¾ç‰‡æè¿°è¿˜æ˜¯æ™®é€šæ–‡æœ¬
                is_image_desc = any(indicator in para_lower for indicator in [
                    "the image", "the diagram", "the figure", "the chart", 
                    "image illustrates", "image shows", "image displays"
                ])
                
                para_type = "image" if is_image_desc else "text"
                
                if current_type is None:
                    current_type = para_type
                    current_section.append(para)
                elif current_type == para_type:
                    current_section.append(para)
                else:
                    # ç±»å‹å˜åŒ–ï¼Œä¿å­˜å½“å‰sectionå¹¶å¼€å§‹æ–°çš„section
                    if current_section:
                        section_content = '\n\n'.join(current_section)
                        parent_chunk, child_chunk = self._create_chunk_pair(
                            chunk, document_id, f"{chunk_index}_{current_type}", 
                            current_type, section_content, section_content
                        )
                        result_chunks.append((parent_chunk, child_chunk))
                    
                    # å¼€å§‹æ–°section
                    current_type = para_type
                    current_section = [para]
            
            # å¤„ç†æœ€åä¸€ä¸ªsection
            if current_section:
                section_content = '\n\n'.join(current_section)
                parent_chunk, child_chunk = self._create_chunk_pair(
                    chunk, document_id, f"{chunk_index}_{current_type}", 
                    current_type, section_content, section_content
                )
                result_chunks.append((parent_chunk, child_chunk))
            
            logger.debug(f"[CHUNKING] Split chunk {chunk_index} into {len(result_chunks)} sub-chunks")
            
        except Exception as e:
            logger.error(f"Failed to split mixed chunk {chunk_index}: {e}")
            # å¦‚æœåˆ†å‰²å¤±è´¥ï¼Œè¿”å›åŸå§‹chunkä½œä¸ºtextç±»å‹
            parent_chunk, child_chunk = self._create_chunk_pair(
                chunk, document_id, chunk_index, "text", content, content
            )
            result_chunks = [(parent_chunk, child_chunk)]
        
        return result_chunks
    
    def _create_chunk_pair(self, chunk, document_id: int, chunk_index, chunk_type: str, 
                          raw_content: str, contextualized_content: str) -> Tuple[ParentChunk, ChildChunk]:
        """
        åˆ›å»ºä¸€å¯¹çˆ¶å—å’Œå­å—
        
        Args:
            chunk: åŸå§‹chunkå¯¹è±¡
            document_id: æ–‡æ¡£ID
            chunk_index: chunkç´¢å¼•
            chunk_type: chunkç±»å‹
            raw_content: åŸå§‹å†…å®¹
            contextualized_content: ä¸Šä¸‹æ–‡åŒ–å†…å®¹
            
        Returns:
            Tuple[ParentChunk, ChildChunk]: çˆ¶å—å’Œå­å—çš„å…ƒç»„
        """
        # åˆ›å»ºçˆ¶å— - å­˜å‚¨åŸå§‹çº¯å‡€å†…å®¹
        parent_chunk = ParentChunk(
            document_id=document_id,
            chunk_type=chunk_type,
            content=raw_content,  # ä½¿ç”¨åŸå§‹å†…å®¹ï¼Œä¿æŒæ•°æ®çº¯å‡€æ€§
            metadata_json=json.dumps({
                "chunk_index": chunk_index,
                "original_text_length": len(raw_content),
                "contextualized_length": len(contextualized_content),
                "doc_items_refs": [item.self_ref for item in chunk.meta.doc_items] if hasattr(chunk.meta, 'doc_items') else [],
                "chunk_id": chunk.meta.chunk_id if hasattr(chunk.meta, 'chunk_id') else None,
                "source": "hybrid_chunker_split" if isinstance(chunk_index, str) else "hybrid_chunker"
            })
        )
        
        # ç”Ÿæˆæ£€ç´¢å‹å¥½çš„å­å—å†…å®¹
        retrieval_content = self._generate_retrieval_summary(contextualized_content, chunk_type)
        
        # åˆ›å»ºå­å—
        child_chunk = ChildChunk(
            parent_chunk_id=0,  # åœ¨å­˜å‚¨æ—¶ä¼šè®¾ç½®æ­£ç¡®çš„ID
            retrieval_content=retrieval_content,
            vector_id=generate_vector_id()
        )
        
        return parent_chunk, child_chunk
    
    def _generate_retrieval_summary(self, content: str, chunk_type: str) -> str:
        """
        ä½¿ç”¨LLMä¸ºchunkå†…å®¹ç”Ÿæˆæ£€ç´¢å‹å¥½çš„æ‘˜è¦
        
        Args:
            content: å®Œæ•´çš„chunkå†…å®¹
            chunk_type: chunkç±»å‹
            
        Returns:
            æ£€ç´¢ä¼˜åŒ–çš„æ‘˜è¦æ–‡æœ¬
        """
        try:
            # å¦‚æœå†…å®¹å¤ªçŸ­ï¼Œç›´æ¥è¿”å›åŸå†…å®¹
            if len(content.strip()) < 50:
                return content.strip()
            
            # æ ¹æ®chunkç±»å‹é€‰æ‹©åˆé€‚çš„æç¤ºè¯
            if chunk_type == "table":
                prompt_prefix = """You are an assistant tasked with summarizing tables for retrieval. 
These summaries will be embedded and used to retrieve the raw table elements. 
Give a concise summary of the table that is well optimized for retrieval.

IMPORTANT: Output ONLY the summary content, without any prefixes like "Here's a summary:", "Summary:", or formatting markers."""
            elif chunk_type == "image":
                prompt_prefix = """You are an assistant tasked with summarizing image content for retrieval. 
These summaries will be embedded and used to retrieve the raw image elements. 
Give a concise summary of the image content that is well optimized for retrieval.

IMPORTANT: Output ONLY the summary content, without any prefixes like "Here's a summary:", "Summary:", or formatting markers."""
            else:
                prompt_prefix = """You are an assistant tasked with summarizing text for retrieval. 
These summaries will be embedded and used to retrieve the raw text elements. 
Give a concise summary of the text that is well optimized for retrieval.

IMPORTANT: Output ONLY the summary content, without any prefixes like "Here's a summary:", "Summary:", or formatting markers."""
            
            # æ„å»ºå®Œæ•´æç¤º
            messages = [
                {"role": "system", "content": prompt_prefix},
                {"role": "user", "content": f"Content to summarize:\n\n{content}"}
            ]
            
            # è°ƒç”¨LLMç”Ÿæˆæ‘˜è¦
            summary = self.models_mgr.get_chat_completion(messages, role_type="base")
            
            if summary and summary.strip():
                # åå¤„ç†ï¼šæ¸…ç†å¸¸è§çš„æ ¼å¼åŒ–å‰ç¼€
                cleaned_summary = self._clean_summary_prefixes(summary.strip())
                return cleaned_summary
            else:
                # å¦‚æœLLMç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ç®€å•çš„æ–‡æœ¬æˆªå–
                return content[:500] + "..." if len(content) > 500 else content
                
        except Exception as e:
            logger.error(f"Failed to generate retrieval summary: {e}")
            # é™çº§å¤„ç†ï¼šç›´æ¥æˆªå–å†…å®¹
            return content[:500] + "..." if len(content) > 500 else content
    
    def _clean_summary_prefixes(self, summary: str) -> str:
        """
        æ¸…ç†LLMç”Ÿæˆæ‘˜è¦ä¸­çš„å¸¸è§æ ¼å¼åŒ–å‰ç¼€
        
        Args:
            summary: åŸå§‹æ‘˜è¦æ–‡æœ¬
            
        Returns:
            æ¸…ç†åçš„æ‘˜è¦æ–‡æœ¬
        """
        # å¸¸è§çš„éœ€è¦æ¸…ç†çš„å‰ç¼€æ¨¡å¼
        prefixes_to_remove = [
            "Here's a retrieval-optimized summary:",
            "Here's a retrieval-optimized summary of the text:",
            "Here's a retrieval-optimized summary of the image:",
            "Here's a retrieval-optimized summary of the table:",
            "Here's a concise summary:",
            "**Summary:**",
            "Summary:",
            "**Image Summary:**",
            "Image Summary:",
            "**Table Summary:**",
            "Table Summary:",
            "The image",
            "The table",
            "This text"
        ]
        
        cleaned = summary.strip()
        
        # ç§»é™¤å¼€å¤´çš„æ ¼å¼åŒ–å‰ç¼€
        for prefix in prefixes_to_remove:
            if cleaned.lower().startswith(prefix.lower()):
                cleaned = cleaned[len(prefix):].strip()
                break
        
        # ç§»é™¤å¼€å¤´çš„å†’å·ã€ç ´æŠ˜å·ç­‰
        while cleaned and cleaned[0] in [':', '-', '*', ' ']:
            cleaned = cleaned[1:].strip()
        
        return cleaned if cleaned else summary  # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œè¿”å›åŸå§‹å†…å®¹
    
    def _create_image_context_chunks(self, parent_chunks: List[ParentChunk], child_chunks: List[ChildChunk], 
                                   document_id: int) -> Tuple[List[ParentChunk], List[ChildChunk]]:
        """
        ä¸ºå›¾åƒå—åˆ›å»ºé¢å¤–çš„ä¸Šä¸‹æ–‡å—ï¼ˆå›¾ç‰‡æè¿° + å‘¨å›´åŸå§‹æ–‡æœ¬çš„æ‘˜è¦ï¼‰
        è¿™ä¸ªåŠŸèƒ½å®ç°äº†MULTIVECTOR.mdä¸­çš„å…³é”®è®¾è®¡ï¼šåŠ å¼ºå›¾åƒä¸æ–‡æœ¬çš„å…³è”æ£€ç´¢
        
        Args:
            parent_chunks: ç°æœ‰çš„çˆ¶å—åˆ—è¡¨
            child_chunks: ç°æœ‰çš„å­å—åˆ—è¡¨
            document_id: æ–‡æ¡£ID
            
        Returns:
            Tuple[List[ParentChunk], List[ChildChunk]]: æ›´æ–°åçš„çˆ¶å—å’Œå­å—åˆ—è¡¨
        """
        try:
            additional_parent_chunks = []
            additional_child_chunks = []
            
            # æ‰¾åˆ°æ‰€æœ‰å›¾åƒå—
            image_chunks = [(i, chunk) for i, chunk in enumerate(parent_chunks) if chunk.chunk_type == "image"]
            
            if not image_chunks:
                return parent_chunks, child_chunks
            
            logger.info(f"Found {len(image_chunks)} image chunks, creating context chunks...")
            
            for chunk_idx, image_chunk in image_chunks:
                try:
                    # è·å–å›¾åƒæè¿°å†…å®¹
                    image_description = image_chunk.content
                    if not image_description or len(image_description.strip()) < 10:
                        continue
                    
                    # è·å–å‘¨å›´çš„æ–‡æœ¬å—å†…å®¹è¿›è¡Œæ‘˜è¦
                    surrounding_texts = self._get_surrounding_text_chunks(parent_chunks, chunk_idx)
                    if not surrounding_texts:
                        continue
                    
                    # ç”Ÿæˆå‘¨å›´æ–‡æœ¬çš„æ‘˜è¦
                    context_summary = self._generate_context_summary(surrounding_texts)
                    if not context_summary:
                        continue
                    
                    # åˆ›å»ºç»„åˆå†…å®¹ï¼šå›¾ç‰‡æè¿° + å‘¨å›´æ–‡æœ¬æ‘˜è¦
                    combined_content = f"å›¾åƒå†…å®¹ï¼š{image_description}\n\nç›¸å…³æ–‡æœ¬èƒŒæ™¯ï¼š{context_summary}"
                    
                    # åˆ›å»ºé¢å¤–çš„çˆ¶å—ï¼ˆå›¾åƒä¸Šä¸‹æ–‡å—ï¼‰
                    context_parent = ParentChunk(
                        document_id=document_id,
                        chunk_type="image_context",
                        content=combined_content,
                        metadata_json=json.dumps({
                            "related_image_chunk_index": chunk_idx,
                            "context_type": "image_with_text_summary",
                            "source": "multivector_image_context",
                            "surrounding_chunks_count": len(surrounding_texts)
                        })
                    )
                    
                    # ç”Ÿæˆæ£€ç´¢å‹å¥½çš„å­å—å†…å®¹
                    retrieval_content = self._generate_retrieval_summary(combined_content, "image_context")
                    
                    # åˆ›å»ºå¯¹åº”çš„å­å—
                    context_child = ChildChunk(
                        parent_chunk_id=0,  # åœ¨å­˜å‚¨æ—¶ä¼šè®¾ç½®æ­£ç¡®çš„ID
                        retrieval_content=retrieval_content,
                        vector_id=generate_vector_id()
                    )
                    
                    additional_parent_chunks.append(context_parent)
                    additional_child_chunks.append(context_child)
                    
                    logger.debug(f"Created image context chunk for image chunk {chunk_idx}")
                    
                except Exception as e:
                    logger.error(f"Failed to create context chunk for image chunk {chunk_idx}: {e}")
                    continue
            
            # åˆå¹¶æ‰€æœ‰å—
            all_parent_chunks = parent_chunks + additional_parent_chunks
            all_child_chunks = child_chunks + additional_child_chunks
            
            logger.info(f"Created {len(additional_parent_chunks)} additional image context chunks")
            return all_parent_chunks, all_child_chunks
            
        except Exception as e:
            logger.error(f"Failed to create image context chunks: {e}")
            return parent_chunks, child_chunks
    
    def _get_surrounding_text_chunks(self, parent_chunks: List[ParentChunk], image_chunk_idx: int, 
                                   context_window: int = 2) -> List[str]:
        """
        è·å–å›¾åƒå—å‘¨å›´çš„æ–‡æœ¬å—å†…å®¹
        
        Args:
            parent_chunks: æ‰€æœ‰çˆ¶å—åˆ—è¡¨
            image_chunk_idx: å›¾åƒå—çš„ç´¢å¼•
            context_window: å‰åæ–‡æœ¬å—çš„æ•°é‡
            
        Returns:
            List[str]: å‘¨å›´æ–‡æœ¬å—çš„å†…å®¹åˆ—è¡¨
        """
        surrounding_texts = []
        
        # è·å–å‰é¢çš„æ–‡æœ¬å—
        start_idx = max(0, image_chunk_idx - context_window)
        for i in range(start_idx, image_chunk_idx):
            chunk = parent_chunks[i]
            if chunk.chunk_type == "text" and chunk.content:
                surrounding_texts.append(chunk.content)
        
        # è·å–åé¢çš„æ–‡æœ¬å—
        end_idx = min(len(parent_chunks), image_chunk_idx + context_window + 1)
        for i in range(image_chunk_idx + 1, end_idx):
            chunk = parent_chunks[i]
            if chunk.chunk_type == "text" and chunk.content:
                surrounding_texts.append(chunk.content)
        
        return surrounding_texts
    
    def _generate_context_summary(self, text_chunks: List[str]) -> str:
        """
        ä¸ºå‘¨å›´çš„æ–‡æœ¬å—ç”Ÿæˆæ‘˜è¦
        
        Args:
            text_chunks: æ–‡æœ¬å—å†…å®¹åˆ—è¡¨
            
        Returns:
            str: ç”Ÿæˆçš„æ‘˜è¦
        """
        if not text_chunks:
            return ""
        
        try:
            # åˆå¹¶æ–‡æœ¬å†…å®¹
            combined_text = "\n\n".join(text_chunks)
            
            # å¦‚æœæ–‡æœ¬å¤ªçŸ­ï¼Œç›´æ¥è¿”å›
            if len(combined_text.strip()) < 50:
                return combined_text.strip()
            
            # æ„å»ºæ‘˜è¦æç¤º
            prompt = """ä½ éœ€è¦ä¸ºä»¥ä¸‹æ–‡æœ¬å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼Œè¿™ä¸ªæ‘˜è¦å°†ä¸å›¾åƒæè¿°ç»„åˆï¼Œç”¨äºå¢å¼ºå›¾åƒä¸æ–‡æœ¬çš„å…³è”æ£€ç´¢ã€‚

è¯·ç”Ÿæˆä¸€ä¸ªçªå‡ºä¸»è¦è§‚ç‚¹å’Œå…³é”®ä¿¡æ¯çš„æ‘˜è¦ï¼Œå¸®åŠ©ç†è§£å›¾åƒåœ¨æ–‡æ¡£ä¸­çš„ä¸Šä¸‹æ–‡èƒŒæ™¯ã€‚

é‡è¦ï¼šåªè¾“å‡ºæ‘˜è¦å†…å®¹ï¼Œä¸è¦åŒ…å«"æ‘˜è¦ï¼š"ã€"æ€»ç»“ï¼š"ç­‰å‰ç¼€ã€‚

æ–‡æœ¬å†…å®¹ï¼š
"""
            
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬æ‘˜è¦åŠ©æ‰‹ï¼Œæ“…é•¿ç”Ÿæˆç®€æ´å‡†ç¡®çš„æ‘˜è¦ã€‚"},
                {"role": "user", "content": f"{prompt}\n\n{combined_text}"}
            ]
            
            # è°ƒç”¨LLMç”Ÿæˆæ‘˜è¦
            summary = self.models_mgr.get_chat_completion(messages, role_type="base")
            
            if summary and summary.strip():
                # æ¸…ç†æ ¼å¼åŒ–å‰ç¼€
                cleaned_summary = self._clean_summary_prefixes(summary.strip())
                return cleaned_summary
            else:
                # å¦‚æœLLMç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ç®€å•æˆªå–
                return combined_text[:300] + "..." if len(combined_text) > 300 else combined_text
                
        except Exception as e:
            logger.error(f"Failed to generate context summary: {e}")
            # é™çº§å¤„ç†ï¼šæˆªå–å‰é¢éƒ¨åˆ†å†…å®¹
            combined_text = "\n\n".join(text_chunks)
            return combined_text[:300] + "..." if len(combined_text) > 300 else combined_text
    
    def _store_chunks(self, parent_chunks: List[ParentChunk], child_chunks: List[ChildChunk]):
        """å­˜å‚¨çˆ¶å—å’Œå­å—åˆ°SQLite"""
        try:
            if len(parent_chunks) != len(child_chunks):
                raise ValueError(f"Parent chunks count ({len(parent_chunks)}) does not match child chunks count ({len(child_chunks)})")
            
            # 1. å…ˆå­˜å‚¨çˆ¶å—
            if parent_chunks:
                self.session.add_all(parent_chunks)
                self.session.commit()
                
                # åˆ·æ–°ä»¥è·å–ç”Ÿæˆçš„ID
                for chunk in parent_chunks:
                    self.session.refresh(chunk)
                
                logger.info(f"[CHUNKING] Stored {len(parent_chunks)} parent chunks")
            
            # 2. è®¾ç½®å­å—çš„parent_chunk_idå¹¶å­˜å‚¨
            if child_chunks:
                for i, child_chunk in enumerate(child_chunks):
                    child_chunk.parent_chunk_id = parent_chunks[i].id
                
                self.session.add_all(child_chunks)
                self.session.commit()
                
                # åˆ·æ–°ä»¥è·å–ç”Ÿæˆçš„ID
                for chunk in child_chunks:
                    self.session.refresh(chunk)
                
                logger.info(f"[CHUNKING] Stored {len(child_chunks)} child chunks")
            
        except Exception as e:
            logger.error(f"Failed to store chunks: {e}")
            self.session.rollback()
            raise
    
    def _vectorize_and_store(self, parent_chunks: List[ParentChunk], child_chunks: List[ChildChunk]):
        """å‘é‡åŒ–å­å—å¹¶å­˜å‚¨åˆ°LanceDBï¼ˆçˆ¶å—ä¸éœ€è¦å‘é‡åŒ–ï¼‰"""
        try:
            # ç¡®ä¿vectorsè¡¨å·²åˆå§‹åŒ–
            self.lancedb_mgr.init_vectors_table()
            
            vector_records = []
            
            # åªå¤„ç†å­å—å‘é‡åŒ–ï¼ˆçˆ¶å—ä¸éœ€è¦å‘é‡åŒ–ï¼Œå®ƒä»¬æ˜¯ç”¨äºç­”æ¡ˆåˆæˆçš„åŸå§‹å†…å®¹ï¼‰
            for i, child_chunk in enumerate(child_chunks):
                try:
                    # è°ƒç”¨embeddingæ¨¡å‹è¿›è¡Œå‘é‡åŒ–
                    embedding = self.models_mgr.get_embedding(child_chunk.retrieval_content)
                    
                    if not embedding:
                        logger.warning(f"Failed to get embedding for child chunk ID: {child_chunk.id}")
                        continue
                    
                    # ä»å¯¹åº”çš„çˆ¶å—è·å–document_id
                    parent_chunk = parent_chunks[i] if i < len(parent_chunks) else None
                    document_id = parent_chunk.document_id if parent_chunk else 0
                    
                    # åˆ›å»ºå­å—å‘é‡è®°å½•
                    vector_record = {
                        "vector_id": child_chunk.vector_id,
                        "vector": embedding,
                        "parent_chunk_id": child_chunk.parent_chunk_id,
                        "document_id": document_id,
                        "retrieval_content": child_chunk.retrieval_content[:500]  # å­˜å‚¨å‰500å­—ç¬¦ç”¨äºæ£€ç´¢æ˜¾ç¤º
                    }
                    vector_records.append(vector_record)
                    
                except Exception as e:
                    logger.error(f"Failed to vectorize child chunk ID {child_chunk.id}: {e}")
                    continue
            
            # æ‰¹é‡å­˜å‚¨åˆ°LanceDB
            if vector_records:
                self.lancedb_mgr.add_vectors(vector_records)
                logger.info(f"[CHUNKING] Vectorized and stored {len(vector_records)} child chunk vectors")
            
            logger.info(f"[CHUNKING] Vector storage completed - {len(parent_chunks)} parent chunks stored in SQLite only, {len(child_chunks)} child chunks vectorized in LanceDB")
            
        except Exception as e:
            logger.error(f"Failed to vectorize and store: {e}")
            raise


# ä¸ºäº†æµ‹è¯•å’Œè°ƒè¯•ä½¿ç”¨
def test_chunking_mgr():
    """æµ‹è¯•åˆ†å—ç®¡ç†å™¨çš„åŸºæœ¬åŠŸèƒ½"""
    # 1. åˆå§‹åŒ–å„ä¸ªç»„ä»¶
    logger.info("ğŸ”§ åˆå§‹åŒ–ç»„ä»¶...")
    # SQLiteæ•°æ®åº“
    from config import TEST_DB_PATH
    from sqlmodel import create_engine
    session = Session(create_engine(f'sqlite:///{TEST_DB_PATH}'))
    # LanceDB
    db_directory = Path(TEST_DB_PATH).parent
    lancedb_mgr = LanceDBMgr(base_dir=db_directory)
    # æ¨¡å‹ç®¡ç†å™¨
    models_mgr = ModelsMgr(session)
    # äº‹ä»¶å‘é€å™¨
    bridge_events = BridgeEventSender()
    # åˆ†å—ç®¡ç†å™¨
    try:
        chunking_mgr = ChunkingMgr(session, lancedb_mgr, models_mgr)
        logging.info('âœ… ChunkingManageråˆå§‹åŒ–æˆåŠŸ')
        logging.info(f'âœ… Tokenizerè§£è€¦æ¶æ„å·²å¯ç”¨')
        logging.info(f'âœ… é…ç½®çš„embeddingç»´åº¦: {chunking_mgr.embedding_dimensions}')
        logging.info(f'âœ… Chunkeræœ€å¤§tokens: {chunking_mgr.chunker.tokenizer.get_max_tokens()}')
    except Exception as e:
        logger.info(f"âŒ Doclingè½¬æ¢å™¨åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    logger.info("âœ… ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")

    # 2. æ‰¾ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£
    file_path = "/Users/dio/Downloads/AIä»£ç†çš„ä¸Šä¸‹æ–‡å·¥ç¨‹ï¼šæ„å»ºManusçš„ç»éªŒæ•™è®­.pdf"
    
    # # 3. ä»process_document()ä¸­æ‹†åˆ†å‡ºçš„æ–¹æ³•è¿›è¡Œç‹¬ç«‹æµ‹è¯•
    # logger.info("ğŸ§ª æµ‹è¯•åŸºæœ¬æ–¹æ³•...")
    # file_hash = chunking_mgr._calculate_file_hash(file_path)
    # logger.info(f"âœ… æ–‡ä»¶å“ˆå¸Œè®¡ç®—å®Œæˆ: {file_hash}")
    # existing_doc = chunking_mgr._get_existing_document(file_path, file_hash)
    # if existing_doc:
    #     logger.info(f"âœ… å·²å­˜åœ¨æ–‡æ¡£è®°å½•: {existing_doc.id}, çŠ¶æ€: {existing_doc.status}")
    # else:
    #     logger.info("âœ… æœªæ‰¾åˆ°ç°æœ‰æ–‡æ¡£è®°å½•")
    # docling_result = chunking_mgr._parse_with_docling(file_path)
    # logger.info(f"âœ… Doclingè§£æå®Œæˆ: {len(docling_result.document.pages)}é¡µ")
    # docling_json_path = chunking_mgr._save_docling_result(file_path, docling_result)
    # logger.info(f"âœ… Doclingç»“æœä¿å­˜å®Œæˆ: {docling_json_path}")
    # document = chunking_mgr._create_or_update_document(file_path, file_hash, docling_json_path)
    # logger.info(f"âœ… æ–‡æ¡£è®°å½•åˆ›å»º/æ›´æ–°å®Œæˆ, ID: {document.id}")
    # parent_chunks, child_chunks = chunking_mgr._generate_chunks(document.id, docling_result.document)
    # logger.info(f"âœ… ç”Ÿæˆå†…å®¹å—å®Œæˆ: {len(parent_chunks)}çˆ¶å—, {len(child_chunks)}å­å—")
    # chunking_mgr._store_chunks(parent_chunks, child_chunks)        
    # chunking_mgr._vectorize_and_store(parent_chunks, child_chunks)
    # document.status = "done"
    # document.processed_at = datetime.now()
    # chunking_mgr.session.add(document)
    # chunking_mgr.session.commit()

    # 4. æœ€åé›†æˆæµ‹è¯•æ–‡æ¡£å¤„ç†
    try:
        # å¤„ç†æ–‡æ¡£
        result = chunking_mgr.process_document(str(file_path))
        logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {result}")
    
    except Exception as e:
        logger.info(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # è®¾ç½®æµ‹è¯•æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    test_chunking_mgr()

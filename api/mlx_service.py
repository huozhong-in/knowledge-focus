"""
ç‹¬ç«‹çš„ MLX-VLM æœåŠ¡

ä¸“é—¨æä¾› OpenAI å…¼å®¹çš„ /v1/chat/completions æ¥å£
ä½¿ç”¨ Tauri sidecar æ–¹å¼å¯åŠ¨ï¼Œä¸ä¸» FastAPI æœåŠ¡éš”ç¦»

ä¼˜åŠ¿ï¼š
- å®Œå…¨ç‹¬ç«‹çš„ Metal ä¸Šä¸‹æ–‡ï¼Œé¿å…å†²çª
- å´©æºƒéš”ç¦»ï¼Œä¸å½±å“ä¸»æœåŠ¡
- æ›´ç®€å•çš„èµ„æºç®¡ç†
"""
import logging
import sys
import argparse
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
# å¯¼å…¥ OpenAI å…¼å®¹å±‚å’Œå†…ç½®æ¨¡å‹é…ç½®
from builtin_openai_compat import (
    get_vlm_manager,
    OpenAIChatCompletionRequest,
    RequestPriority
)

# é…ç½®æ—¥å¿—ï¼ˆç®€å•é…ç½®ï¼Œè¾“å‡ºåˆ° stdoutï¼‰
# æ³¨æ„ï¼šå®é™…æ—¥å¿—æ–‡ä»¶ç”±çˆ¶è¿›ç¨‹ï¼ˆmodels_builtin.pyï¼‰é€šè¿‡ stdout é‡å®šå‘æ§åˆ¶
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)  # åªè¾“å‡ºåˆ° stdoutï¼Œçˆ¶è¿›ç¨‹ä¼šæ•è·
    ]
)
logger = logging.getLogger(__name__)

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="MLX-VLM Service",
    description="OpenAI-compatible chat completions endpoint powered by MLX-VLM",
    version="1.0.0"
)

# é…ç½® CORSï¼ˆå…è®¸ä¸»æœåŠ¡è°ƒç”¨ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/v1/chat/completions")
async def chat_completions(request: OpenAIChatCompletionRequest):
    """
    OpenAI å…¼å®¹çš„èŠå¤©è¡¥å…¨æ¥å£
    
    æ”¯æŒï¼š
    - æ–‡æœ¬å¯¹è¯
    - å¤šæ¨¡æ€è¾“å…¥ï¼ˆå›¾ç‰‡+æ–‡æœ¬ï¼‰
    - æµå¼å’Œéæµå¼å“åº”
    - ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼ˆä¼šè¯ > æ‰¹é‡ä»»åŠ¡ï¼‰
    
    æ”¯æŒä¸¤ç§æ¨¡å‹æ ‡è¯†ç¬¦ï¼š
    1. model_id (å¦‚ "qwen3-vl-4b") - æˆ‘ä»¬è‡ªå®šä¹‰çš„ alias
    2. hf_model_id (å¦‚ "mlx-community/Qwen3-VL-4B-Instruct-3bit") - HuggingFace å®Œæ•´æ¨¡å‹ ID
    
    é‡è¦ï¼šä¸ºäº†æ”¯æŒç¦»çº¿ä½¿ç”¨ï¼Œä¼šå°è¯•è§£æä¸ºæœ¬åœ°è·¯å¾„
    """
    logger.info(f"Received chat completion request: model={request.model}, stream={request.stream}")
    
    # è·å– VLM ç®¡ç†å™¨
    manager = get_vlm_manager()
    
    # è§£ææ¨¡å‹è·¯å¾„ï¼ˆä¼˜å…ˆä½¿ç”¨æœ¬åœ°è·¯å¾„ä»¥æ”¯æŒç¦»çº¿ï¼‰
    model_id = request.model
    model_path = None
    
    # å°è¯•ä» ModelsBuiltin è·å–æœ¬åœ°è·¯å¾„
    try:
        from models_builtin import ModelsBuiltin, BUILTIN_MODELS
        from sqlmodel import create_engine
        import os
        
        # è·å– base_dir
        base_dir = app.state.base_dir
        
        # åˆ›å»ºä¸´æ—¶ engineï¼ˆåªç”¨äºæŸ¥è¯¢ï¼‰
        db_path = os.path.join(base_dir, 'knowledge-focus.db')
        engine = create_engine(f'sqlite:///{db_path}')
        
        # è·å– ModelsBuiltin å®ä¾‹
        models_builtin = ModelsBuiltin(engine=engine, base_dir=base_dir)
        
        # æ”¯æŒä¸¤ç§æ¨¡å‹æ ‡è¯†ç¬¦:
        # 1. model_id (å¦‚ "qwen3-vl-4b")
        if model_id in BUILTIN_MODELS:
            # å°è¯•è·å–æœ¬åœ°è·¯å¾„
            local_path = models_builtin.get_model_path(model_id)
            if local_path:
                model_path = local_path
                logger.info(f"âœ… Using local model path for '{model_id}': {model_path}")
            else:
                # æœ¬åœ°æœªä¸‹è½½ï¼Œä½¿ç”¨ HuggingFace IDï¼ˆä¼šå°è¯•è”ç½‘ä¸‹è½½ï¼‰
                model_path = BUILTIN_MODELS[model_id]["hf_model_id"]
                logger.warning(f"âš ï¸  Model '{model_id}' not downloaded locally, using HF ID: {model_path}")
        
        # 2. hf_model_id (å¦‚ "mlx-community/Qwen3-VL-4B-Instruct-3bit")
        else:
            # å°è¯•é€šè¿‡ hf_model_id æŸ¥æ‰¾å¯¹åº”çš„ model_id
            found = False
            for mid, config in BUILTIN_MODELS.items():
                if config["hf_model_id"] == model_id:
                    # æ‰¾åˆ°å¯¹åº”çš„ model_idï¼Œå°è¯•è·å–æœ¬åœ°è·¯å¾„
                    local_path = models_builtin.get_model_path(mid)
                    if local_path:
                        model_path = local_path
                        logger.info(f"âœ… Found local model by HF ID '{model_id}' -> alias: {mid}, path: {model_path}")
                    else:
                        model_path = model_id  # ä½¿ç”¨ HF ID
                        logger.warning(f"âš ï¸  Model '{mid}' not downloaded locally, using HF ID: {model_path}")
                    found = True
                    break
            
            if not found:
                # æœªæ‰¾åˆ°ï¼Œç›´æ¥ä½¿ç”¨ï¼ˆå¯èƒ½æ˜¯å®Œæ•´è·¯å¾„æˆ– HF IDï¼‰
                model_path = model_id
                logger.warning(f"Model '{model_id}' not found in BUILTIN_MODELS, using as-is")
    
    except Exception as e:
        # å¦‚æœè§£æå¤±è´¥ï¼Œå›é€€åˆ°ä½¿ç”¨åŸå§‹ model_id
        logger.error(f"Failed to resolve model path: {e}, using model_id as-is")
        model_path = model_id
    
    # ç¡®å®šä¼˜å…ˆçº§
    # é»˜è®¤ä¸º LOW ä¼˜å…ˆçº§ï¼Œä¼šè¯ç•Œé¢å¯ä»¥è®¾ç½®ä¸º HIGH
    # TODO: å¯ä»¥é€šè¿‡è¯·æ±‚å¤´æˆ–å‚æ•°ä¼ é€’ä¼˜å…ˆçº§
    priority = RequestPriority.LOW
    
    # å°†è¯·æ±‚åŠ å…¥é˜Ÿåˆ—
    logger.info(f"Enqueueing request with priority: {priority.name}, model_path: {model_path}")
    result = await manager.enqueue_request(request, model_path, priority)
    
    logger.info("Request completed successfully")
    return result

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {
        "status": "healthy",
        "service": "mlx-vlm",
        "version": "1.0.0"
    }

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "MLX-VLM Service",
        "endpoints": {
            "chat_completions": "/v1/chat/completions",
            "health": "/health"
        }
    }

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="MLX-VLM Service")
    parser.add_argument("--port", type=int, default=60316, help="æœåŠ¡ç›‘å¬ç«¯å£")
    parser.add_argument("--host", type=str, default="127.0.0.1", help="æœåŠ¡ç›‘å¬åœ°å€")
    parser.add_argument("--base-dir", type=str, help="ç”¨æˆ·åº”ç”¨ä¸´æ—¶ç›®å½•")
    args = parser.parse_args()
    app.state.base_dir = args.base_dir
    
    print(f"ğŸš€ Starting MLX-VLM Service on {args.host}:{args.port}")
    print(f"ğŸ“– API Documentation: http://{args.host}:{args.port}/docs")
    print(f"ğŸ—„ï¸  Base Directory: {args.base_dir}")
    
    # å¯åŠ¨æœåŠ¡
    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        log_level="info",
        access_log=True
    )

if __name__ == "__main__":
    from utils import kill_process_on_port
    kill_process_on_port(60316)
    main()

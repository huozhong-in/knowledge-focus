"""
ç‹¬ç«‹çš„ MLX-VLM æœåŠ¡

ä¸“é—¨æä¾› OpenAI å…¼å®¹çš„ /v1/chat/completions æ¥å£
ä½¿ç”¨ Tauri sidecar æ–¹å¼å¯åŠ¨ï¼Œä¸ä¸» FastAPI æœåŠ¡éš”ç¦»

ä¼˜åŠ¿ï¼š
- å®Œå…¨ç‹¬ç«‹çš„ Metal ä¸Šä¸‹æ–‡ï¼Œé¿å…å†²çª
- å´©æºƒéš”ç¦»ï¼Œä¸å½±å“ä¸»æœåŠ¡
- æ›´ç®€å•çš„èµ„æºç®¡ç†
"""
import logging
import sys
import argparse
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
# å¯¼å…¥ OpenAI å…¼å®¹å±‚å’Œå†…ç½®æ¨¡å‹é…ç½®
from builtin_openai_compat import (
    get_vlm_manager,
    OpenAIChatCompletionRequest,
    RequestPriority
)
from models_builtin import BUILTIN_MODELS

# é…ç½®æ—¥å¿—ï¼ˆç®€å•é…ç½®ï¼Œè¾“å‡ºåˆ° stdoutï¼‰
# æ³¨æ„ï¼šå®é™…æ—¥å¿—æ–‡ä»¶ç”±çˆ¶è¿›ç¨‹ï¼ˆmodels_builtin.pyï¼‰é€šè¿‡ stdout é‡å®šå‘æ§åˆ¶
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)  # åªè¾“å‡ºåˆ° stdoutï¼Œçˆ¶è¿›ç¨‹ä¼šæ•è·
    ]
)
logger = logging.getLogger(__name__)

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="MLX-VLM Service",
    description="OpenAI-compatible chat completions endpoint powered by MLX-VLM",
    version="1.0.0"
)

# é…ç½® CORSï¼ˆå…è®¸ä¸»æœåŠ¡è°ƒç”¨ï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/v1/chat/completions")
async def chat_completions(request: OpenAIChatCompletionRequest):
    """
    OpenAI å…¼å®¹çš„èŠå¤©è¡¥å…¨æ¥å£
    
    æ”¯æŒï¼š
    - æ–‡æœ¬å¯¹è¯
    - å¤šæ¨¡æ€è¾“å…¥ï¼ˆå›¾ç‰‡+æ–‡æœ¬ï¼‰
    - æµå¼å’Œéæµå¼å“åº”
    - ä¼˜å…ˆçº§é˜Ÿåˆ—ï¼ˆä¼šè¯ > æ‰¹é‡ä»»åŠ¡ï¼‰
    
    æ”¯æŒä¸¤ç§æ¨¡å‹æ ‡è¯†ç¬¦ï¼š
    1. model_id (å¦‚ "qwen3-vl-4b") - æˆ‘ä»¬è‡ªå®šä¹‰çš„ alias
    2. hf_model_id (å¦‚ "mlx-community/Qwen3-VL-4B-Instruct-3bit") - HuggingFace å®Œæ•´æ¨¡å‹ ID
    """
    logger.info(f"Received chat completion request: model={request.model}, stream={request.stream}")
    
    # è·å– VLM ç®¡ç†å™¨
    manager = get_vlm_manager()
    
    # è§£ææ¨¡å‹è·¯å¾„
    model_id = request.model
    model_path = None
    
    # æ”¯æŒä¸¤ç§æ¨¡å‹æ ‡è¯†ç¬¦:
    # 1. model_id (å¦‚ "qwen3-vl-4b")
    # 2. hf_model_id (å¦‚ "mlx-community/Qwen3-VL-4B-Instruct-3bit")
    if model_id in BUILTIN_MODELS:
        # ç›´æ¥ä½¿ç”¨ model_idï¼ˆaliasï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬éœ€è¦è·å–å®é™…çš„æ¨¡å‹è·¯å¾„
        # ä½† mlx_service.py æ˜¯ç‹¬ç«‹è¿›ç¨‹ï¼Œæ²¡æœ‰ ModelsBuiltin å®ä¾‹
        # æ‰€ä»¥æˆ‘ä»¬ä¼ é€’ HuggingFace model_id ç»™ MLXï¼Œè®©å®ƒè‡ªåŠ¨æŸ¥æ‰¾
        model_path = BUILTIN_MODELS[model_id]["hf_model_id"]
        logger.info(f"Using model alias '{model_id}' -> HF model: {model_path}")
    else:
        # å°è¯•é€šè¿‡ hf_model_id æŸ¥æ‰¾
        found = False
        for mid, config in BUILTIN_MODELS.items():
            if config["hf_model_id"] == model_id:
                model_path = config["hf_model_id"]
                logger.info(f"Found model by HF ID '{model_id}' -> alias: {mid}")
                found = True
                break
        
        if not found:
            # æœªæ‰¾åˆ°ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨ï¼ˆå¯èƒ½æ˜¯å®Œæ•´è·¯å¾„ï¼‰
            model_path = model_id
            logger.warning(f"Model '{model_id}' not found in BUILTIN_MODELS, using as-is")
    
    # ç¡®å®šä¼˜å…ˆçº§
    # é»˜è®¤ä¸º LOW ä¼˜å…ˆçº§ï¼Œä¼šè¯ç•Œé¢å¯ä»¥è®¾ç½®ä¸º HIGH
    # TODO: å¯ä»¥é€šè¿‡è¯·æ±‚å¤´æˆ–å‚æ•°ä¼ é€’ä¼˜å…ˆçº§
    priority = RequestPriority.LOW
    
    # å°†è¯·æ±‚åŠ å…¥é˜Ÿåˆ—
    logger.info(f"Enqueueing request with priority: {priority.name}, model_path: {model_path}")
    result = await manager.enqueue_request(request, model_path, priority)
    
    logger.info("Request completed successfully")
    return result

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return {
        "status": "healthy",
        "service": "mlx-vlm",
        "version": "1.0.0"
    }

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "MLX-VLM Service",
        "endpoints": {
            "chat_completions": "/v1/chat/completions",
            "health": "/health"
        }
    }

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="MLX-VLM Service")
    parser.add_argument("--port", type=int, default=60316, help="æœåŠ¡ç›‘å¬ç«¯å£")
    parser.add_argument("--host", type=str, default="127.0.0.1", help="æœåŠ¡ç›‘å¬åœ°å€")
    args = parser.parse_args()
    
    print(f"ğŸš€ Starting MLX-VLM Service on {args.host}:{args.port}")
    print(f"ğŸ“– API Documentation: http://{args.host}:{args.port}/docs")
    
    # å¯åŠ¨æœåŠ¡
    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        log_level="info",
        access_log=True
    )

if __name__ == "__main__":
    main()

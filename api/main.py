import os
import sys
import argparse
import logging
import time
import threading
import signal
import asyncio
from datetime import datetime
from typing import Dict, Any
from pathlib import Path
from contextlib import asynccontextmanager
from fastapi import FastAPI, Body, Depends
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from utils import kill_process_on_port, monitor_parent, kill_orphaned_processes
from sqlmodel import create_engine, Session, select
from sqlalchemy import Engine, event, text
from db_mgr import (
    DBManager, 
    TaskStatus, 
    TaskResult, 
    TaskType, 
    TaskPriority, 
    Task, 
    SystemConfig,
)
from screening_mgr import FileScreeningResult
from models_mgr import ModelsMgr
from lancedb_mgr import LanceDBMgr
from file_tagging_mgr import FileTaggingMgr, configure_parsing_warnings
from multivector_mgr import MultiVectorMgr
from task_mgr import TaskManager
# APIè·¯ç”±å¯¼å…¥å°†åœ¨lifespanå‡½æ•°ä¸­è¿›è¡Œ

# # åˆå§‹åŒ–logger
logger = logging.getLogger()

# --- SQLite WAL Mode Setup ---
def setup_sqlite_wal_mode(engine):
    """ä¸ºSQLiteå¼•æ“è®¾ç½®WALæ¨¡å¼å’Œä¼˜åŒ–å‚æ•°"""
    @event.listens_for(engine, "connect")
    def set_sqlite_pragma(dbapi_connection, connection_record):
        """è®¾ç½®SQLiteä¼˜åŒ–å‚æ•°å’ŒWALæ¨¡å¼"""
        cursor = dbapi_connection.cursor()
        # å¯ç”¨WALæ¨¡å¼ï¼ˆWrite-Ahead Loggingï¼‰
        # WALæ¨¡å¼å…è®¸è¯»å†™æ“ä½œå¹¶å‘æ‰§è¡Œï¼Œæ˜¾è‘—å‡å°‘é”å®šå†²çª
        cursor.execute("PRAGMA journal_mode=WAL")
        # è®¾ç½®åŒæ­¥æ¨¡å¼ä¸ºNORMALï¼Œåœ¨WALæ¨¡å¼ä¸‹æä¾›è‰¯å¥½çš„æ€§èƒ½å’Œå®‰å…¨æ€§å¹³è¡¡
        cursor.execute("PRAGMA synchronous=NORMAL")
        # è®¾ç½®ç¼“å­˜å¤§å°ï¼ˆè´Ÿæ•°è¡¨ç¤ºKBï¼Œè¿™é‡Œè®¾ç½®ä¸º64MBï¼‰
        cursor.execute("PRAGMA cache_size=-65536")
        # å¯ç”¨å¤–é”®çº¦æŸ
        cursor.execute("PRAGMA foreign_keys=ON")
        # è®¾ç½®ä¸´æ—¶å­˜å‚¨ä¸ºå†…å­˜æ¨¡å¼
        cursor.execute("PRAGMA temp_store=MEMORY")
        # è®¾ç½®WALè‡ªåŠ¨æ£€æŸ¥ç‚¹é˜ˆå€¼ï¼ˆé¡µé¢æ•°ï¼‰
        cursor.execute("PRAGMA wal_autocheckpoint=1000")
        cursor.close()

def create_optimized_sqlite_engine(sqlite_url, **kwargs):
    """åˆ›å»ºä¼˜åŒ–çš„SQLiteå¼•æ“ï¼Œè‡ªåŠ¨é…ç½®WALæ¨¡å¼"""
    default_connect_args = {"check_same_thread": False, "timeout": 30}
    # åˆå¹¶ç”¨æˆ·æä¾›çš„connect_args
    if "connect_args" in kwargs:
        default_connect_args.update(kwargs["connect_args"])
    kwargs["connect_args"] = default_connect_args
    # åˆ›å»ºå¼•æ“
    engine = create_engine(sqlite_url, echo=False, **kwargs)
    # è®¾ç½®WALæ¨¡å¼
    setup_sqlite_wal_mode(engine)
    return engine

# --- Centralized Logging Setup ---
def setup_logging(logging_dir: str):
    """
    Configures the root logger for the application.

    args:
        logging_dir (str): The directory where log files will be stored.
    """
    
    try:
        # Determine log directory
        log_dir = Path(logging_dir) / 'logs'
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        log_dir.mkdir(parents=True, exist_ok=True)
        
        log_filename = f'api_{time.strftime("%Y%m%d")}.log'
        log_filepath = log_dir / log_filename
        
        # è·å–æ ¹æ—¥å¿—å™¨
        root_logger = logging.getLogger()
        
        # æ¸…é™¤å¯èƒ½å­˜åœ¨çš„é»˜è®¤handlersï¼Œé¿å…é‡å¤
        if root_logger.handlers:
            root_logger.handlers.clear()
            
        # è®¾ç½®æ—¥å¿—çº§åˆ«
        root_logger.setLevel(logging.INFO)
        
        # åˆ›å»ºformatter
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        
        # Console handler - è¾“å‡ºåˆ°æ§åˆ¶å°
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.INFO)
        console_handler.setFormatter(formatter)
        root_logger.addHandler(console_handler)

        # File handler - è¾“å‡ºåˆ°æ–‡ä»¶
        file_handler = logging.FileHandler(log_filepath, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(formatter)
        root_logger.addHandler(file_handler)
        
        # é˜²æ­¢æ—¥å¿—ä¼ æ’­åˆ°çˆ¶loggerï¼Œé¿å…é‡å¤è¾“å‡º
        root_logger.propagate = False
        
        print(f"æ—¥å¿—é…ç½®æˆåŠŸ: æ–‡ä»¶è·¯å¾„ {log_filepath}")

    except Exception as e:
        print(f"Failed to set up logging: {e}", file=sys.stderr)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨"""
    # é‡æ–°é…ç½®loggingä»¥ç¡®ä¿uvicornå¯åŠ¨åä»ç„¶æœ‰æ•ˆ
    if hasattr(app.state, "db_path"):
        db_directory = os.path.dirname(app.state.db_path)
        setup_logging(logging_dir=db_directory)
    
    # åœ¨åº”ç”¨å¯åŠ¨æ—¶æ‰§è¡Œåˆå§‹åŒ–æ“ä½œ
    logger.info("Application is starting...")
    
    try:
        logger.info(f"Debug Info: Python version {sys.version}")
        logger.info(f"Debug Info: Current working directory {os.getcwd()}")
        
        # åˆå§‹åŒ–æ•°æ®åº“å¼•æ“
        if hasattr(app.state, "db_path"):
            sqlite_url = f"sqlite:///{app.state.db_path}"
            logger.info(f"Initializing database engine, URL: {sqlite_url}")
            # ä¿å­˜æ•°æ®åº“ç›®å½•è·¯å¾„ä¾›å…¶ä»–ç»„ä»¶ä½¿ç”¨
            app.state.db_directory = os.path.dirname(app.state.db_path)
            try:
                # åˆ›å»ºä¼˜åŒ–çš„SQLiteæ•°æ®åº“å¼•æ“ï¼Œè‡ªåŠ¨é…ç½®WALæ¨¡å¼
                app.state.engine = create_optimized_sqlite_engine(
                    sqlite_url,
                    pool_size=5,       # è®¾ç½®è¿æ¥æ± å¤§å°
                    max_overflow=10,   # å…è®¸çš„æœ€å¤§æº¢å‡ºè¿æ¥æ•°
                    pool_timeout=30,   # è·å–è¿æ¥çš„è¶…æ—¶æ—¶é—´
                    pool_recycle=1800  # 30åˆ†é’Ÿå›æ”¶ä¸€æ¬¡è¿æ¥
                )
                logger.info("SQLite WAL mode and optimization parameters have been set")
                logger.info(f"Database engine initialized, path: {app.state.db_path}")
                
                # Initialize database structure - use single connection method to avoid connection contention
                try:
                    logger.info("Starting database structure initialization...")
                    # Use a single connection to complete all database initialization operations
                    with app.state.engine.connect() as conn:
                        logger.info("Setting WAL mode and optimization parameters...")
                        # æ˜¾å¼è®¾ç½®WALæ¨¡å¼å’Œä¼˜åŒ–å‚æ•°ï¼ˆç¡®ä¿åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä¹Ÿæ­£ç¡®è®¾ç½®ï¼‰
                        conn.execute(text("PRAGMA journal_mode=WAL"))
                        conn.execute(text("PRAGMA synchronous=NORMAL"))
                        conn.execute(text("PRAGMA cache_size=-65536"))
                        conn.execute(text("PRAGMA foreign_keys=ON"))
                        conn.execute(text("PRAGMA temp_store=MEMORY"))
                        conn.execute(text("PRAGMA wal_autocheckpoint=1000"))
                        
                        # éªŒè¯WALæ¨¡å¼è®¾ç½®
                        journal_mode = conn.execute(text("PRAGMA journal_mode")).fetchone()[0]
                        if journal_mode.upper() != 'WAL':
                            logger.warning(f"WAL mode setup might have failed, current mode: {journal_mode}")
                        else:
                            logger.info("WAL mode successfully set")

                        # æœ€ç»ˆæäº¤è¿æ¥çº§åˆ«çš„äº‹åŠ¡
                        conn.commit()
                    
                    db_mgr = DBManager(app.state.engine)
                    db_mgr.init_db()
                    logger.info("Database structure initialization completed")
                            
                        
                except Exception as init_err:
                    logger.error(f"åˆå§‹åŒ–æ•°æ®åº“ç»“æ„å¤±è´¥: {str(init_err)}", exc_info=True)
                    # ç»§ç»­è¿è¡Œåº”ç”¨ï¼Œä¸è¦å› ä¸ºåˆå§‹åŒ–å¤±è´¥è€Œä¸­æ–­
                    # å¯èƒ½æ˜¯å› ä¸ºè¡¨å·²ç»å­˜åœ¨ï¼Œè¿™ç§æƒ…å†µæ˜¯æ­£å¸¸çš„
            except Exception as db_err:
                logger.error(f"åˆå§‹åŒ–æ•°æ®åº“å¼•æ“å¤±è´¥: {str(db_err)}", exc_info=True)
                raise
        else:
            logger.warning("æœªè®¾ç½®æ•°æ®åº“è·¯å¾„ï¼Œæ•°æ®åº“å¼•æ“æœªåˆå§‹åŒ–")
        
        # å…ˆæ¸…ç†å¯èƒ½å­˜åœ¨çš„å­¤ç«‹å­è¿›ç¨‹
        try:
            logger.info("Cleaning up potentially orphaned subprocesses...")
            kill_orphaned_processes("python", "task_processor")
            kill_orphaned_processes("Python", "task_processor")
            kill_orphaned_processes("python", "high_priority_task_processor")
            kill_orphaned_processes("Python", "high_priority_task_processor")
        except Exception as proc_err:
            logger.error(f"æ¸…ç†å­¤ç«‹è¿›ç¨‹å¤±è´¥: {str(proc_err)}", exc_info=True)
        
        # åˆå§‹åŒ–åå°ä»»åŠ¡å¤„ç†çº¿ç¨‹ï¼ˆä½¿ç”¨å…±äº«å¼•æ“ï¼‰
        try:
            logger.info("Initializing background task processing thread...")
            # åˆ›å»ºä¸€ä¸ªäº‹ä»¶æ¥ä¼˜é›…åœ°åœæ­¢çº¿ç¨‹
            app.state.task_processor_stop_event = threading.Event()
            app.state.task_processor_thread = threading.Thread(
                target=task_processor,
                args=(app.state.engine, app.state.db_directory, app.state.task_processor_stop_event),
                daemon=True
            )
            app.state.task_processor_thread.start()
            logger.info("Background task processing thread has started")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–åå°ä»»åŠ¡å¤„ç†çº¿ç¨‹å¤±è´¥: {e}", exc_info=True)
            raise
        
        # åˆå§‹åŒ–é«˜ä¼˜å…ˆçº§ä»»åŠ¡å¤„ç†çº¿ç¨‹ï¼ˆä½¿ç”¨å…±äº«å¼•æ“ï¼‰
        try:
            logger.info("Initializing high-priority task processing thread...")
            # åˆ›å»ºä¸€ä¸ªäº‹ä»¶æ¥ä¼˜é›…åœ°åœæ­¢çº¿ç¨‹
            app.state.high_priority_task_processor_stop_event = threading.Event()
            app.state.high_priority_task_processor_thread = threading.Thread(
                target=high_priority_task_processor,
                args=(app.state.engine, app.state.db_directory, app.state.high_priority_task_processor_stop_event),
                daemon=True
            )
            app.state.high_priority_task_processor_thread.start()
            logger.info("High-priority task processing thread has started")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–é«˜ä¼˜å…ˆçº§ä»»åŠ¡å¤„ç†çº¿ç¨‹å¤±è´¥: {e}", exc_info=True)
            raise
        
        # Start monitor can kill self process if parent process is dead or exit
        try:
            logger.info("Starting parent process monitoring thread...")
            monitor_thread = threading.Thread(target=monitor_parent, daemon=True)
            monitor_thread.start()
            logger.info("Parent process monitoring thread has started")
        except Exception as monitor_err:
            logger.error(f"å¯åŠ¨çˆ¶è¿›ç¨‹ç›‘æ§çº¿ç¨‹å¤±è´¥: {str(monitor_err)}", exc_info=True)

        # é…ç½®è§£æåº“çš„è­¦å‘Šå’Œæ—¥å¿—çº§åˆ«
        try:
            configure_parsing_warnings()
            logger.info("Parsing library log configuration has been applied")
        except Exception as parsing_config_err:
            logger.error(f"é…ç½®è§£æåº“æ—¥å¿—å¤±è´¥: {str(parsing_config_err)}", exc_info=True)

        # æ³¨å†ŒAPIè·¯ç”±ï¼ˆåœ¨æ•°æ®åº“åˆå§‹åŒ–å®Œæˆåï¼‰
        try:
            logger.info("Registering API routes...")
            
            # åŠ¨æ€å¯¼å…¥APIè·¯ç”±
            from models_api import get_router as get_models_router
            from tagging_api import get_router as get_tagging_router
            from chatsession_api import get_router as get_chatsession_router
            from myfolders_api import get_router as get_myfolders_router
            from screening_api import get_router as get_screening_router
            from search_api import get_router as get_search_router
            from unified_tools_api import get_router as get_tools_router
            from documents_api import get_router as get_documents_router
            from user_api import get_router as get_user_router
            
            # æ³¨å†Œå„ä¸ªAPIè·¯ç”±
            models_router = get_models_router(get_engine=get_engine, base_dir=app.state.db_directory)
            app.include_router(models_router, prefix="", tags=["models"])
            
            tagging_router = get_tagging_router(get_engine=get_engine, base_dir=app.state.db_directory)
            app.include_router(tagging_router, prefix="", tags=["tagging"])
            
            chatsession_router = get_chatsession_router(get_engine=get_engine, base_dir=app.state.db_directory)
            app.include_router(chatsession_router, prefix="", tags=["chat-sessions"])
            
            myfolders_router = get_myfolders_router(get_engine=get_engine)
            app.include_router(myfolders_router, prefix="", tags=["myfolders"])
            
            screening_router = get_screening_router(get_engine=get_engine)
            app.include_router(screening_router, prefix="", tags=["screening"])
            
            search_router = get_search_router(get_engine=get_engine, base_dir=app.state.db_directory)
            app.include_router(search_router, prefix="", tags=["search"])
            
            tools_router = get_tools_router(get_engine=get_engine)
            app.include_router(tools_router, prefix="", tags=["tools"])
            
            documents_router = get_documents_router(get_engine=get_engine, base_dir=app.state.db_directory)
            app.include_router(documents_router, prefix="", tags=["documents"])
            
            # ç”¨æˆ·è®¤è¯ç›¸å…³è·¯ç”±
            user_router = get_user_router(get_engine=get_engine)
            app.include_router(user_router, prefix="", tags=["user", "auth"])
            
            logger.info("All API routes have been successfully registered")
        except Exception as router_err:
            logger.error(f"æ³¨å†ŒAPIè·¯ç”±å¤±è´¥: {str(router_err)}", exc_info=True)
            raise

        # å¯åŠ¨ MLX æœåŠ¡è¿›ç¨‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
        try:
            from models_builtin import ModelsBuiltin
            logger.info("Checking if MLX service needs to be started...")
            builtin_mgr = ModelsBuiltin(engine=app.state.engine, base_dir=app.state.db_directory)
            is_running = builtin_mgr.ensure_mlx_service_running()
            if is_running:
                logger.info("MLX service is confirmed to be running on port 60316")
            else:
                logger.info("MLX service is not required to run or has been stopped")
        except Exception as mlx_err:
            logger.error(f"MLX æœåŠ¡å¯åŠ¨æ£€æŸ¥å¤±è´¥: {str(mlx_err)}", exc_info=True)
            # ä¸ä¸­æ–­å¯åŠ¨æµç¨‹
        
        # å¯åŠ¨ MLX æœåŠ¡ç›‘æ§ä»»åŠ¡ï¼ˆè‡ªåŠ¨é‡å¯å´©æºƒçš„æœåŠ¡ï¼‰
        try:
            import asyncio
            logger.info("Starting MLX service monitor task...")
            app.state.mlx_monitor_stop_event = asyncio.Event()
            app.state.mlx_monitor_task = asyncio.create_task(
                mlx_service_monitor(
                    engine=app.state.engine,
                    base_dir=app.state.db_directory,
                    stop_event=app.state.mlx_monitor_stop_event
                )
            )
            logger.info("MLX service monitor task has been started")
        except Exception as monitor_err:
            logger.error(f"å¯åŠ¨ MLX æœåŠ¡ç›‘æ§ä»»åŠ¡å¤±è´¥: {str(monitor_err)}", exc_info=True)
            # ä¸ä¸­æ–­å¯åŠ¨æµç¨‹ï¼ˆç›‘æ§æ˜¯å¯é€‰çš„ï¼‰

        # æ­£å¼å¼€å§‹æœåŠ¡
        logger.info("Application initialization completed, starting to provide services...")
        yield

    except Exception as e:
        logger.critical(f"åº”ç”¨å¯åŠ¨è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}", exc_info=True)
        # ç¡®ä¿å¼‚å¸¸ä¼ æ’­ï¼Œè¿™æ ·FastAPIä¼šçŸ¥é“å¯åŠ¨å¤±è´¥
        raise
    finally:
        # é€€å‡ºå‰çš„æ¸…ç†å·¥ä½œ
        logger.info("Application is starting to shut down...")
        
        # åœæ­¢ MLX æœåŠ¡ç›‘æ§ä»»åŠ¡
        try:
            if hasattr(app.state, "mlx_monitor_task") and not app.state.mlx_monitor_task.done():
                logger.info("Stopping MLX service monitor task...")
                app.state.mlx_monitor_stop_event.set()
                # ç­‰å¾…ä»»åŠ¡å®Œæˆï¼ˆæœ€å¤š 5 ç§’ï¼‰
                try:
                    await asyncio.wait_for(app.state.mlx_monitor_task, timeout=5.0)
                    logger.info("MLX service monitor task has stopped")
                except asyncio.TimeoutError:
                    logger.warning("MLX æœåŠ¡ç›‘æ§ä»»åŠ¡åœ¨ 5 ç§’å†…æœªåœæ­¢ï¼Œå¼ºåˆ¶å–æ¶ˆ")
                    app.state.mlx_monitor_task.cancel()
        except Exception as e:
            logger.error(f"åœæ­¢ MLX æœåŠ¡ç›‘æ§ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
        
        try:
            if hasattr(app.state, "task_processor_thread") and app.state.task_processor_thread.is_alive():
                logger.info("Stopping background task processing thread...")
                app.state.task_processor_stop_event.set()
                app.state.task_processor_thread.join(timeout=5) # ç­‰å¾…5ç§’
                if app.state.task_processor_thread.is_alive():
                    logger.warning("åå°ä»»åŠ¡å¤„ç†çº¿ç¨‹åœ¨5ç§’å†…æœªåœæ­¢")
                else:
                    logger.info("Background task processing thread has stopped")
        except Exception as e:
            logger.error(f"åœæ­¢åå°ä»»åŠ¡å¤„ç†çº¿ç¨‹å¤±è´¥: {e}", exc_info=True)
        
        try:
            if hasattr(app.state, "high_priority_task_processor_thread") and app.state.high_priority_task_processor_thread.is_alive():
                logger.info("Stopping high-priority task processing thread...")
                app.state.high_priority_task_processor_stop_event.set()
                app.state.high_priority_task_processor_thread.join(timeout=5) # ç­‰å¾…5ç§’
                if app.state.high_priority_task_processor_thread.is_alive():
                    logger.warning("é«˜ä¼˜å…ˆçº§ä»»åŠ¡å¤„ç†çº¿ç¨‹åœ¨5ç§’å†…æœªåœæ­¢")
                else:
                    logger.info("High-priority task processing thread has stopped")
        except Exception as e:
            logger.error(f"åœæ­¢é«˜ä¼˜å…ˆçº§ä»»åŠ¡å¤„ç†çº¿ç¨‹å¤±è´¥: {e}", exc_info=True)
        
        # æ¸…ç†å¯èƒ½æ®‹ç•™çš„å­è¿›ç¨‹
        try:
            logger.info("Cleaning up potentially remaining subprocesses...")
            kill_orphaned_processes("python", "task_processor")
            kill_orphaned_processes("Python", "task_processor")
            kill_orphaned_processes("python", "high_priority_task_processor")
            kill_orphaned_processes("Python", "high_priority_task_processor")
        except Exception as cleanup_err:
            logger.error(f"æ¸…ç†æ®‹ç•™è¿›ç¨‹å¤±è´¥: {str(cleanup_err)}", exc_info=True)
        
        # åœæ­¢ MLX æœåŠ¡è¿›ç¨‹ï¼ˆå¦‚æœåœ¨è¿è¡Œï¼‰
        try:
            from utils import is_port_in_use, kill_process_on_port
            MLX_SERVICE_PORT = 60316
            if is_port_in_use(MLX_SERVICE_PORT):
                logger.info(f"Stopping MLX service process (port {MLX_SERVICE_PORT})...")
                success = kill_process_on_port(MLX_SERVICE_PORT)
                if success:
                    logger.info("MLX service process has stopped")
                else:
                    logger.warning("MLX æœåŠ¡è¿›ç¨‹åœæ­¢å¤±è´¥")
        except Exception as mlx_cleanup_err:
            logger.error(f"åœæ­¢ MLX æœåŠ¡å¤±è´¥: {str(mlx_cleanup_err)}", exc_info=True)
        
        # åœ¨åº”ç”¨å…³é—­æ—¶æ‰§è¡Œæ¸…ç†æ“ä½œ
        try:
            if hasattr(app.state, "engine") and app.state.engine is not None:
                logger.info("Releasing database connection pool...")
                app.state.engine.dispose()  # Release the database connection pool
                logger.info("Database connection pool has been released")
        except Exception as db_close_err:
            logger.error(f"å…³é—­æ•°æ®åº“è¿æ¥å¤±è´¥: {str(db_close_err)}", exc_info=True)
        
        logger.info("Application has been fully shut down")

app = FastAPI(lifespan=lifespan)
origins = [
    "http://localhost:1420",  # Your Tauri dev server
    "tauri://localhost",      # Often used by Tauri in production
    "https://tauri.localhost" # Also used by Tauri in production
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,  # Allows specific origins
    # allow_origins=["*"], # Or, to allow all origins (less secure, use with caution)
    allow_credentials=True, # Allows cookies to be included in requests
    allow_methods=["*"],    # Allows all methods (GET, POST, PUT, DELETE, etc.)
    allow_headers=["*"],    # Allows all headers
)

def get_engine():
    """FastAPIä¾èµ–å‡½æ•°ï¼Œç”¨äºè·å–æ•°æ®åº“å¼•æ“"""
    if not hasattr(app.state, "engine") or app.state.engine is None:
        # ç¡®ä¿æ•°æ®åº“å¼•æ“å·²åˆå§‹åŒ–
        raise RuntimeError("æ•°æ®åº“å¼•æ“æœªåˆå§‹åŒ–")
    return app.state.engine

# è·å– TaskManager çš„ä¾èµ–å‡½æ•°
def get_task_manager(engine: Engine = Depends(get_engine)) -> TaskManager:
    """è·å–ä»»åŠ¡ç®¡ç†å™¨å®ä¾‹"""
    return TaskManager(engine)

# ä»»åŠ¡å¤„ç†è€…
def _process_task(task: Task, lancedb_mgr, task_mgr: TaskManager, engine: Engine) -> None:
    """é€šç”¨ä»»åŠ¡å¤„ç†é€»è¾‘"""
    models_mgr = ModelsMgr(engine=engine, base_dir=app.state.db_directory)
    file_tagging_mgr = FileTaggingMgr(engine=engine, lancedb_mgr=lancedb_mgr, models_mgr=models_mgr)
    multivector_mgr = MultiVectorMgr(engine=engine, lancedb_mgr=lancedb_mgr, models_mgr=models_mgr)

    if task.task_type == TaskType.TAGGING.value:
        # æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§
        if not file_tagging_mgr.check_file_tagging_model_availability():
            logger.warning(f"æ–‡ä»¶æ‰“æ ‡ç­¾æ¨¡å‹æš‚ä¸å¯ç”¨ï¼ˆå¯èƒ½æ­£åœ¨ä¸‹è½½æˆ–åŠ è½½ä¸­ï¼‰ï¼Œä»»åŠ¡ {task.id} å°†ä¿æŒ PENDING çŠ¶æ€ç­‰å¾…é‡è¯•")
            # ä¸æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼Œä¿æŒä¸º PENDINGï¼Œè®©ä»»åŠ¡å¤„ç†çº¿ç¨‹ç¨åé‡è¯•
            # è¿™æ ·å¯ä»¥ç­‰å¾…å†…ç½®æ¨¡å‹ä¸‹è½½å’ŒåŠ è½½å®Œæˆ
            return
        
        # é«˜ä¼˜å…ˆçº§ä»»åŠ¡: å•ä¸ªæ–‡ä»¶å¤„ç†
        if task.priority == TaskPriority.HIGH.value and task.extra_data and 'screening_result_id' in task.extra_data:
            logger.info(f"Starting high-priority file tagging task (Task ID: {task.id})")
            success = file_tagging_mgr.process_single_file_task(task.extra_data['screening_result_id'])
            if success:
                task_mgr.update_task_status(task.id, TaskStatus.COMPLETED, result=TaskResult.SUCCESS)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨è¡”æ¥MULTIVECTORä»»åŠ¡ï¼ˆä»…å½“æ–‡ä»¶è¢«pinæ—¶ï¼‰
                if multivector_mgr.check_multivector_model_availability():
                    _check_and_create_multivector_task(engine, task_mgr, task.extra_data.get('screening_result_id'))
            else:
                task_mgr.update_task_status(task.id, TaskStatus.FAILED, result=TaskResult.FAILURE)
        # ä¸­ä½ä¼˜å…ˆçº§ä»»åŠ¡: æ‰¹é‡å¤„ç†
        else:
            logger.info(f"Starting batch file tagging task (Task ID: {task.id})")
            result_data = file_tagging_mgr.process_pending_batch(task_id=task.id)
            
            # æ— è®ºæ‰¹é‡ä»»åŠ¡å¤„ç†äº†å¤šå°‘æ–‡ä»¶ï¼Œéƒ½å°†è§¦å‘ä»»åŠ¡æ–‡ä»¶æ‰“æ ‡ç­¾ä¸ºå®Œæˆ
            task_mgr.update_task_status(
                task.id, 
                TaskStatus.COMPLETED, 
                result=TaskResult.SUCCESS, 
                message=f"Batch processing completed: Processed {result_data.get('processed', 0)} files."
            )
    
    elif task.task_type == TaskType.MULTIVECTOR.value:
        if not multivector_mgr.check_multivector_model_availability():
            logger.warning(f"å¤šæ¨¡æ€å‘é‡åŒ–æ¨¡å‹æš‚ä¸å¯ç”¨ï¼ˆå¯èƒ½æ­£åœ¨ä¸‹è½½æˆ–åŠ è½½ä¸­ï¼‰ï¼Œä»»åŠ¡ {task.id} å°†ä¿æŒ PENDING çŠ¶æ€ç­‰å¾…é‡è¯•")
            # ä¸æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼Œä¿æŒä¸º PENDINGï¼Œè®©ä»»åŠ¡å¤„ç†çº¿ç¨‹ç¨åé‡è¯•
            # è¿™æ ·å¯ä»¥ç­‰å¾…å†…ç½®æ¨¡å‹ä¸‹è½½å’ŒåŠ è½½å®Œæˆ
            return
        
        # é«˜ä¼˜å…ˆçº§ä»»åŠ¡: å•æ–‡ä»¶å¤„ç†ï¼ˆç”¨æˆ·pinæ“ä½œæˆ–æ–‡ä»¶å˜åŒ–è¡”æ¥ï¼‰
        if task.priority == TaskPriority.HIGH.value and task.extra_data and 'file_path' in task.extra_data:
            file_path = task.extra_data['file_path']
            logger.info(f"Starting high-priority multimodal vectorization task (Task ID: {task.id}): {file_path}")
            
            try:
                # ä¼ é€’task_idä»¥ä¾¿äº‹ä»¶è¿½è¸ª
                success = multivector_mgr.process_document(file_path, str(task.id))
                if success:
                    task_mgr.update_task_status(
                        task.id, 
                        TaskStatus.COMPLETED, 
                        result=TaskResult.SUCCESS,
                        message=f"Multimodal vectorization completed: {file_path}"
                    )
                    logger.info(f"Multimodal vectorization successfully completed: {file_path}")
                else:
                    task_mgr.update_task_status(
                        task.id, 
                        TaskStatus.FAILED, 
                        result=TaskResult.FAILURE,
                        message=f"Multimodal vectorization failed: {file_path}"
                    )
                    logger.error(f"Multimodal vectorization failed: {file_path}")
            except Exception as e:
                error_msg = f"å¤šæ¨¡æ€å‘é‡åŒ–å¼‚å¸¸: {file_path} - {str(e)}"
                task_mgr.update_task_status(
                    task.id, 
                    TaskStatus.FAILED, 
                    result=TaskResult.FAILURE,
                    message=error_msg
                )
                logger.error(error_msg, exc_info=True)
        else:
            # TODO ä¸­ä½ä¼˜å…ˆçº§ä»»åŠ¡: æ‰¹é‡å¤„ç†ï¼ˆæœªæ¥æ”¯æŒï¼‰
            logger.info(f"Other task types are not yet implemented (Task ID: {task.id})")
            task_mgr.update_task_status(
                task.id, 
                TaskStatus.COMPLETED, 
                result=TaskResult.SUCCESS,
                message="æ‰¹é‡å¤„ç†ä»»åŠ¡å·²è·³è¿‡"
            )
    
    else:
        logger.warning(f"æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task.task_type} for task ID: {task.id}")
        task_mgr.update_task_status(task.id, TaskStatus.FAILED, result=TaskResult.FAILURE, message=f"Unknown task type: {task.task_type}")


def _generic_task_processor(engine, db_directory: str, stop_event: threading.Event, processor_name: str, task_getter_func: str, sleep_duration: int = 5):
    """é€šç”¨ä»»åŠ¡å¤„ç†å™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼šç¼©çŸ­äº‹åŠ¡æŒç»­æ—¶é—´ï¼‰
    
    Args:
        engine: å…±äº«çš„SQLAlchemyå¼•æ“å®ä¾‹
        db_directory: æ•°æ®åº“ç›®å½•è·¯å¾„ï¼ˆç”¨äºLanceDBï¼‰
        stop_event: åœæ­¢äº‹ä»¶
        processor_name: å¤„ç†å™¨åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        task_getter_func: TaskManagerä¸­è·å–ä»»åŠ¡çš„æ–¹æ³•å
        sleep_duration: æ²¡æœ‰ä»»åŠ¡æ—¶çš„ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    """
    logger.info(f"{processor_name} has started")
    
    lancedb_mgr = LanceDBMgr(base_dir=db_directory)

    while not stop_event.is_set():
        task_id = None
        task_to_process = None

        try:
            # --- è·å–å¹¶é”å®šä»»åŠ¡ ---
            # è·å–ä»»åŠ¡å¹¶æ ‡è®°ä¸ºå¤„ç†ä¸­
            try:
                task_mgr = TaskManager(engine=engine)
                task_getter = getattr(task_mgr, task_getter_func)
                locked_task: Task = task_getter()

                if locked_task:
                    task_id = locked_task.id
                    # åˆ›å»ºä¸€ä¸ªä»»åŠ¡çš„éæ‰˜ç®¡å‰¯æœ¬ï¼Œä»¥ä¾¿åœ¨ä¼šè¯å…³é—­åä½¿ç”¨
                    task_to_process = {
                        "id": locked_task.id,
                        "task_name": locked_task.task_name,
                        "task_type": locked_task.task_type,
                        "priority": locked_task.priority,
                        "extra_data": locked_task.extra_data,
                    }
                    logger.info(f"{processor_name} has locked the task: ID={task_id}")
                else:
                    # æ²¡æœ‰ä»»åŠ¡ï¼Œç›´æ¥ç»“æŸæœ¬æ¬¡å¾ªç¯
                    pass
            except Exception as e:
                logger.error(f"{processor_name}åœ¨è·å–ä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)

            # --- å¦‚æœæ²¡æœ‰ä»»åŠ¡ï¼Œåˆ™ä¼‘çœ å¹¶ç»§ç»­ ---
            if not task_to_process:
                time.sleep(sleep_duration)
                continue

            # --- æ‰§è¡Œè€—æ—¶æ“ä½œ ---
            logger.info(f"{processor_name} started processing task: ID={task_id}, Name='{task_to_process['task_name']}'")
            try:
                task_mgr_for_processing = TaskManager(engine=engine)
                
                # ä»å­—å…¸é‡å»ºTaskå¯¹è±¡ï¼Œæˆ–ä»æ•°æ®åº“é‡æ–°è·å–
                task_obj_for_processing = task_mgr_for_processing.get_task(task_id)
                if not task_obj_for_processing:
                    raise ValueError(f"ä»»åŠ¡ {task_id} åœ¨å¤„ç†å‰æ¶ˆå¤±")

                # è°ƒç”¨åŸå§‹çš„ä»»åŠ¡å¤„ç†é€»è¾‘ï¼Œä½†ç°åœ¨å®ƒåœ¨ä¸€ä¸ªç‹¬ç«‹çš„ä¼šè¯ä¸­è¿è¡Œ
                # è¿™ä¸ªä¼šè¯ä»ç„¶å¯èƒ½é•¿æ—¶é—´è¿è¡Œï¼Œä½†å®ƒä¸åº”è¯¥æŒæœ‰å¯¹taskè¡¨çš„å†™é”
                _process_task(task=task_obj_for_processing, lancedb_mgr=lancedb_mgr, task_mgr=task_mgr_for_processing, engine=engine)
                
                
                # --- äº‹åŠ¡ä¸‰: æ›´æ–°æœ€ç»ˆç»“æœ ---
                # ä»»åŠ¡æˆåŠŸå®Œæˆ
                task_mgr_final = TaskManager(engine=engine)
                task_mgr_final.update_task_status(task_id, TaskStatus.COMPLETED, result=TaskResult.SUCCESS)
                logger.info(f"{processor_name} successfully completed the task: ID={task_id}")

            except Exception as task_error:
                logger.error(f"{processor_name}å¤„ç†ä»»åŠ¡ {task_id} æ—¶å‘ç”Ÿé”™è¯¯: {task_error}", exc_info=True)
                # --- äº‹åŠ¡ä¸‰ (å¤±è´¥æƒ…å†µ): æ›´æ–°æœ€ç»ˆç»“æœ ---
                task_mgr_final = TaskManager(engine=engine)
                task_mgr_final.update_task_status(task_id, TaskStatus.FAILED, result=TaskResult.FAILURE, message=str(task_error))
                logger.warning(f"{processor_name}ä»»åŠ¡å¤±è´¥: ID={task_id}")

        except Exception as e:
            logger.error(f"{processor_name}å‘ç”Ÿæ„å¤–çš„é¡¶å±‚é”™è¯¯: {e}", exc_info=True)
            # å¦‚æœåœ¨è·å–ä»»åŠ¡IDåå‘ç”ŸæœªçŸ¥é”™è¯¯ï¼Œä¹Ÿå°è¯•æ ‡è®°ä»»åŠ¡å¤±è´¥
            if task_id:
                try:
                    task_mgr_final = TaskManager(engine=engine)
                    task_mgr_final.update_task_status(task_id, TaskStatus.FAILED, result=TaskResult.FAILURE, message=f"å¤„ç†å™¨é¡¶å±‚é”™è¯¯: {e}")
                except Exception as final_update_error:
                    logger.error(f"å°è¯•æ ‡è®°ä»»åŠ¡ {task_id} å¤±è´¥æ—¶å†æ¬¡å‡ºé”™: {final_update_error}", exc_info=True)
            time.sleep(30) # å‘ç”Ÿä¸¥é‡é”™è¯¯æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´

    logger.info(f"{processor_name} is stopping as requested")


def task_processor(engine, db_directory: str, stop_event: threading.Event):
    """æ™®é€šä»»åŠ¡å¤„ç†çº¿ç¨‹å·¥ä½œå‡½æ•°ï¼ˆå¤„ç†æ‰€æœ‰ä¼˜å…ˆçº§ä»»åŠ¡ï¼‰"""
    _generic_task_processor(
        engine=engine,
        db_directory=db_directory,
        stop_event=stop_event,
        processor_name="General Task Processing Thread",
        task_getter_func="get_and_lock_next_task",
        sleep_duration=5
    )


def high_priority_task_processor(engine, db_directory: str, stop_event: threading.Event):
    """é«˜ä¼˜å…ˆçº§ä»»åŠ¡å¤„ç†çº¿ç¨‹å·¥ä½œå‡½æ•°ï¼ˆä»…å¤„ç†HIGHä¼˜å…ˆçº§ä»»åŠ¡ï¼‰"""
    _generic_task_processor(
        engine=engine,
        db_directory=db_directory,
        stop_event=stop_event,
        processor_name="High-Priority Task Processing Thread",
        task_getter_func="get_and_lock_next_high_priority_task",
        sleep_duration=2
    )

async def mlx_service_monitor(engine: Engine, base_dir: str, stop_event: asyncio.Event):
    """
    MLX æœåŠ¡ç›‘æ§ä»»åŠ¡ï¼ˆsupervisord å¼çš„è¿›ç¨‹ç®¡ç†ï¼‰
    
    åŠŸèƒ½ï¼š
    - å®šæœŸæ£€æŸ¥ MLX æœåŠ¡ï¼ˆ60316 ç«¯å£ï¼‰æ˜¯å¦åœ¨è¿è¡Œ
    - å¦‚æœé…ç½®éœ€è¦ä½†æœåŠ¡å´©æºƒï¼Œè‡ªåŠ¨é‡å¯
    - å®ç°æŒ‡æ•°é€€é¿ç­–ç•¥ï¼Œé˜²æ­¢é¢‘ç¹é‡å¯å¯¼è‡´çš„å¯åŠ¨é£æš´
    
    Args:
        engine: æ•°æ®åº“å¼•æ“
        base_dir: åº”ç”¨æ•°æ®ç›®å½•
        stop_event: åœæ­¢ä¿¡å·äº‹ä»¶
    """
    import asyncio
    from utils import is_port_in_use
    from models_builtin import ModelsBuiltin
    
    logger.info("ğŸ” MLX service monitor started")
    
    # é‡å¯ç»Ÿè®¡
    restart_count = 0
    last_restart_time = 0.0
    total_restarts = 0
    
    # ç›‘æ§é…ç½®
    CHECK_INTERVAL = 10  # æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
    RESTART_COOLDOWN = 60  # é‡å¯å†·å´æ—¶é—´ï¼ˆç§’ï¼‰
    MAX_RESTART_ATTEMPTS = 5  # å•ä½æ—¶é—´å†…æœ€å¤§é‡å¯æ¬¡æ•°
    
    builtin_mgr = ModelsBuiltin(engine=engine, base_dir=base_dir)
    
    while not stop_event.is_set():
        try:
            # ç­‰å¾…æ£€æŸ¥é—´éš”æˆ–åœæ­¢ä¿¡å·
            try:
                await asyncio.wait_for(stop_event.wait(), timeout=CHECK_INTERVAL)
                # å¦‚æœç­‰å¾…æˆåŠŸï¼Œè¯´æ˜æ”¶åˆ°åœæ­¢ä¿¡å·
                break
            except asyncio.TimeoutError:
                # è¶…æ—¶æ˜¯æ­£å¸¸çš„ï¼Œç»§ç»­æ£€æŸ¥
                pass
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ MLX æœåŠ¡
            should_run, model_id = builtin_mgr.should_auto_load(base_dir=base_dir)
            
            if not should_run:
                # ä¸éœ€è¦è¿è¡Œï¼Œè·³è¿‡æ£€æŸ¥
                # logger.debug("MLX service not required, skipping check")
                continue
            
            # æ£€æŸ¥ç«¯å£æ˜¯å¦åœ¨ä½¿ç”¨
            if not is_port_in_use(60316):
                # æœåŠ¡å´©æºƒäº†ï¼
                logger.warning(f"âš ï¸  MLX service is DOWN (port 60316 not in use), model: {model_id}")
                
                # æ£€æŸ¥é‡å¯é¢‘ç‡
                current_time = time.time()
                time_since_last_restart = current_time - last_restart_time
                
                if time_since_last_restart < RESTART_COOLDOWN:
                    # åœ¨å†·å´æ—¶é—´å†…ï¼Œå¢åŠ é‡å¯è®¡æ•°
                    restart_count += 1
                    
                    if restart_count >= MAX_RESTART_ATTEMPTS:
                        # é‡å¯æ¬¡æ•°è¿‡å¤šï¼Œä½¿ç”¨æ›´é•¿çš„é€€é¿æ—¶é—´
                        backoff_time = min(2 ** (restart_count - MAX_RESTART_ATTEMPTS + 1), 300)
                        logger.error(
                            f"ğŸš¨ MLX service crashed {restart_count} times in {RESTART_COOLDOWN}s! "
                            f"Backing off for {backoff_time}s before retry."
                        )
                        await asyncio.sleep(backoff_time)
                    else:
                        # çŸ­æš‚ç­‰å¾…åé‡è¯•
                        logger.info(f"â³ Waiting 5s before restart attempt #{restart_count}...")
                        await asyncio.sleep(5)
                else:
                    # è¶…è¿‡å†·å´æ—¶é—´ï¼Œé‡ç½®è®¡æ•°å™¨
                    restart_count = 1
                
                # å°è¯•é‡å¯æœåŠ¡
                logger.info(f"ğŸ”„ Attempting to restart MLX service (model: {model_id}, attempt #{restart_count})...")
                
                try:
                    success = builtin_mgr._start_mlx_service_process()
                    
                    if success:
                        total_restarts += 1
                        last_restart_time = current_time
                        logger.info(
                            f"âœ… MLX service restarted successfully (total restarts: {total_restarts})"
                        )
                        
                        # é‡å¯æˆåŠŸåï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´å†æ£€æŸ¥ï¼Œç»™æœåŠ¡å¯åŠ¨æ—¶é—´
                        await asyncio.sleep(10)
                        
                        # éªŒè¯æœåŠ¡æ˜¯å¦çœŸçš„èµ·æ¥äº†
                        if is_port_in_use(60316):
                            logger.info("âœ… MLX service confirmed running after restart")
                            # æˆåŠŸé‡å¯åï¼Œå¯ä»¥éƒ¨åˆ†é‡ç½®è®¡æ•°å™¨ï¼ˆä½†ä¸å®Œå…¨æ¸…é›¶ï¼‰
                            restart_count = max(0, restart_count - 1)
                        else:
                            logger.error("âŒ MLX service failed to start (port still not in use)")
                    else:
                        logger.error("âŒ Failed to restart MLX service (startup function returned False)")
                        
                except Exception as restart_err:
                    logger.error(f"âŒ Exception during MLX service restart: {restart_err}", exc_info=True)
            else:
                # æœåŠ¡æ­£å¸¸è¿è¡Œ
                # logger.debug("âœ… MLX service is running normally")
                
                # å¦‚æœä¹‹å‰æœ‰é‡å¯è¿‡ï¼Œé‡ç½®è®¡æ•°å™¨ï¼ˆæœåŠ¡å·²ç¨³å®šè¿è¡Œï¼‰
                if restart_count > 0:
                    current_time = time.time()
                    if current_time - last_restart_time > RESTART_COOLDOWN * 2:
                        # æœåŠ¡å·²ç¨³å®šè¿è¡Œè¶…è¿‡2å€å†·å´æ—¶é—´ï¼Œé‡ç½®è®¡æ•°å™¨
                        restart_count = 0
                        logger.info("âœ… MLX service stabilized, reset restart counter")
        
        except Exception as e:
            logger.error(f"âŒ Error in MLX service monitor: {e}", exc_info=True)
            # å‘ç”Ÿé”™è¯¯åç­‰å¾…ä¸€æ®µæ—¶é—´å†ç»§ç»­
            await asyncio.sleep(30)
    
    logger.info(f"ğŸ” MLX service monitor stopped (total restarts during session: {total_restarts})")

def _check_and_create_multivector_task(engine: Engine, task_mgr: TaskManager, screening_result_id: int):
    """
    æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å¤„äºpinçŠ¶æ€ï¼Œå¦‚æœæ˜¯åˆ™è‡ªåŠ¨åˆ›å»ºMULTIVECTORä»»åŠ¡
    
    Args:
        engine: æ•°æ®åº“å¼•æ“
        task_mgr: ä»»åŠ¡ç®¡ç†å™¨
        screening_result_id: ç²—ç­›ç»“æœID
    """
    if not screening_result_id:
        return
    
    try:
        with Session(bind=engine) as session:
            # è·å–ç²—ç­›ç»“æœï¼ŒåŒ…å«æ–‡ä»¶è·¯å¾„ä¿¡æ¯
            screening_result = session.get(FileScreeningResult, screening_result_id)
            if not screening_result:
                logger.warning(f"æœªæ‰¾åˆ°screening_result_id: {screening_result_id}")
                return
            
            file_path = screening_result.file_path
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨æœ€è¿‘24å°æ—¶å†…è¢«pinè¿‡
            is_recently_pinned = _check_file_pin_status(file_path, task_mgr)
            
            if is_recently_pinned:
                logger.info(f"File {file_path} has been pinned in the last 24 hours, creating MULTIVECTOR task")
                task_mgr.add_task(
                    task_name=f"Multimodal Vectorization: {Path(file_path).name}",
                    task_type=TaskType.MULTIVECTOR,
                    priority=TaskPriority.HIGH,
                    extra_data={"file_path": file_path},
                    target_file_path=file_path  # Set redundant field for easier querying
                )
            else:
                logger.info(f"File {file_path} has not been pinned in the last 8 hours, skipping MULTIVECTOR task")
            
    except Exception as e:
        logger.error(f"æ£€æŸ¥å’Œåˆ›å»ºMULTIVECTORä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)

def _check_file_pin_status(file_path: str, task_mgr: TaskManager = Depends(get_task_manager)) -> bool:
    """
    æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨æœ€è¿‘24å°æ—¶å†…è¢«pinè¿‡ï¼ˆå³æœ‰æˆåŠŸçš„MULTIVECTORä»»åŠ¡ï¼‰
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        task_mgr: ä»»åŠ¡ç®¡ç†å™¨å®ä¾‹
        
    Returns:
        bool: æ–‡ä»¶æ˜¯å¦åœ¨æœ€è¿‘24å°æ—¶å†…è¢«pinè¿‡
    """
    try:
        return task_mgr.is_file_recently_pinned(file_path, hours=24)
    except Exception as e:
        logger.error(f"æ£€æŸ¥æ–‡ä»¶pinçŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return False

@app.get("/task/{task_id}")
def get_task_status(task_id: int, task_mgr: TaskManager = Depends(get_task_manager)):
    """
    è·å–ä»»åŠ¡çŠ¶æ€
    
    å‚æ•°:
    - task_id: ä»»åŠ¡ID
    
    è¿”å›:
    - ä»»åŠ¡è¯¦ç»†ä¿¡æ¯
    """
    try:
        task = task_mgr.get_task(task_id)
        if not task:
            return {"success": False, "error": f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}"}
        
        return {
            "success": True,
            "task": {
                "id": task.id,
                "task_name": task.task_name,
                "task_type": task.task_type,
                "priority": task.priority,
                "status": task.status,
                "result": task.result,
                "error_message": task.error_message,
                "created_at": task.created_at,
                "updated_at": task.updated_at,
                "start_time": task.start_time,
                "extra_data": task.extra_data,
                "target_file_path": task.target_file_path
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return {"success": False, "error": f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}"}

@app.get("/")
def read_root():
    # ç°åœ¨å¯ä»¥åœ¨ä»»ä½•è·¯ç”±ä¸­ä½¿ç”¨ app.state.db_path
    return {
        "Success": True,
        "message": "APIæœåŠ¡è¿è¡Œä¸­",
        "db_path": app.state.db_path,
        "db_pool_status": str(app.state.engine.pool.status()) if hasattr(app.state, "engine") and app.state.engine else "N/A"
        }

# æ·»åŠ å¥åº·æ£€æŸ¥ç«¯ç‚¹
@app.get("/health")
def health_check():
    """APIå¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼Œç”¨äºéªŒè¯APIæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ"""
    return {
        "status": "ok", 
        "timestamp": datetime.now().isoformat(),
    }

@app.get("/system-config/{config_key}")
def get_system_config(config_key: str, engine: Engine = Depends(get_engine)):
    """è·å–ç³»ç»Ÿé…ç½®
    
    å‚æ•°:
    - config_key: é…ç½®é”®å
    
    è¿”å›:
    - é…ç½®å€¼å’Œæè¿°ä¿¡æ¯
    """
    try:
        with Session(bind=engine) as session:
            config = session.exec(select(SystemConfig).where(SystemConfig.key == config_key)).first()
            if not config:
                return {"success": False, "error": f"é…ç½®é¡¹ '{config_key}' ä¸å­˜åœ¨"}
            
            return {
                "success": True,
                "config": {
                    "key": config.key,
                    "value": config.value,
                    "description": config.description,
                    "updated_at": config.updated_at
                }
            }
        
    except Exception as e:
        logger.error(f"è·å–ç³»ç»Ÿé…ç½®æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return {"success": False, "error": f"è·å–é…ç½®å¤±è´¥: {str(e)}"}

@app.put("/system-config/{config_key}")
def update_system_config(
    config_key: str, 
    data: Dict[str, Any] = Body(...),
    engine: Engine = Depends(get_engine),
):
    """æ›´æ–°ç³»ç»Ÿé…ç½®
    
    å‚æ•°:
    - config_key: é…ç½®é”®å
    
    è¯·æ±‚ä½“:
    - value: æ–°çš„é…ç½®å€¼
    
    è¿”å›:
    - æ›´æ–°ç»“æœ
    """
    try:
        new_value = data.get("value", "")
        with Session(bind=engine) as session:
            config = session.exec(select(SystemConfig).where(SystemConfig.key == config_key)).first()
            if not config:
                return {"success": False, "error": f"Configuration item '{config_key}' does not exist"}
            
            # æ›´æ–°é…ç½®å€¼å’Œæ—¶é—´æˆ³
            config.value = new_value
            config.updated_at = datetime.now()
            
            session.add(config)
            session.commit()
            
            logger.info(f"System configuration '{config_key}' has been updated to: {new_value}")
            
            return {
                "success": True,
                "message": f"Configuration item '{config_key}' updated successfully",
                "config": {
                    "key": config.key,
                    "value": config.value,
                    "description": config.description,
                    "updated_at": config.updated_at
                }
            }
        
    except Exception as e:
        logger.error(f"æ›´æ–°ç³»ç»Ÿé…ç½®æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return {"success": False, "error": f"æ›´æ–°é…ç½®å¤±è´¥: {str(e)}"}

@app.post("/pin-file")
async def pin_file(
    data: Dict[str, Any] = Body(...),
    task_mgr: TaskManager = Depends(get_task_manager),
    engine: Engine = Depends(get_engine),
):
    """Pinæ–‡ä»¶å¹¶åˆ›å»ºå¤šæ¨¡æ€å‘é‡åŒ–ä»»åŠ¡
    
    ç”¨æˆ·pinæ–‡ä»¶æ—¶è°ƒç”¨æ­¤ç«¯ç‚¹ï¼Œç«‹å³åˆ›å»ºHIGHä¼˜å…ˆçº§çš„MULTIVECTORä»»åŠ¡
    
    è¯·æ±‚ä½“:
    - file_path: è¦pinçš„æ–‡ä»¶ç»å¯¹è·¯å¾„
    
    è¿”å›:
    - success: æ“ä½œæ˜¯å¦æˆåŠŸ
    - task_id: åˆ›å»ºçš„ä»»åŠ¡ID
    - message: æ“ä½œç»“æœæ¶ˆæ¯
    """
    try:
        file_path = data.get("file_path")
        
        if not file_path:
            logger.warning("Pinæ–‡ä»¶è¯·æ±‚ä¸­æœªæä¾›æ–‡ä»¶è·¯å¾„")
            return {
                "success": False,
                "task_id": None,
                "message": "æ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º"
            }
        
        # éªŒè¯æ–‡ä»¶è·¯å¾„å’Œæƒé™
        if not os.path.exists(file_path):
            logger.warning(f"Pinæ–‡ä»¶å¤±è´¥ï¼Œæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return {
                "success": False,
                "task_id": None,
                "message": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
            }
        
        if not os.access(file_path, os.R_OK):
            logger.warning(f"Pinæ–‡ä»¶å¤±è´¥ï¼Œæ–‡ä»¶æ— è¯»å–æƒé™: {file_path}")
            return {
                "success": False,
                "task_id": None,
                "message": f"æ–‡ä»¶æ— è¯»å–æƒé™: {file_path}"
            }
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹æ˜¯å¦æ”¯æŒ
        from multivector_mgr import SUPPORTED_FORMATS
        file_ext = Path(file_path).suffix.split('.')[-1].lower()
        if file_ext not in SUPPORTED_FORMATS:
            logger.warning(f"Pinæ–‡ä»¶å¤±è´¥ï¼Œä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")
            return {
                "success": False,
                "task_id": None,
                "message": f"Unsupported file type: {file_ext}. Supported types: {SUPPORTED_FORMATS}"
            }

        # åœ¨åˆ›å»ºä»»åŠ¡å‰æ£€æŸ¥å¤šæ¨¡æ€å‘é‡åŒ–æ‰€éœ€çš„æ¨¡å‹é…ç½®
        lancedb_mgr = LanceDBMgr(base_dir=app.state.db_directory)
        models_mgr = ModelsMgr(engine=engine, base_dir=app.state.db_directory)
        multivector_mgr = MultiVectorMgr(engine=engine, lancedb_mgr=lancedb_mgr, models_mgr=models_mgr)
        
        # æ£€æŸ¥å¤šæ¨¡æ€å‘é‡åŒ–æ‰€éœ€çš„æ¨¡å‹æ˜¯å¦å·²é…ç½®
        if not multivector_mgr.check_multivector_model_availability():
            logger.warning(f"Pinæ–‡ä»¶å¤±è´¥ï¼Œå¤šæ¨¡æ€å‘é‡åŒ–æ‰€éœ€çš„æ¨¡å‹é…ç½®ç¼ºå¤±: {file_path}")
            return {
                "success": False,
                "task_id": None,
                "error_type": "model_missing",
                "message": "Multimodal vectorization requires configuration of text and vision models. Please go to the settings page to configure them.",
                "missing_models": ["text", "vision"]
            }

        # åˆ›å»ºHIGHä¼˜å…ˆçº§MULTIVECTORä»»åŠ¡
        task = task_mgr.add_task(
            task_name=f"Pinæ–‡ä»¶å¤šæ¨¡æ€å‘é‡åŒ–: {Path(file_path).name}",
            task_type=TaskType.MULTIVECTOR,
            priority=TaskPriority.HIGH,
            extra_data={"file_path": file_path}
        )
        
        logger.info(f"Successfully created a multimodal vectorization task for the pinned file: {file_path} (Task ID: {task.id})")
        
        return {
            "success": True,
            "task_id": task.id,
            "message": f"Multimodal vectorization task created successfully, Task ID: {task.id}"
        }
        
    except Exception as e:
        logger.error(f"Pinæ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "success": False,
            "task_id": None,
            "message": f"Pinæ–‡ä»¶å¤±è´¥: {str(e)}"
        }

@app.get("/test-bridge-stdout")
def test_bridge_stdout():
    """æµ‹è¯•æ¡¥æ¥äº‹ä»¶çš„stdoutè¾“å‡ºèƒ½åŠ›"""
    from test_bridge_stdout import test_bridge_stdout_main
    test_bridge_stdout_main()
    return {"status": "ok"}


def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å™¨ï¼Œç”¨äºä¼˜é›…å…³é—­"""
    print(f"æ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œå¼€å§‹ä¼˜é›…å…³é—­...")
    # æ¸…ç†å¯èƒ½æ®‹ç•™çš„å­è¿›ç¨‹
    try:
        kill_orphaned_processes("python", "task_processor")
        kill_orphaned_processes("Python", "task_processor")
        kill_orphaned_processes("python", "high_priority_task_processor")
        kill_orphaned_processes("Python", "high_priority_task_processor")
    except Exception as e:
        print(f"ä¿¡å·å¤„ç†å™¨æ¸…ç†è¿›ç¨‹å¤±è´¥: {e}")
    sys.exit(0)

if __name__ == "__main__":
    try:
        # æ³¨å†Œä¿¡å·å¤„ç†å™¨
        signal.signal(signal.SIGTERM, signal_handler)
        signal.signal(signal.SIGINT, signal_handler)
        
        parser = argparse.ArgumentParser()
        parser.add_argument("--port", type=int, default=60315, help="APIæœåŠ¡ç›‘å¬ç«¯å£")
        parser.add_argument("--host", type=str, default="127.0.0.1", help="APIæœåŠ¡ç›‘å¬åœ°å€")
        parser.add_argument("--db-path", type=str, default="knowledge-focus.db", help="æ•°æ®åº“æ–‡ä»¶è·¯å¾„")
        args = parser.parse_args()

        print("APIæœåŠ¡ç¨‹åºå¯åŠ¨")
        print(f"å‘½ä»¤è¡Œå‚æ•°: port={args.port}, host={args.host}, db_path={args.db_path}")

        # æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨ï¼Œå¦‚æœè¢«å ç”¨åˆ™ç»ˆæ­¢å ç”¨è¿›ç¨‹
        try:
            print(f"æ£€æŸ¥ç«¯å£ {args.port} æ˜¯å¦è¢«å ç”¨...")
            kill_process_on_port(args.port)
            time.sleep(2)  # ç­‰å¾…ç«¯å£é‡Šæ”¾
            print(f"ç«¯å£ {args.port} å·²é‡Šæ”¾æˆ–æœ¬æ¥å°±æ²¡è¢«å ç”¨")
        except Exception as e:
            print(f"é‡Šæ”¾ç«¯å£ {args.port} å¤±è´¥: {str(e)}")
            # ç»§ç»­æ‰§è¡Œï¼Œç«¯å£å¯èƒ½æœ¬æ¥å°±æ²¡æœ‰è¢«å ç”¨
        
        # è®¾ç½®æ•°æ®åº“è·¯å¾„
        app.state.db_path = args.db_path
        print(f"è®¾ç½®æ•°æ®åº“è·¯å¾„: {args.db_path}")
        # å¯åŠ¨æœåŠ¡å™¨
        print(f"APIæœåŠ¡å¯åŠ¨åœ¨: http://{args.host}:{args.port}")
        # é…ç½®uvicornæ—¥å¿—ï¼Œé˜²æ­¢è¦†ç›–æˆ‘ä»¬çš„æ—¥å¿—é…ç½®
        uvicorn.run(
            app, 
            host=args.host, 
            port=args.port, 
            log_level="info",
            access_log=False,  # ç¦ç”¨uvicornçš„è®¿é—®æ—¥å¿—ï¼Œä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„
            use_colors=False   # ç¦ç”¨é¢œè‰²è¾“å‡ºï¼Œä¿æŒæ—¥å¿—æ–‡ä»¶çš„æ•´æ´
        )
    
    except Exception as e:
        print(f"APIæœåŠ¡å¯åŠ¨å¤±è´¥: {str(e)}")

        # è¿”å›é€€å‡ºç 2ï¼Œè¡¨ç¤ºå‘ç”Ÿé”™è¯¯
        sys.exit(2)

import os
import sys
import argparse
import logging
import time
import pathlib
import threading
import json
from datetime import datetime
from typing import Dict, Any
from pathlib import Path
from contextlib import asynccontextmanager
from fastapi import FastAPI, Body, Depends
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from utils import kill_process_on_port, monitor_parent, kill_orphaned_processes
from sqlmodel import create_engine, Session
from db_mgr import (
    DBManager, TaskStatus, TaskResult, TaskType, TaskPriority, Task,
)
from screening_mgr import ScreeningManager
from file_tagging_mgr import FileTaggingMgr
from task_mgr import TaskManager
from lancedb_mgr import LanceDBMgr
from models_mgr import ModelsMgr
from search_mgr import SearchManager
from multivector_mgr import MultiVectorMgr
from models_api import get_router as get_models_router
from tagging_api import get_router as get_tagging_router
from chatsession_api import get_router as get_chatsession_router
from myfiles_api import get_router as get_myfiles_router

# --- Centralized Logging Setup ---
def setup_logging(logging_dir: str = None):
    """
    Configures the root logger for the application.

    args:
        logging_dir (str): The directory where log files will be stored.
    """
    
    try:
        # Determine log directory
        if logging_dir is not None:
            log_dir = pathlib.Path(logging_dir)
        else:
            script_path = os.path.abspath(__file__)
            log_dir = pathlib.Path(script_path).parent / 'logs'
        if not log_dir.exists():
            log_dir.mkdir(exist_ok=True, parents=True)

        log_filename = f'api_{time.strftime("%Y%m%d")}.log'
        log_filepath = log_dir / log_filename

        # Get the root logger and configure it
        root_logger = logging.getLogger()
        root_logger.setLevel(logging.INFO)

        # Avoid adding duplicate handlers
        if not root_logger.handlers:
            # Console handler
            console_handler = logging.StreamHandler(sys.stdout)
            console_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            console_handler.setFormatter(console_formatter)
            root_logger.addHandler(console_handler)

            # File handler
            file_handler = logging.FileHandler(log_filepath, encoding='utf-8')
            file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            file_handler.setFormatter(file_formatter)
            root_logger.addHandler(file_handler)

    except Exception as e:
        # Fallback to basic config if setup fails
        logging.basicConfig(level=logging.INFO)
        logging.error(f"Error setting up custom logging: {e}", exc_info=True)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨"""
    # åœ¨åº”ç”¨å¯åŠ¨æ—¶æ‰§è¡Œåˆå§‹åŒ–æ“ä½œ
    logger.info("åº”ç”¨æ­£åœ¨å¯åŠ¨...")
    
    try:
        logger.info(f"è°ƒè¯•ä¿¡æ¯: Pythonç‰ˆæœ¬ {sys.version}")
        logger.info(f"è°ƒè¯•ä¿¡æ¯: å½“å‰å·¥ä½œç›®å½• {os.getcwd()}")
        
        # åˆå§‹åŒ–æ•°æ®åº“å¼•æ“‹
        if hasattr(app.state, "db_path"):
            sqlite_url = f"sqlite:///{app.state.db_path}"
            logger.info(f"åˆå§‹åŒ–æ•°æ®åº“å¼•æ“ï¼ŒURL: {sqlite_url}")
            try:
                # For SQLite, especially when accessed by FastAPI (which can use threads for async routes)
                # and potentially by background tasks, 'check_same_thread': False is often needed.
                # The set_sqlite_pragma event listener will configure WAL mode.
                app.state.engine = create_engine(
                    sqlite_url, 
                    echo=False, 
                    connect_args={"check_same_thread": False, "timeout": 30},
                    pool_size=5,       # è®¾ç½®è¿æ¥æ± å¤§å°
                    max_overflow=10,   # å…è®¸çš„æœ€å¤§æº¢å‡ºè¿æ¥æ•°
                    pool_timeout=30,   # è·å–è¿æ¥çš„è¶…æ—¶æ—¶é—´
                    pool_recycle=1800  # 30åˆ†é’Ÿå›æ”¶ä¸€æ¬¡è¿æ¥
                )
                logger.info(f"æ•°æ®åº“å¼•æ“å·²åˆå§‹åŒ–ï¼Œè·¯å¾„: {app.state.db_path}")
                
                # åˆå§‹åŒ–æ•°æ®åº“ç»“æ„
                try:
                    logger.info("å¼€å§‹åˆå§‹åŒ–æ•°æ®åº“ç»“æ„...")
                    with Session(app.state.engine) as session:
                        db_mgr = DBManager(session)
                        db_mgr.init_db()
                    logger.info("æ•°æ®åº“ç»“æ„åˆå§‹åŒ–å®Œæˆ")
                except Exception as init_err:
                    logger.error(f"åˆå§‹åŒ–æ•°æ®åº“ç»“æ„å¤±è´¥: {str(init_err)}", exc_info=True)
                    # ç»§ç»­è¿è¡Œåº”ç”¨ï¼Œä¸è¦å› ä¸ºåˆå§‹åŒ–å¤±è´¥è€Œä¸­æ–­
                    # å¯èƒ½æ˜¯å› ä¸ºè¡¨å·²ç»å­˜åœ¨ï¼Œè¿™ç§æƒ…å†µæ˜¯æ­£å¸¸çš„
            except Exception as db_err:
                logger.error(f"åˆå§‹åŒ–æ•°æ®åº“å¼•æ“å¤±è´¥: {str(db_err)}", exc_info=True)
                raise
        else:
            logger.warning("æœªè®¾ç½®æ•°æ®åº“è·¯å¾„ï¼Œæ•°æ®åº“å¼•æ“æœªåˆå§‹åŒ–")
        
        # å…ˆæ¸…ç†å¯èƒ½å­˜åœ¨çš„å­¤ç«‹å­è¿›ç¨‹
        try:
            logger.info("æ¸…ç†å¯èƒ½å­˜åœ¨çš„å­¤ç«‹å­è¿›ç¨‹...")
            kill_orphaned_processes("python", "task_processor")
        except Exception as proc_err:
            logger.error(f"æ¸…ç†å­¤ç«‹è¿›ç¨‹å¤±è´¥: {str(proc_err)}", exc_info=True)
        
        # åˆå§‹åŒ–åå°ä»»åŠ¡å¤„ç†çº¿ç¨‹
        try:
            logger.info("åˆå§‹åŒ–åå°ä»»åŠ¡å¤„ç†çº¿ç¨‹...")
            # åˆ›å»ºä¸€ä¸ªäº‹ä»¶æ¥ä¼˜é›…åœ°åœæ­¢çº¿ç¨‹
            app.state.task_processor_stop_event = threading.Event()
            app.state.task_processor_thread = threading.Thread(
                target=task_processor,
                args=(app.state.db_path, app.state.task_processor_stop_event),
                daemon=True
            )
            app.state.task_processor_thread.start()
            logger.info("åå°ä»»åŠ¡å¤„ç†çº¿ç¨‹å·²å¯åŠ¨")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–åå°ä»»åŠ¡å¤„ç†çº¿ç¨‹å¤±è´¥: {e}", exc_info=True)
            raise
        
        # å¯åŠ¨é€šçŸ¥æ£€æŸ¥ä»»åŠ¡
        # try:
        #     logger.info("å¯åŠ¨é€šçŸ¥æ£€æŸ¥ä»»åŠ¡...")
        #     asyncio.create_task(check_notifications())
        # except Exception as notify_err:
        #     logger.error(f"å¯åŠ¨é€šçŸ¥æ£€æŸ¥ä»»åŠ¡å¤±è´¥: {str(notify_err)}", exc_info=True)
            
        # Start monitor can kill self process if parent process is dead or exit
        try:
            logger.info("å¯åŠ¨çˆ¶è¿›ç¨‹ç›‘æ§çº¿ç¨‹...")
            monitor_thread = threading.Thread(target=monitor_parent, daemon=True)
            monitor_thread.start()
            logger.info("çˆ¶è¿›ç¨‹ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨")
        except Exception as monitor_err:
            logger.error(f"å¯åŠ¨çˆ¶è¿›ç¨‹ç›‘æ§çº¿ç¨‹å¤±è´¥: {str(monitor_err)}", exc_info=True)

        # é…ç½®è§£æåº“çš„è­¦å‘Šå’Œæ—¥å¿—çº§åˆ«
        try:
            from file_tagging_mgr import configure_parsing_warnings
            configure_parsing_warnings()
            logger.info("è§£æåº“æ—¥å¿—é…ç½®å·²åº”ç”¨")
        except Exception as parsing_config_err:
            logger.error(f"é…ç½®è§£æåº“æ—¥å¿—å¤±è´¥: {str(parsing_config_err)}", exc_info=True)

        # æ­£å¼å¼€å§‹æœåŠ¡
        logger.info("åº”ç”¨åˆå§‹åŒ–å®Œæˆï¼Œå¼€å§‹æä¾›æœåŠ¡...")
        yield
    except Exception as e:
        logger.critical(f"åº”ç”¨å¯åŠ¨è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}", exc_info=True)
        # ç¡®ä¿å¼‚å¸¸ä¼ æ’­ï¼Œè¿™æ ·FastAPIä¼šçŸ¥é“å¯åŠ¨å¤±è´¥
        raise
    finally:
        # é€€å‡ºå‰çš„æ¸…ç†å·¥ä½œ
        logger.info("åº”ç”¨å¼€å§‹å…³é—­...")
        
        try:
            if hasattr(app.state, "task_processor_thread") and app.state.task_processor_thread.is_alive():
                logger.info("æ­£åœ¨åœæ­¢åå°ä»»åŠ¡å¤„ç†çº¿ç¨‹...")
                app.state.task_processor_stop_event.set()
                app.state.task_processor_thread.join(timeout=5) # ç­‰å¾…5ç§’
                if app.state.task_processor_thread.is_alive():
                    logger.warning("åå°ä»»åŠ¡å¤„ç†çº¿ç¨‹åœ¨5ç§’å†…æœªåœæ­¢")
                else:
                    logger.info("åå°ä»»åŠ¡å¤„ç†çº¿ç¨‹å·²åœæ­¢")
        except Exception as e:
            logger.error(f"åœæ­¢åå°ä»»åŠ¡å¤„ç†çº¿ç¨‹å¤±è´¥: {e}", exc_info=True)
        
        # åœ¨åº”ç”¨å…³é—­æ—¶æ‰§è¡Œæ¸…ç†æ“ä½œ
        try:
            if hasattr(app.state, "engine") and app.state.engine is not None:
                logger.info("é‡Šæ”¾æ•°æ®åº“è¿æ¥æ± ...")
                app.state.engine.dispose()  # é‡Šæ”¾æ•°æ®åº“è¿æ¥æ± 
                logger.info("æ•°æ®åº“è¿æ¥æ± å·²é‡Šæ”¾")
        except Exception as db_close_err:
            logger.error(f"å…³é—­æ•°æ®åº“è¿æ¥å¤±è´¥: {str(db_close_err)}", exc_info=True)
        
        logger.info("åº”ç”¨å·²å®Œå…¨å…³é—­")

app = FastAPI(lifespan=lifespan)
origins = [
    "http://localhost:1420",  # Your Tauri dev server
    "tauri://localhost",      # Often used by Tauri in production
    "https://tauri.localhost" # Also used by Tauri in production
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,  # Allows specific origins
    # allow_origins=["*"], # Or, to allow all origins (less secure, use with caution)
    allow_credentials=True, # Allows cookies to be included in requests
    allow_methods=["*"],    # Allows all methods (GET, POST, PUT, DELETE, etc.)
    allow_headers=["*"],    # Allows all headers
)

# å‘¨æœŸæ€§æ£€æŸ¥æ–°é€šçŸ¥å¹¶å¹¿æ’­
# async def check_notifications():
#     while True:
#         # å¹¿æ’­æ¶ˆæ¯
#         # await manager.broadcast("New notification")
#         await asyncio.sleep(8)

def get_session():
    """FastAPIä¾èµ–å‡½æ•°ï¼Œç”¨äºè·å–æ•°æ®åº“ä¼šè¯"""
    if not hasattr(app.state, "engine") or app.state.engine is None:
        # ç¡®ä¿æ•°æ®åº“å¼•æ“å·²åˆå§‹åŒ–
        raise RuntimeError("æ•°æ®åº“å¼•æ“æœªåˆå§‹åŒ–")
    
    with Session(app.state.engine) as session:
        yield session

# æœ¬åœ°å¤§æ¨¡å‹APIç«¯ç‚¹æ·»åŠ 
models_router = get_models_router(external_get_session=get_session)
app.include_router(models_router, prefix="", tags=["models"])

# æ·»åŠ æ–°çš„æ ‡ç­¾APIè·¯ç”±
tagging_router = get_tagging_router(external_get_session=get_session)
app.include_router(tagging_router, prefix="", tags=["tagging"])

# æ·»åŠ èŠå¤©ä¼šè¯APIè·¯ç”±
chatsession_router = get_chatsession_router(external_get_session=get_session)
app.include_router(chatsession_router, prefix="", tags=["chat-sessions"])

# æ·»åŠ æ–‡ä»¶ç®¡ç†APIè·¯ç”±
myfiles_router = get_myfiles_router(external_get_session=get_session)
app.include_router(myfiles_router, prefix="", tags=["myfiles"])

# è·å– ScreeningManager çš„ä¾èµ–å‡½æ•°
def get_screening_manager(session: Session = Depends(get_session)):
    """è·å–æ–‡ä»¶ç²—ç­›ç»“æœç®¡ç†ç±»å®ä¾‹"""
    return ScreeningManager(session)

# è·å– FileTaggingMgr çš„ä¾èµ–å‡½æ•°
def get_file_tagging_manager(session: Session = Depends(get_session)):
    """è·å–æ–‡ä»¶è§£æç®¡ç†ç±»å®ä¾‹"""
    return FileTaggingMgr(session)

# è·å– TaskManager çš„ä¾èµ–å‡½æ•°
def get_task_manager(session: Session = Depends(get_session)):
    """è·å–ä»»åŠ¡ç®¡ç†å™¨å®ä¾‹"""
    return TaskManager(session)

def get_lancedb_manager():
    """è·å–LanceDBç®¡ç†å™¨å®ä¾‹"""
    if not hasattr(app.state, "engine") or app.state.engine is None:
        raise RuntimeError("æ•°æ®åº“å¼•æ“æœªåˆå§‹åŒ–")
    
    # ä»SQLiteæ•°æ®åº“è·¯å¾„æ¨å¯¼å‡ºbase_dir
    sqlite_url = str(app.state.engine.url)
    if sqlite_url.startswith('sqlite:///'):
        db_path = sqlite_url.replace('sqlite:///', '')
        db_directory = os.path.dirname(db_path)
        return LanceDBMgr(base_dir=db_directory)
    else:
        raise RuntimeError("æ— æ³•ä»æ•°æ®åº“URLæ¨å¯¼å‡ºLanceDBè·¯å¾„")

def get_models_manager(session: Session = Depends(get_session)):
    """è·å–æ¨¡å‹ç®¡ç†å™¨å®ä¾‹"""
    return ModelsMgr(session)

def get_search_manager(
    session: Session = Depends(get_session),
    lancedb_mgr: LanceDBMgr = Depends(get_lancedb_manager),
    models_mgr: ModelsMgr = Depends(get_models_manager)
):
    """è·å–æœç´¢ç®¡ç†å™¨å®ä¾‹"""
    return SearchManager(session, lancedb_mgr, models_mgr)

def get_multivector_manager(
    session: Session = Depends(get_session),
    lancedb_mgr: LanceDBMgr = Depends(get_lancedb_manager),
    models_mgr: ModelsMgr = Depends(get_models_manager)
):
    """è·å–å¤šæ¨¡æ€å‘é‡ç®¡ç†å™¨å®ä¾‹"""
    return MultiVectorMgr(session, lancedb_mgr, models_mgr)



# ä»»åŠ¡å¤„ç†è€…
def task_processor(db_path: str, stop_event: threading.Event):
    """å¤„ç†ä»»åŠ¡çš„åå°å·¥ä½œçº¿ç¨‹"""
    logger.info("ä»»åŠ¡å¤„ç†çº¿ç¨‹å·²å¯åŠ¨")
    sqlite_url = f"sqlite:///{db_path}"
    engine = create_engine(
        sqlite_url, 
        echo=False, 
        connect_args={"check_same_thread": False, "timeout": 30}
    )
    db_directory = os.path.dirname(db_path)
    lancedb_mgr = LanceDBMgr(base_dir=db_directory)

    while not stop_event.is_set():
        try:
            with Session(engine) as session:
                task_mgr = TaskManager(session)
                task: Task = task_mgr.get_next_task()

                if task is None:
                    time.sleep(5) # æ²¡æœ‰ä»»åŠ¡æ—¶ï¼Œç­‰å¾…5ç§’
                    continue

                logger.info(f"ä»»åŠ¡å¤„ç†çº¿ç¨‹æ¥æ”¶ä»»åŠ¡: ID={task.id}, Name='{task.task_name}', Type='{task.task_type}', Priority={task.priority}")
                task_mgr.update_task_status(task.id, TaskStatus.RUNNING)

                models_mgr = ModelsMgr(session)
                file_tagging_mgr = FileTaggingMgr(session, lancedb_mgr, models_mgr)
                multivector_mgr = MultiVectorMgr(session, lancedb_mgr, models_mgr)

                if task.task_type == TaskType.TAGGING.value:
                    # æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§
                    if not file_tagging_mgr.check_file_tagging_model_availability():
                        logger.error("ç›¸å…³æ¨¡å‹ä¸å¯ç”¨ï¼Œæ— æ³•å¤„ç†æ–‡ä»¶æ‰“æ ‡ç­¾ä»»åŠ¡")
                        time.sleep(5)
                        continue
                    # é«˜ä¼˜å…ˆçº§ä»»åŠ¡: é€šå¸¸æ˜¯å•ä¸ªæ–‡ä»¶å¤„ç†
                    if task.priority == TaskPriority.HIGH.value and task.extra_data and 'screening_result_id' in task.extra_data:
                        logger.info(f"å¼€å§‹å¤„ç†é«˜ä¼˜å…ˆçº§æ–‡ä»¶æ‰“æ ‡ç­¾ä»»åŠ¡ (Task ID: {task.id})")
                        success = file_tagging_mgr.process_single_file_task(task.extra_data['screening_result_id'])
                        if success:
                            task_mgr.update_task_status(task.id, TaskStatus.COMPLETED, result=TaskResult.SUCCESS)
                            
                            # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨è¡”æ¥MULTIVECTORä»»åŠ¡ï¼ˆä»…å½“æ–‡ä»¶è¢«pinæ—¶ï¼‰
                            if multivector_mgr.check_vision_embedding_model_availability():
                                _check_and_create_multivector_task(session, task_mgr, task.extra_data.get('screening_result_id'))
                        else:
                            task_mgr.update_task_status(task.id, TaskStatus.FAILED, result=TaskResult.FAILURE)
                    # ä½ä¼˜å…ˆçº§ä»»åŠ¡: æ‰¹é‡å¤„ç†
                    else:
                        logger.info(f"å¼€å§‹æ‰¹é‡æ–‡ä»¶æ‰“æ ‡ç­¾ä»»åŠ¡ (Task ID: {task.id})")
                        result_data = file_tagging_mgr.process_pending_batch(task_id=task.id, batch_size=10) # æ¯æ¬¡å¤„ç†10ä¸ª
                        
                        # æ— è®ºæ‰¹é‡ä»»åŠ¡å¤„ç†äº†å¤šå°‘æ–‡ä»¶ï¼Œéƒ½å°†è§¦å‘ä»»åŠ¡æ–‡ä»¶æ‰“æ ‡ç­¾ä¸ºå®Œæˆ
                        task_mgr.update_task_status(
                            task.id, 
                            TaskStatus.COMPLETED, 
                            result=TaskResult.SUCCESS, 
                            message=f"æ‰¹é‡å¤„ç†å®Œæˆ: å¤„ç†äº† {result_data.get('processed', 0)} ä¸ªæ–‡ä»¶ã€‚"
                        )
                
                elif task.task_type == TaskType.MULTIVECTOR.value:
                    if not multivector_mgr.check_multivector_model_availability():
                        logger.error("ç›¸å…³æ¨¡å‹ä¸å¯ç”¨ï¼Œæ— æ³•å¤„ç†å¤šæ¨¡æ€å‘é‡åŒ–ä»»åŠ¡")
                        time.sleep(5)
                        continue
                    # é«˜ä¼˜å…ˆçº§ä»»åŠ¡: å•æ–‡ä»¶å¤„ç†ï¼ˆç”¨æˆ·pinæ“ä½œæˆ–æ–‡ä»¶å˜åŒ–è¡”æ¥ï¼‰
                    if task.priority == TaskPriority.HIGH.value and task.extra_data and 'file_path' in task.extra_data:
                        file_path = task.extra_data['file_path']
                        logger.info(f"å¼€å§‹å¤„ç†é«˜ä¼˜å…ˆçº§å¤šæ¨¡æ€å‘é‡åŒ–ä»»åŠ¡ (Task ID: {task.id}): {file_path}")
                        
                        try:
                            # ä¼ é€’task_idä»¥ä¾¿äº‹ä»¶è¿½è¸ª
                            success = multivector_mgr.process_document(file_path, str(task.id))
                            if success:
                                task_mgr.update_task_status(
                                    task.id, 
                                    TaskStatus.COMPLETED, 
                                    result=TaskResult.SUCCESS,
                                    message=f"å¤šæ¨¡æ€å‘é‡åŒ–å®Œæˆ: {file_path}"
                                )
                                logger.info(f"å¤šæ¨¡æ€å‘é‡åŒ–æˆåŠŸå®Œæˆ: {file_path}")
                            else:
                                task_mgr.update_task_status(
                                    task.id, 
                                    TaskStatus.FAILED, 
                                    result=TaskResult.FAILURE,
                                    message=f"å¤šæ¨¡æ€å‘é‡åŒ–å¤±è´¥: {file_path}"
                                )
                                logger.error(f"å¤šæ¨¡æ€å‘é‡åŒ–å¤±è´¥: {file_path}")
                        except Exception as e:
                            error_msg = f"å¤šæ¨¡æ€å‘é‡åŒ–å¼‚å¸¸: {file_path} - {str(e)}"
                            task_mgr.update_task_status(
                                task.id, 
                                TaskStatus.FAILED, 
                                result=TaskResult.FAILURE,
                                message=error_msg
                            )
                            logger.error(error_msg, exc_info=True)
                    else:
                        # ä¸­ä½ä¼˜å…ˆçº§ä»»åŠ¡: æ‰¹é‡å¤„ç†ï¼ˆæœªæ¥æ”¯æŒï¼‰
                        logger.info(f"å…¶ä»–ä»»åŠ¡ç±»å‹æš‚æœªå®ç° (Task ID: {task.id})")
                        task_mgr.update_task_status(
                            task.id, 
                            TaskStatus.COMPLETED, 
                            result=TaskResult.SUCCESS,
                            message="æ‰¹é‡å¤„ç†ä»»åŠ¡å·²è·³è¿‡"
                        )
                
                else:
                    logger.warning(f"æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task.task_type} for task ID: {task.id}")
                    task_mgr.update_task_status(task.id, TaskStatus.FAILED, result=TaskResult.FAILURE, message=f"Unknown task type: {task.task_type}")

        except Exception as e:
            logger.error(f"ä»»åŠ¡å¤„ç†çº¿ç¨‹å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
            time.sleep(30)

    logger.info("ä»»åŠ¡å¤„ç†çº¿ç¨‹å·²åœæ­¢")

def _check_and_create_multivector_task(session: Session, task_mgr: TaskManager, screening_result_id: int):
    """
    æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å¤„äºpinçŠ¶æ€ï¼Œå¦‚æœæ˜¯åˆ™è‡ªåŠ¨åˆ›å»ºMULTIVECTORä»»åŠ¡
    
    Args:
        session: æ•°æ®åº“ä¼šè¯
        task_mgr: ä»»åŠ¡ç®¡ç†å™¨
        screening_result_id: ç²—ç­›ç»“æœID
    """
    if not screening_result_id:
        return
    
    try:
        from screening_mgr import ScreeningResult
        # è·å–ç²—ç­›ç»“æœï¼ŒåŒ…å«æ–‡ä»¶è·¯å¾„ä¿¡æ¯
        screening_result = session.get(ScreeningResult, screening_result_id)
        if not screening_result:
            logger.warning(f"æœªæ‰¾åˆ°screening_result_id: {screening_result_id}")
            return
        
        file_path = screening_result.file_path
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨æœ€è¿‘8å°æ—¶å†…è¢«pinè¿‡
        is_recently_pinned = _check_file_pin_status(file_path, session)
        
        if is_recently_pinned:
            logger.info(f"æ–‡ä»¶ {file_path} åœ¨æœ€è¿‘8å°æ—¶å†…è¢«pinè¿‡ï¼Œåˆ›å»ºMULTIVECTORä»»åŠ¡")
            task_mgr.add_task(
                task_name=f"å¤šæ¨¡æ€å‘é‡åŒ–: {Path(file_path).name}",
                task_type=TaskType.MULTIVECTOR,
                priority=TaskPriority.HIGH,
                extra_data={"file_path": file_path},
                target_file_path=file_path  # è®¾ç½®å†—ä½™å­—æ®µä¾¿äºæŸ¥è¯¢
            )
        else:
            logger.info(f"æ–‡ä»¶ {file_path} åœ¨æœ€è¿‘8å°æ—¶å†…æœªè¢«pinè¿‡ï¼Œè·³è¿‡MULTIVECTORä»»åŠ¡")
            
    except Exception as e:
        logger.error(f"æ£€æŸ¥å’Œåˆ›å»ºMULTIVECTORä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)

def _check_file_pin_status(file_path: str, session: Session) -> bool:
    """
    æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨æœ€è¿‘8å°æ—¶å†…è¢«pinè¿‡ï¼ˆå³æœ‰æˆåŠŸçš„MULTIVECTORä»»åŠ¡ï¼‰
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        session: æ•°æ®åº“ä¼šè¯
        
    Returns:
        bool: æ–‡ä»¶æ˜¯å¦åœ¨æœ€è¿‘8å°æ—¶å†…è¢«pinè¿‡
    """
    try:
        task_mgr = TaskManager(session)
        return task_mgr.is_file_recently_pinned(file_path, hours=8)
    except Exception as e:
        logger.error(f"æ£€æŸ¥æ–‡ä»¶pinçŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return False

# @app.post("/pin-file")
# def pin_file(
#     request: Dict[str, Any] = Body(...),
#     task_mgr: TaskManager = Depends(get_task_manager)
# ):
#     """
#     Pinä¸€ä¸ªæ–‡ä»¶ï¼Œåˆ›å»ºé«˜ä¼˜å…ˆçº§çš„å¤šæ¨¡æ€å‘é‡åŒ–ä»»åŠ¡
    
#     å‚æ•°:
#     - file_path: æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
    
#     è¿”å›:
#     - task_id: åˆ›å»ºçš„ä»»åŠ¡ID
#     - message: çŠ¶æ€ä¿¡æ¯
#     """
#     try:
#         file_path = request.get("file_path")
#         if not file_path:
#             return {"success": False, "error": "ç¼ºå°‘file_pathå‚æ•°"}
        
#         # éªŒè¯æ–‡ä»¶è·¯å¾„
#         if not os.path.exists(file_path):
#             return {"success": False, "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}
        
#         if not os.path.isfile(file_path):
#             return {"success": False, "error": f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {file_path}"}
        
#         # æ£€æŸ¥æ–‡ä»¶æƒé™
#         if not os.access(file_path, os.R_OK):
#             return {"success": False, "error": f"æ–‡ä»¶æ— è¯»å–æƒé™: {file_path}"}
        
#         # åˆ›å»ºé«˜ä¼˜å…ˆçº§MULTIVECTORä»»åŠ¡
#         file_name = os.path.basename(file_path)
#         task = task_mgr.add_task(
#             task_name=f"Pinæ–‡ä»¶å‘é‡åŒ–: {file_name}",
#             task_type=TaskType.MULTIVECTOR,
#             priority=TaskPriority.HIGH,
#             extra_data={"file_path": file_path, "source": "user_pin"},
#             target_file_path=file_path
#         )
        
#         logger.info(f"ç”¨æˆ·Pinæ–‡ä»¶æˆåŠŸï¼Œåˆ›å»ºä»»åŠ¡ID: {task.id}, æ–‡ä»¶: {file_path}")
        
#         return {
#             "success": True,
#             "task_id": task.id,
#             "message": f"æ–‡ä»¶PinæˆåŠŸï¼Œæ­£åœ¨å¤„ç†: {file_name}",
#             "file_path": file_path
#         }
        
#     except Exception as e:
#         logger.error(f"Pinæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
#         return {"success": False, "error": f"Pinæ–‡ä»¶å¤±è´¥: {str(e)}"}

@app.get("/task/{task_id}")
def get_task_status(task_id: int, task_mgr: TaskManager = Depends(get_task_manager)):
    """
    è·å–ä»»åŠ¡çŠ¶æ€
    
    å‚æ•°:
    - task_id: ä»»åŠ¡ID
    
    è¿”å›:
    - ä»»åŠ¡è¯¦ç»†ä¿¡æ¯
    """
    try:
        task = task_mgr.get_task(task_id)
        if not task:
            return {"success": False, "error": f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}"}
        
        return {
            "success": True,
            "task": {
                "id": task.id,
                "task_name": task.task_name,
                "task_type": task.task_type,
                "priority": task.priority,
                "status": task.status,
                "result": task.result,
                "error_message": task.error_message,
                "created_at": task.created_at,
                "updated_at": task.updated_at,
                "start_time": task.start_time,
                "extra_data": task.extra_data,
                "target_file_path": task.target_file_path
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return {"success": False, "error": f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}"}

@app.get("/images/{image_filename}")
def get_image(image_filename: str, session: Session = Depends(get_session)):
    """
    è·å–å›¾ç‰‡æ–‡ä»¶å†…å®¹
    
    å‚æ•°:
    - image_filename: å›¾ç‰‡æ–‡ä»¶å (ä¾‹å¦‚: image_000000_hash.png)
    
    è¿”å›:
    - å›¾ç‰‡æ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹
    """
    try:
        from fastapi.responses import FileResponse
        from pathlib import Path
        
        # éªŒè¯æ–‡ä»¶åæ ¼å¼ï¼ˆå®‰å…¨æ£€æŸ¥ï¼‰
        if not image_filename.endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):
            return {"success": False, "error": "ä¸æ”¯æŒçš„å›¾ç‰‡æ ¼å¼"}
        
        if ".." in image_filename or "/" in image_filename or "\\" in image_filename:
            return {"success": False, "error": "æ— æ•ˆçš„æ–‡ä»¶å"}
        
        # è·å–doclingç¼“å­˜ç›®å½•
        try:
            # ä»æ•°æ®åº“å¼•æ“è·å–åŸºç¡€ç›®å½•
            sqlite_url = str(app.state.engine.url)
            if sqlite_url.startswith('sqlite:///'):
                db_path = sqlite_url.replace('sqlite:///', '')
                base_dir = Path(db_path).parent
            else:
                base_dir = Path.cwd()
            
            docling_cache_dir = base_dir / "docling_cache"
            image_path = docling_cache_dir / image_filename
            
        except Exception as e:
            logger.error(f"è·å–doclingç¼“å­˜ç›®å½•å¤±è´¥: {e}")
            return {"success": False, "error": "æ— æ³•ç¡®å®šå›¾ç‰‡å­˜å‚¨ä½ç½®"}
        
        # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not image_path.exists():
            logger.warning(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
            return {"success": False, "error": f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_filename}"}
        
        # éªŒè¯è¿™ä¸ªå›¾ç‰‡æ˜¯å¦å±äºæŸä¸ªå·²å¤„ç†çš„æ–‡æ¡£ï¼ˆå®‰å…¨æ£€æŸ¥ï¼‰
        from sqlmodel import select
        from db_mgr import ParentChunk
        
        # æŸ¥æ‰¾åŒ…å«æ­¤å›¾ç‰‡æ–‡ä»¶åçš„ParentChunkï¼ˆåœ¨metadataçš„image_file_pathä¸­æŸ¥æ‰¾ï¼‰
        stmt = select(ParentChunk).where(
            ParentChunk.chunk_type == "image",
            ParentChunk.metadata_json.contains(image_filename)
        )
        chunk = session.exec(stmt).first()
        
        if not chunk:
            logger.warning(f"å›¾ç‰‡æ–‡ä»¶æœªåœ¨æ•°æ®åº“ä¸­æ‰¾åˆ°å…³è”è®°å½•: {image_filename}")
            return {"success": False, "error": "å›¾ç‰‡æ–‡ä»¶æ— æ•ˆæˆ–å·²è¿‡æœŸ"}
        
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åç¡®å®šæ­£ç¡®çš„ MIME ç±»å‹
        file_ext = image_filename.lower().split('.')[-1]
        mime_type_map = {
            'png': 'image/png',
            'jpg': 'image/jpeg',
            'jpeg': 'image/jpeg',
            'gif': 'image/gif',
            'bmp': 'image/bmp',
            'webp': 'image/webp'
        }
        media_type = mime_type_map.get(file_ext, 'image/png')
        
        # è¿”å›å›¾ç‰‡æ–‡ä»¶
        return FileResponse(
            path=str(image_path),
            media_type=media_type,
            headers={"Content-Disposition": "inline"}  # è®©æµè§ˆå™¨ç›´æ¥æ˜¾ç¤ºè€Œä¸æ˜¯ä¸‹è½½
        )
        
    except Exception as e:
        logger.error(f"è·å–å›¾ç‰‡æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return {"success": False, "error": f"è·å–å›¾ç‰‡å¤±è´¥: {str(e)}"}

@app.get("/images/by-chunk/{parent_chunk_id}")
def get_image_by_chunk(parent_chunk_id: int, session: Session = Depends(get_session)):
    """
    é€šè¿‡ParentChunk IDè·å–å…³è”çš„å›¾ç‰‡
    
    å‚æ•°:
    - parent_chunk_id: çˆ¶å—ID
    
    è¿”å›:
    - å›¾ç‰‡æ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹ï¼Œæˆ–é‡å®šå‘åˆ°å›¾ç‰‡ç«¯ç‚¹
    """
    try:
        from fastapi.responses import RedirectResponse
        from pathlib import Path
        from sqlmodel import select
        from db_mgr import ParentChunk
        
        # æŸ¥æ‰¾æŒ‡å®šçš„ParentChunk
        stmt = select(ParentChunk).where(
            ParentChunk.id == parent_chunk_id,
            ParentChunk.chunk_type == "image"
        )
        chunk = session.exec(stmt).first()
        
        if not chunk:
            return {"success": False, "error": f"å›¾ç‰‡å—ä¸å­˜åœ¨: {parent_chunk_id}"}
        
        # ä»chunkä¸­æå–å›¾ç‰‡æ–‡ä»¶è·¯å¾„
        image_filename = None
        
        # ä»metadataä¸­è·å–image_file_path
        try:
            metadata = json.loads(chunk.metadata_json)
            image_file_path = metadata.get("image_file_path")
            
            if image_file_path and os.path.exists(image_file_path):
                # metadataä¸­æœ‰å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
                image_path = Path(image_file_path)
                image_filename = image_path.name
                logger.info(f"Found image file from metadata: {image_filename}")
            else:
                logger.warning(f"Image file path not found or file does not exist: {image_file_path}")
                        
        except Exception as e:
            logger.warning(f"æ— æ³•ä»metadataæå–å›¾ç‰‡è·¯å¾„: {e}")
        
        if not image_filename:
            return {"success": False, "error": "æ— æ³•ç¡®å®šå›¾ç‰‡æ–‡ä»¶è·¯å¾„"}
        
        # é‡å®šå‘åˆ°å›¾ç‰‡è·å–ç«¯ç‚¹
        return RedirectResponse(url=f"/images/{image_filename}")
        
    except Exception as e:
        logger.error(f"é€šè¿‡chunkè·å–å›¾ç‰‡æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return {"success": False, "error": f"è·å–å›¾ç‰‡å¤±è´¥: {str(e)}"}

@app.get("/documents/{document_id}/images")
def get_document_images(document_id: int, session: Session = Depends(get_session)):
    """
    è·å–æ–‡æ¡£ä¸­çš„æ‰€æœ‰å›¾ç‰‡åˆ—è¡¨
    
    å‚æ•°:
    - document_id: æ–‡æ¡£ID
    
    è¿”å›:
    - å›¾ç‰‡åˆ—è¡¨ï¼ŒåŒ…å«chunk_idã€æ–‡ä»¶åã€æè¿°ç­‰ä¿¡æ¯
    """
    try:
        from sqlmodel import select
        from db_mgr import ParentChunk
        from pathlib import Path
        import json
        
        # æŸ¥æ‰¾æ–‡æ¡£ä¸­æ‰€æœ‰çš„å›¾ç‰‡å—
        stmt = select(ParentChunk).where(
            ParentChunk.document_id == document_id,
            ParentChunk.chunk_type == "image"
        )
        image_chunks = session.exec(stmt).all()
        
        images = []
        for chunk in image_chunks:
            try:
                # æå–å›¾ç‰‡æ–‡ä»¶å - ä»metadataä¸­è·å–
                image_filename = None
                
                # ä»metadataä¸­è·å–image_file_path
                try:
                    metadata = json.loads(chunk.metadata_json)
                    image_file_path = metadata.get("image_file_path")
                    
                    if image_file_path and os.path.exists(image_file_path):
                        # metadataä¸­æœ‰å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
                        image_path = Path(image_file_path)
                        image_filename = image_path.name
                    else:
                        logger.warning(f"Image file path not found or file does not exist for chunk {chunk.id}: {image_file_path}")
                                
                except Exception as e:
                    logger.warning(f"å¤„ç†å›¾ç‰‡å— {chunk.id} metadataæ—¶å‡ºé”™: {e}")
                
                # å¦‚æœæ— æ³•ç¡®å®šæ–‡ä»¶åï¼Œè·³è¿‡è¿™ä¸ªå›¾ç‰‡å—
                if not image_filename:
                    logger.warning(f"æ— æ³•ç¡®å®šå›¾ç‰‡å— {chunk.id} çš„æ–‡ä»¶åï¼Œè·³è¿‡")
                    continue
                
                # è·å–å›¾ç‰‡æè¿° - ç°åœ¨ç›´æ¥ä»contentå­—æ®µè·å–
                image_description = chunk.content if chunk.content else ""
                
                images.append({
                    "chunk_id": chunk.id,
                    "filename": image_filename,
                    "description": image_description,
                    "image_url": f"/images/{image_filename}",
                    "chunk_url": f"/images/by-chunk/{chunk.id}"
                })
                
            except Exception as e:
                logger.warning(f"å¤„ç†å›¾ç‰‡å— {chunk.id} æ—¶å‡ºé”™: {e}")
                continue
        
        return {
            "success": True,
            "document_id": document_id,
            "images": images,
            "total_count": len(images)
        }
        
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£å›¾ç‰‡åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return {"success": False, "error": f"è·å–å›¾ç‰‡åˆ—è¡¨å¤±è´¥: {str(e)}"}

# =============================================================================
# ğŸ“Š å‘é‡å†…å®¹æœç´¢APIç«¯ç‚¹
# =============================================================================

@app.post("/search/content")
def search_document_content(
    request: Dict[str, Any] = Body(...),
    search_mgr: SearchManager = Depends(get_search_manager)
):
    """
    æ–‡æ¡£å†…å®¹çš„è‡ªç„¶è¯­è¨€æ£€ç´¢
    
    å‚æ•°:
    - query: è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ–‡æœ¬
    - top_k: è¿”å›çš„æœ€å¤§ç»“æœæ•° (å¯é€‰ï¼Œé»˜è®¤10)
    - document_ids: æ–‡æ¡£IDè¿‡æ»¤åˆ—è¡¨ (å¯é€‰)
    - distance_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ (å¯é€‰)
    
    è¿”å›:
    - success: æ˜¯å¦æˆåŠŸ
    - results: æ ¼å¼åŒ–çš„æ£€ç´¢ç»“æœ
    - query_info: æŸ¥è¯¢å…ƒä¿¡æ¯
    """
    try:
        # æå–å‚æ•°
        query = request.get("query", "").strip()
        top_k = request.get("top_k", 10)
        document_ids = request.get("document_ids")
        distance_threshold = request.get("distance_threshold")
        
        logger.info(f"[SEARCH API] Content search request: '{query[:50]}...'")
        
        # åŸºç¡€éªŒè¯
        if not query:
            return {
                "success": False,
                "error": "æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º",
                "results": None
            }
        
        # æ‰§è¡Œæœç´¢
        search_result = search_mgr.search_documents(
            query=query,
            top_k=top_k,
            document_ids=document_ids,
            distance_threshold=distance_threshold
        )
        
        # è¿”å›ç»“æœ
        logger.info(f"[SEARCH API] Search completed with {search_result.get('success', False)} status")
        return search_result
        
    except Exception as e:
        logger.error(f"[SEARCH API] Content search failed: {e}")
        return {
            "success": False,
            "error": f"æœç´¢å¤±è´¥: {str(e)}",
            "results": None
        }

@app.post("/documents/{document_id}/search/content")  
def search_document_content_by_id(
    document_id: int,
    request: Dict[str, Any] = Body(...),
    search_mgr: SearchManager = Depends(get_search_manager)
):
    """
    åœ¨æŒ‡å®šæ–‡æ¡£å†…è¿›è¡Œå‘é‡å†…å®¹æ£€ç´¢
    
    å‚æ•°:
    - document_id: æ–‡æ¡£ID
    - query: è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ–‡æœ¬
    - top_k: è¿”å›çš„æœ€å¤§ç»“æœæ•° (å¯é€‰ï¼Œé»˜è®¤10)
    - distance_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ (å¯é€‰)
    
    è¿”å›:
    - success: æ˜¯å¦æˆåŠŸ
    - results: æ ¼å¼åŒ–çš„æ£€ç´¢ç»“æœ
    - query_info: æŸ¥è¯¢å…ƒä¿¡æ¯
    """
    try:
        # æå–å‚æ•°
        query = request.get("query", "").strip()
        top_k = request.get("top_k", 10)
        distance_threshold = request.get("distance_threshold")
        
        logger.info(f"[SEARCH API] Document {document_id} content search: '{query[:50]}...'")
        
        # åŸºç¡€éªŒè¯
        if not query:
            return {
                "success": False,
                "error": "æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º",
                "results": None
            }
        
        # æ‰§è¡Œæœç´¢ï¼ˆé™åˆ¶åœ¨æŒ‡å®šæ–‡æ¡£ï¼‰
        search_result = search_mgr.search_documents(
            query=query,
            top_k=top_k,
            document_ids=[document_id],  # é™åˆ¶åœ¨æŒ‡å®šæ–‡æ¡£
            distance_threshold=distance_threshold
        )
        
        # æ·»åŠ æ–‡æ¡£IDä¿¡æ¯åˆ°ç»“æœä¸­
        if search_result.get("success", False):
            if "query_info" not in search_result:
                search_result["query_info"] = {}
            search_result["query_info"]["target_document_id"] = document_id
        
        logger.info(f"[SEARCH API] Document {document_id} search completed")
        return search_result
        
    except Exception as e:
        logger.error(f"[SEARCH API] Document {document_id} content search failed: {e}")
        return {
            "success": False,
            "error": f"æ–‡æ¡£å†…æœç´¢å¤±è´¥: {str(e)}",
            "results": None
        }

@app.post("/file-screening/batch")
def add_batch_file_screening_results(
    request: Dict[str, Any] = Body(...), 
    screening_mgr: ScreeningManager = Depends(get_screening_manager),
    task_mgr: TaskManager = Depends(get_task_manager)
):
    """æ‰¹é‡æ·»åŠ æ–‡ä»¶ç²—ç­›ç»“æœ
    
    å‚æ•°:
    - data_list: æ–‡ä»¶ç²—ç­›ç»“æœåˆ—è¡¨
    """
    try:
        # ä»è¯·æ±‚ä½“ä¸­æå–æ•°æ®å’Œå‚æ•°
        logger.info(f"æ¥æ”¶åˆ°æ‰¹é‡æ–‡ä»¶ç²—ç­›ç»“æœï¼Œè¯·æ±‚ä½“é”®å: {list(request.keys())}")
        
        # é€‚é…Rustå®¢æˆ·ç«¯å‘é€çš„æ ¼å¼: {data_list: [...], auto_create_tasks: true}
        if "data_list" in request:
            data_list = request.get("data_list", [])
        elif isinstance(request, dict):
            data_list = request.get("files", [])
        else:
            # å‡è®¾è¯·æ±‚ä½“æœ¬èº«å°±æ˜¯åˆ—è¡¨
            data_list = request
            
        if not data_list:
            return {"success": True, "processed_count": 0, "failed_count": 0, "message": "æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶"}

        # é¢„å¤„ç†æ¯ä¸ªæ–‡ä»¶è®°å½•ä¸­çš„æ—¶é—´æˆ³ï¼Œè½¬æ¢ä¸ºPython datetimeå¯¹è±¡
        for data in data_list:
            # å¤„ç†Unixæ—¶é—´æˆ³çš„è½¬æ¢ (ä»Rustå‘é€çš„ç§’æ•°è½¬æ¢ä¸ºPython datetime)
            if "created_time" in data and isinstance(data["created_time"], (int, float)):
                data["created_time"] = datetime.fromtimestamp(data["created_time"])
                
            if "modified_time" in data and isinstance(data["modified_time"], (int, float)):
                data["modified_time"] = datetime.fromtimestamp(data["modified_time"])
                
            if "accessed_time" in data and isinstance(data["accessed_time"], (int, float)):
                data["accessed_time"] = datetime.fromtimestamp(data["accessed_time"])
        
        # å¤„ç†å­—ç¬¦ä¸²æ ¼å¼çš„æ—¶é—´å­—æ®µï¼ˆå¤„ç†ä¹‹å‰å·²ç»å…ˆå¤„ç†äº†æ•´æ•°æ—¶é—´æˆ³ï¼‰
        for data in data_list:
            for time_field in ["created_time", "modified_time", "accessed_time"]:
                # åªå¤„ç†ä»ç„¶æ˜¯å­—ç¬¦ä¸²æ ¼å¼çš„æ—¶é—´å­—æ®µï¼ˆæ•´æ•°æ—¶é—´æˆ³å·²åœ¨å‰ä¸€æ­¥è½¬æ¢ï¼‰
                if time_field in data and isinstance(data[time_field], str):
                    try:
                        data[time_field] = datetime.fromisoformat(data[time_field].replace("Z", "+00:00"))
                    except Exception as e:
                        logger.warning(f"è½¬æ¢å­—ç¬¦ä¸²æ—¶é—´å­—æ®µ {time_field} å¤±è´¥: {str(e)}")
                        # å¦‚æœæ˜¯ä¿®æ”¹æ—¶é—´å­—æ®µè½¬æ¢å¤±è´¥ï¼Œè®¾ç½®ä¸ºå½“å‰æ—¶é—´
                        if time_field == "modified_time":
                            data[time_field] = datetime.now()
                
                # ç¡®ä¿æ¯ä¸ªæ—¶é—´å­—æ®µéƒ½æœ‰å€¼ï¼Œå¯¹äºå¿…å¡«å­—æ®µ
                if time_field == "modified_time" and (time_field not in data or data[time_field] is None):
                    logger.warning(f"ç¼ºå°‘å¿…å¡«æ—¶é—´å­—æ®µ {time_field}ï¼Œä½¿ç”¨å½“å‰æ—¶é—´")
                    data[time_field] = datetime.now()
                            
            # Ensure 'extra_metadata' is used, but allow 'metadata' for backward compatibility from client
            if "metadata" in data and "extra_metadata" not in data:
                data["extra_metadata"] = data.pop("metadata")

        # 1. å…ˆåˆ›å»ºä»»åŠ¡ï¼Œè·å– task_id
        task_name = f"æ‰¹é‡å¤„ç†æ–‡ä»¶: {len(data_list)} ä¸ªæ–‡ä»¶"
        task: Task = task_mgr.add_task(
            task_name=task_name,
            task_type=TaskType.TAGGING,
            priority=TaskPriority.MEDIUM,
            extra_data={"file_count": len(data_list)}
        )
        logger.info(f"å·²åˆ›å»ºæ ‡è®°ä»»åŠ¡ ID: {task.id}ï¼Œå‡†å¤‡å¤„ç† {len(data_list)} ä¸ªæ–‡ä»¶")

        # 2. æ‰¹é‡æ·»åŠ ç²—ç­›ç»“æœï¼Œå¹¶å…³è” task_id
        result = screening_mgr.add_batch_screening_results(data_list, task_id=task.id)
        
        # 3. è¿”å›ç»“æœ
        if result["success"] > 0:
            message = f"å·²ä¸º {result['success']} ä¸ªæ–‡ä»¶åˆ›å»ºå¤„ç†ä»»åŠ¡ï¼Œå¤±è´¥ {result['failed']} ä¸ª"
        else:
            message = f"æœªèƒ½å¤„ç†ä»»ä½•æ–‡ä»¶ï¼Œå¤±è´¥ {result['failed']} ä¸ª"

        return {
            "success": result["success"] > 0,
            "processed_count": result["success"],
            "failed_count": result["failed"],
            "errors": result.get("errors"),
            "task_id": task.id,
            "message": message
        }
        
    except Exception as e:
        logger.error(f"æ‰¹é‡å¤„ç†æ–‡ä»¶ç²—ç­›ç»“æœå¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}"
        }

@app.get("/file-screening/results")
def get_file_screening_results(
    limit: int = 1000,
    category_id: int = None,
    time_range: str = None,
    screening_mgr: ScreeningManager = Depends(get_screening_manager)
):
    """è·å–æ–‡ä»¶ç²—ç­›ç»“æœåˆ—è¡¨ï¼Œæ”¯æŒæŒ‰åˆ†ç±»å’Œæ—¶é—´èŒƒå›´ç­›é€‰
    
    å‚æ•°:
    - limit: æœ€å¤§è¿”å›ç»“æœæ•°
    - category_id: å¯é€‰ï¼ŒæŒ‰æ–‡ä»¶åˆ†ç±»IDè¿‡æ»¤
    - time_range: å¯é€‰ï¼ŒæŒ‰æ—¶é—´èŒƒå›´è¿‡æ»¤ ("today", "last7days", "last30days")
    """
    try:
        from datetime import datetime, timedelta
        
        # åŸºç¡€æŸ¥è¯¢
        results = screening_mgr.get_all_results(limit)
        
        # å¦‚æœç»“æœä¸ºç©ºï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨ï¼Œé˜²æ­¢åç»­å¤„ç†å‡ºé”™
        if not results:
            return {
                "success": True,
                "count": 0,
                "data": []
            }
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–å­—å…¸åˆ—è¡¨
        results_dict = [result.model_dump() for result in results]
        
        # è¿‡æ»¤é€»è¾‘
        filtered_results = results_dict
        
        # æŒ‰åˆ†ç±»è¿‡æ»¤
        if (category_id is not None):
            filtered_results = [r for r in filtered_results if r.get('category_id') == category_id]
        
        # æŒ‰æ—¶é—´èŒƒå›´è¿‡æ»¤
        if time_range:
            now = datetime.now()
            # Ensure modified_time is a string before parsing
            date_format = "%Y-%m-%d %H:%M:%S" # Define the correct format

            if time_range == "today":
                today = datetime(now.year, now.month, now.day)
                filtered_results = [r for r in filtered_results if r.get('modified_time') and datetime.strptime(r.get('modified_time'), date_format) >= today]
            elif time_range == "last7days":
                week_ago = now - timedelta(days=7)
                filtered_results = [r for r in filtered_results if r.get('modified_time') and datetime.strptime(r.get('modified_time'), date_format) >= week_ago]
            elif time_range == "last30days":
                month_ago = now - timedelta(days=30)
                filtered_results = [r for r in filtered_results if r.get('modified_time') and datetime.strptime(r.get('modified_time'), date_format) >= month_ago]
        
        return {
            "success": True,
            "count": len(filtered_results),
            "data": filtered_results
        }
        
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶ç²—ç­›ç»“æœåˆ—è¡¨å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            "success": False,
            "message": f"è·å–å¤±è´¥: {str(e)}"
        }
@app.get("/file-screening/results/search")
def search_files_by_path_substring(
    substring: str,
    limit: int = 100,
    screening_mgr: ScreeningManager = Depends(get_screening_manager)
):
    """æ ¹æ®è·¯å¾„å­å­—ç¬¦ä¸²æœç´¢æ–‡ä»¶ç²—ç­›ç»“æœ
    
    å‚æ•°:
    - substring: è¦æœç´¢çš„è·¯å¾„å­å­—ç¬¦ä¸²
    - limit: æœ€å¤§è¿”å›ç»“æœæ•°
    """
    try:
        # ä½¿ç”¨ ScreeningManager çš„æœç´¢æ–¹æ³•ï¼Œç°åœ¨è¿”å›å­—å…¸åˆ—è¡¨
        results_dict = screening_mgr.search_files_by_path_substring(substring, limit)
        
        return {
            "success": True,
            "count": len(results_dict),
            "data": results_dict
        }
        
    except Exception as e:
        logger.error(f"æ ¹æ®è·¯å¾„å­å­—ç¬¦ä¸²æœç´¢æ–‡ä»¶ç²—ç­›ç»“æœå¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"æœç´¢å¤±è´¥: {str(e)}"
        }

@app.get("/")
def read_root():
    # ç°åœ¨å¯ä»¥åœ¨ä»»ä½•è·¯ç”±ä¸­ä½¿ç”¨ app.state.db_path
    return {"Hello": "World", "db_path": app.state.db_path}

# æ·»åŠ å¥åº·æ£€æŸ¥ç«¯ç‚¹
@app.get("/health")
def health_check():
    """APIå¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼Œç”¨äºéªŒè¯APIæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ"""
    return {
        "status": "ok", 
        "timestamp": datetime.now().isoformat(),
        # "cache_stats": {
        #     "config": config_cache.get_stats(),
        #     "folders": folder_hierarchy_cache.get_stats()
        # }
    }

@app.post("/screening/clean-by-path")
def clean_screening_results_by_path(
    data: Dict[str, Any] = Body(...),
    screening_mgr: ScreeningManager = Depends(get_screening_manager)
):
    """æ‰‹åŠ¨æ¸…ç†æŒ‡å®šè·¯å¾„ä¸‹çš„ç²—ç­›ç»“æœï¼ˆç”¨äºæ·»åŠ é»‘åå•å­æ–‡ä»¶å¤¹æ—¶ï¼‰
    
    å‰ç«¯å¯ä»¥ä½¿ç”¨æ­¤ç«¯ç‚¹åœ¨ç”¨æˆ·åœ¨ç™½åå•ä¸‹æ·»åŠ é»‘åå•å­æ–‡ä»¶å¤¹åæ¸…ç†æ•°æ®ï¼Œ
    ç›¸å½“äºåœ¨é›†åˆä¸­æ‰£å‡ºä¸€ä¸ªå­é›†æ¥åˆ æ‰ã€‚
    """
    try:
        folder_path = data.get("path", "").strip()
        
        if not folder_path:
            return {"status": "error", "message": "æ–‡ä»¶å¤¹è·¯å¾„ä¸èƒ½ä¸ºç©º"}
        
        # ä½¿ç”¨ delete_screening_results_by_path_prefix æ–¹æ³•ï¼Œç”¨äºåœ¨ç™½åå•ä¸‹æ·»åŠ é»‘åå•å­æ–‡ä»¶å¤¹
        deleted_count = screening_mgr.delete_screening_results_by_path_prefix(folder_path)
        return {
            "status": "success", 
            "deleted": deleted_count,
            "message": f"å·²æ¸…ç† {deleted_count} æ¡ä¸è·¯å¾„å‰ç¼€ '{folder_path}' ç›¸å…³çš„ç²—ç­›ç»“æœ"
        }
            
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨æ¸…ç†ç²—ç­›ç»“æœå¤±è´¥: {str(e)}")
        return {"status": "error", "message": f"æ¸…ç†å¤±è´¥: {str(e)}"}

@app.post("/screening/delete-by-path")
def delete_screening_by_path(
    data: Dict[str, Any] = Body(...),
    screening_mgr: ScreeningManager = Depends(get_screening_manager)
):
    """åˆ é™¤æŒ‡å®šè·¯å¾„çš„æ–‡ä»¶ç²—ç­›è®°å½•
    
    å½“å®¢æˆ·ç«¯æ£€æµ‹åˆ°æ–‡ä»¶åˆ é™¤äº‹ä»¶æ—¶ï¼Œè°ƒç”¨æ­¤APIç«¯ç‚¹åˆ é™¤å¯¹åº”çš„ç²—ç­›è®°å½•ã€‚
    
    è¯·æ±‚ä½“:
    - file_path: è¦åˆ é™¤çš„æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
    - success: æ“ä½œæ˜¯å¦æˆåŠŸ
    - deleted_count: åˆ é™¤çš„è®°å½•æ•°é‡
    - message: æ“ä½œç»“æœæ¶ˆæ¯
    """
    try:
        file_path = data.get("file_path")
        
        if not file_path:
            logger.warning("åˆ é™¤ç²—ç­›è®°å½•è¯·æ±‚ä¸­æœªæä¾›æ–‡ä»¶è·¯å¾„")
            return {
                "success": False,
                "deleted_count": 0,
                "message": "æ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º"
            }
        
        # å¯¹äºå•ä¸ªæ–‡ä»¶åˆ é™¤ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿è·¯å¾„æ˜¯ç²¾ç¡®åŒ¹é…çš„
        # æˆ‘ä»¬å¯ä»¥ä½¿ç”¨delete_screening_results_by_path_prefixæ–¹æ³•ï¼Œä½†éœ€è¦ç¡®ä¿åªåˆ é™¤è¿™ä¸ªç¡®åˆ‡è·¯å¾„
        # é€šå¸¸æƒ…å†µä¸‹ï¼Œè¿™ä¸ªè·¯å¾„åº”è¯¥æ˜¯ä¸€ä¸ªæ–‡ä»¶è·¯å¾„ï¼Œä¸ä¼šåŒ¹é…åˆ°å…¶ä»–æ–‡ä»¶
        
        # æ ‡å‡†åŒ–è·¯å¾„
        normalized_path = os.path.normpath(file_path).replace("\\", "/")
        
        # æ‰§è¡Œåˆ é™¤æ“ä½œ
        deleted_count = screening_mgr.delete_screening_results_by_path_prefix(normalized_path)
        
        # è®°å½•æ“ä½œç»“æœ
        if deleted_count > 0:
            logger.info(f"æˆåŠŸåˆ é™¤æ–‡ä»¶ '{normalized_path}' çš„ç²—ç­›è®°å½•ï¼Œå…± {deleted_count} æ¡")
        else:
            logger.info(f"æœªæ‰¾åˆ°æ–‡ä»¶ '{normalized_path}' çš„ç²—ç­›è®°å½•ï¼Œæ— éœ€åˆ é™¤")
        
        return {
            "success": True,
            "deleted_count": deleted_count,
            "message": f"æˆåŠŸåˆ é™¤æ–‡ä»¶ '{normalized_path}' çš„ç²—ç­›è®°å½•ï¼Œå…± {deleted_count} æ¡"
        }
        
    except Exception as e:
        logger.error(f"åˆ é™¤æ–‡ä»¶ç²—ç­›è®°å½•å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            "success": False,
            "deleted_count": 0,
            "message": f"åˆ é™¤å¤±è´¥: {str(e)}"
        }

@app.post("/pin-file")
async def pin_file(
    data: Dict[str, Any] = Body(...),
    task_mgr: TaskManager = Depends(get_task_manager)
):
    """Pinæ–‡ä»¶å¹¶åˆ›å»ºå¤šæ¨¡æ€å‘é‡åŒ–ä»»åŠ¡
    
    ç”¨æˆ·pinæ–‡ä»¶æ—¶è°ƒç”¨æ­¤ç«¯ç‚¹ï¼Œç«‹å³åˆ›å»ºHIGHä¼˜å…ˆçº§çš„MULTIVECTORä»»åŠ¡
    
    è¯·æ±‚ä½“:
    - file_path: è¦pinçš„æ–‡ä»¶ç»å¯¹è·¯å¾„
    
    è¿”å›:
    - success: æ“ä½œæ˜¯å¦æˆåŠŸ
    - task_id: åˆ›å»ºçš„ä»»åŠ¡ID
    - message: æ“ä½œç»“æœæ¶ˆæ¯
    """
    try:
        file_path = data.get("file_path")
        
        if not file_path:
            logger.warning("Pinæ–‡ä»¶è¯·æ±‚ä¸­æœªæä¾›æ–‡ä»¶è·¯å¾„")
            return {
                "success": False,
                "task_id": None,
                "message": "æ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º"
            }
        
        # éªŒè¯æ–‡ä»¶è·¯å¾„å’Œæƒé™
        if not os.path.exists(file_path):
            logger.warning(f"Pinæ–‡ä»¶å¤±è´¥ï¼Œæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return {
                "success": False,
                "task_id": None,
                "message": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
            }
        
        if not os.access(file_path, os.R_OK):
            logger.warning(f"Pinæ–‡ä»¶å¤±è´¥ï¼Œæ–‡ä»¶æ— è¯»å–æƒé™: {file_path}")
            return {
                "success": False,
                "task_id": None,
                "message": f"æ–‡ä»¶æ— è¯»å–æƒé™: {file_path}"
            }
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹æ˜¯å¦æ”¯æŒ
        supported_extensions = {'.pdf', '.docx', '.pptx', '.doc', '.ppt'}
        file_ext = Path(file_path).suffix.lower()
        if file_ext not in supported_extensions:
            logger.warning(f"Pinæ–‡ä»¶å¤±è´¥ï¼Œä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")
            return {
                "success": False,
                "task_id": None,
                "message": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}ï¼Œæ”¯æŒçš„ç±»å‹: {supported_extensions}"
            }
        
        # åˆ›å»ºHIGHä¼˜å…ˆçº§MULTIVECTORä»»åŠ¡
        task = task_mgr.add_task(
            task_name=f"Pinæ–‡ä»¶å¤šæ¨¡æ€å‘é‡åŒ–: {Path(file_path).name}",
            task_type=TaskType.MULTIVECTOR,
            priority=TaskPriority.HIGH,
            extra_data={"file_path": file_path}
        )
        
        logger.info(f"æˆåŠŸåˆ›å»ºPinæ–‡ä»¶çš„å¤šæ¨¡æ€å‘é‡åŒ–ä»»åŠ¡: {file_path} (Task ID: {task.id})")
        
        return {
            "success": True,
            "task_id": task.id,
            "message": f"å·²åˆ›å»ºå¤šæ¨¡æ€å‘é‡åŒ–ä»»åŠ¡ï¼ŒTask ID: {task.id}"
        }
        
    except Exception as e:
        logger.error(f"Pinæ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "success": False,
            "task_id": None,
            "message": f"Pinæ–‡ä»¶å¤±è´¥: {str(e)}"
        }

@app.get("/test-bridge-stdout")
def test_bridge_stdout():
    """æµ‹è¯•æ¡¥æ¥äº‹ä»¶çš„stdoutè¾“å‡ºèƒ½åŠ›"""
    from test_bridge_stdout import test_bridge_stdout_main
    test_bridge_stdout_main()
    return {"status": "ok"}


if __name__ == "__main__":
    try:
        parser = argparse.ArgumentParser()
        parser.add_argument("--port", type=int, default=60315, help="APIæœåŠ¡ç›‘å¬ç«¯å£")
        parser.add_argument("--host", type=str, default="127.0.0.1", help="APIæœåŠ¡ç›‘å¬åœ°å€")
        parser.add_argument("--db-path", type=str, default="knowledge-focus.db", help="æ•°æ®åº“æ–‡ä»¶è·¯å¾„")
        parser.add_argument("--mode", type=str, default="dev", help="æ ‡è®°æ˜¯å¼€å‘ç¯å¢ƒè¿˜æ˜¯ç”Ÿäº§ç¯å¢ƒ")
        args = parser.parse_args()

        # æ£€æŸ¥æ•°æ®åº“è·¯å¾„æ˜¯å¦å­˜åœ¨
        db_dir = os.path.dirname(os.path.abspath(args.db_path))
        if db_dir and not os.path.exists(db_dir):
            os.makedirs(db_dir, exist_ok=True)

        if args.mode is not None and args.mode == "dev":
            setup_logging()
        else:
            logging_dir = os.path.join(db_dir, "logs")
            setup_logging(logging_dir)

        logger = logging.getLogger(__name__)
        logger.info("APIæœåŠ¡ç¨‹åºå¯åŠ¨")
        logger.info(f"å‘½ä»¤è¡Œå‚æ•°: port={args.port}, host={args.host}, db_path={args.db_path}, mode={args.mode}")

        # æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨ï¼Œå¦‚æœè¢«å ç”¨åˆ™ç»ˆæ­¢å ç”¨è¿›ç¨‹
        try:
            logger.info(f"æ£€æŸ¥ç«¯å£ {args.port} æ˜¯å¦è¢«å ç”¨...")
            kill_process_on_port(args.port)
            time.sleep(2)  # ç­‰å¾…ç«¯å£é‡Šæ”¾
            logger.info(f"ç«¯å£ {args.port} å·²é‡Šæ”¾æˆ–æœ¬æ¥å°±æ²¡è¢«å ç”¨")
        except Exception as e:
            logger.error(f"é‡Šæ”¾ç«¯å£ {args.port} å¤±è´¥: {str(e)}", exc_info=True)
            # ç»§ç»­æ‰§è¡Œï¼Œç«¯å£å¯èƒ½æœ¬æ¥å°±æ²¡æœ‰è¢«å ç”¨
        
        # è®¾ç½®æ•°æ®åº“è·¯å¾„
        app.state.db_path = args.db_path
        logger.info(f"è®¾ç½®æ•°æ®åº“è·¯å¾„: {args.db_path}")
        
        # å¯åŠ¨æœåŠ¡å™¨
        logger.info(f"APIæœåŠ¡å¯åŠ¨åœ¨: http://{args.host}:{args.port}")        
        uvicorn.run(app, host=args.host, port=args.port, log_level="info")
    
    except Exception as e:
        logger.critical(f"APIæœåŠ¡å¯åŠ¨å¤±è´¥: {str(e)}", exc_info=True)

        # è¿”å›é€€å‡ºç 2ï¼Œè¡¨ç¤ºå‘ç”Ÿé”™è¯¯
        sys.exit(2)

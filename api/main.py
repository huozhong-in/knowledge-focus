import os
import sys
import argparse
import logging
import time
import pathlib
import asyncio
import threading
import json
from datetime import datetime
from typing import List, Dict, Any
from pathlib import Path
from contextlib import asynccontextmanager
from fastapi import FastAPI, Body, Depends
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from utils import kill_process_on_port, monitor_parent, kill_orphaned_processes
from sqlmodel import create_engine, Session, select
from api_cache_optimization import TimedCache, cached
from db_mgr import (
    DBManager, TaskStatus, TaskResult, TaskType, TaskPriority, Task,
    MyFiles, FileCategory, FileFilterRule, FileExtensionMap, ProjectRecognitionRule,
)
from myfiles_mgr import MyFilesManager
from screening_mgr import ScreeningManager
from file_tagging_mgr import FileTaggingMgr
from task_mgr import TaskManager
from lancedb_mgr import LanceDBMgr
from models_mgr import ModelsMgr
from search_mgr import SearchManager

# --- Centralized Logging Setup ---
def setup_logging(logging_dir: str = None):
    """
    Configures the root logger for the application.

    args:
        logging_dir (str): The directory where log files will be stored.
    """
    
    try:
        # Determine log directory
        if logging_dir is not None:
            log_dir = pathlib.Path(logging_dir)
        else:
            script_path = os.path.abspath(__file__)
            log_dir = pathlib.Path(script_path).parent / 'logs'
        if not log_dir.exists():
            log_dir.mkdir(exist_ok=True, parents=True)

        log_filename = f'api_{time.strftime("%Y%m%d")}.log'
        log_filepath = log_dir / log_filename

        # Get the root logger and configure it
        root_logger = logging.getLogger()
        root_logger.setLevel(logging.INFO)

        # Avoid adding duplicate handlers
        if not root_logger.handlers:
            # Console handler
            console_handler = logging.StreamHandler(sys.stdout)
            console_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            console_handler.setFormatter(console_formatter)
            root_logger.addHandler(console_handler)

            # File handler
            file_handler = logging.FileHandler(log_filepath, encoding='utf-8')
            file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            file_handler.setFormatter(file_formatter)
            root_logger.addHandler(file_handler)

    except Exception as e:
        # Fallback to basic config if setup fails
        logging.basicConfig(level=logging.INFO)
        logging.error(f"Error setting up custom logging: {e}", exc_info=True)

# å…¨å±€ç¼“å­˜å®ä¾‹
config_cache = TimedCache[Dict[str, Any]](expiry_seconds=300)  # 5åˆ†é’Ÿè¿‡æœŸ
folder_hierarchy_cache = TimedCache[List[Any]](expiry_seconds=180)  # 3åˆ†é’Ÿè¿‡æœŸ
bundle_ext_cache = TimedCache[List[str]](expiry_seconds=600)   # 10åˆ†é’Ÿè¿‡æœŸ

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨"""
    # åœ¨åº”ç”¨å¯åŠ¨æ—¶æ‰§è¡Œåˆå§‹åŒ–æ“ä½œ
    logger.info("åº”ç”¨æ­£åœ¨å¯åŠ¨...")
    
    try:
        logger.info(f"è°ƒè¯•ä¿¡æ¯: Pythonç‰ˆæœ¬ {sys.version}")
        logger.info(f"è°ƒè¯•ä¿¡æ¯: å½“å‰å·¥ä½œç›®å½• {os.getcwd()}")
        
        # åˆå§‹åŒ–æ•°æ®åº“å¼•æ“‹
        if hasattr(app.state, "db_path"):
            sqlite_url = f"sqlite:///{app.state.db_path}"
            logger.info(f"åˆå§‹åŒ–æ•°æ®åº“å¼•æ“ï¼ŒURL: {sqlite_url}")
            try:
                # For SQLite, especially when accessed by FastAPI (which can use threads for async routes)
                # and potentially by background tasks, 'check_same_thread': False is often needed.
                # The set_sqlite_pragma event listener will configure WAL mode.
                app.state.engine = create_engine(
                    sqlite_url, 
                    echo=False, 
                    connect_args={"check_same_thread": False, "timeout": 30},
                    pool_size=5,       # è®¾ç½®è¿æ¥æ± å¤§å°
                    max_overflow=10,   # å…è®¸çš„æœ€å¤§æº¢å‡ºè¿æ¥æ•°
                    pool_timeout=30,   # è·å–è¿æ¥çš„è¶…æ—¶æ—¶é—´
                    pool_recycle=1800  # 30åˆ†é’Ÿå›æ”¶ä¸€æ¬¡è¿æ¥
                )
                logger.info(f"æ•°æ®åº“å¼•æ“å·²åˆå§‹åŒ–ï¼Œè·¯å¾„: {app.state.db_path}")
                
                # åˆå§‹åŒ–æ•°æ®åº“ç»“æ„
                try:
                    logger.info("å¼€å§‹åˆå§‹åŒ–æ•°æ®åº“ç»“æ„...")
                    with Session(app.state.engine) as session:
                        db_mgr = DBManager(session)
                        db_mgr.init_db()
                    logger.info("æ•°æ®åº“ç»“æ„åˆå§‹åŒ–å®Œæˆ")
                except Exception as init_err:
                    logger.error(f"åˆå§‹åŒ–æ•°æ®åº“ç»“æ„å¤±è´¥: {str(init_err)}", exc_info=True)
                    # ç»§ç»­è¿è¡Œåº”ç”¨ï¼Œä¸è¦å› ä¸ºåˆå§‹åŒ–å¤±è´¥è€Œä¸­æ–­
                    # å¯èƒ½æ˜¯å› ä¸ºè¡¨å·²ç»å­˜åœ¨ï¼Œè¿™ç§æƒ…å†µæ˜¯æ­£å¸¸çš„
            except Exception as db_err:
                logger.error(f"åˆå§‹åŒ–æ•°æ®åº“å¼•æ“å¤±è´¥: {str(db_err)}", exc_info=True)
                raise
        else:
            logger.warning("æœªè®¾ç½®æ•°æ®åº“è·¯å¾„ï¼Œæ•°æ®åº“å¼•æ“æœªåˆå§‹åŒ–")
        
        # å…ˆæ¸…ç†å¯èƒ½å­˜åœ¨çš„å­¤ç«‹å­è¿›ç¨‹
        try:
            logger.info("æ¸…ç†å¯èƒ½å­˜åœ¨çš„å­¤ç«‹å­è¿›ç¨‹...")
            kill_orphaned_processes("python", "task_processor")
        except Exception as proc_err:
            logger.error(f"æ¸…ç†å­¤ç«‹è¿›ç¨‹å¤±è´¥: {str(proc_err)}", exc_info=True)
        
        # åˆå§‹åŒ–åå°ä»»åŠ¡å¤„ç†çº¿ç¨‹
        try:
            logger.info("åˆå§‹åŒ–åå°ä»»åŠ¡å¤„ç†çº¿ç¨‹...")
            # åˆ›å»ºä¸€ä¸ªäº‹ä»¶æ¥ä¼˜é›…åœ°åœæ­¢çº¿ç¨‹
            app.state.task_processor_stop_event = threading.Event()
            app.state.task_processor_thread = threading.Thread(
                target=task_processor,
                args=(app.state.db_path, app.state.task_processor_stop_event),
                daemon=True
            )
            app.state.task_processor_thread.start()
            logger.info("åå°ä»»åŠ¡å¤„ç†çº¿ç¨‹å·²å¯åŠ¨")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–åå°ä»»åŠ¡å¤„ç†çº¿ç¨‹å¤±è´¥: {e}", exc_info=True)
            raise
        
        # å¯åŠ¨é€šçŸ¥æ£€æŸ¥ä»»åŠ¡
        # try:
        #     logger.info("å¯åŠ¨é€šçŸ¥æ£€æŸ¥ä»»åŠ¡...")
        #     asyncio.create_task(check_notifications())
        # except Exception as notify_err:
        #     logger.error(f"å¯åŠ¨é€šçŸ¥æ£€æŸ¥ä»»åŠ¡å¤±è´¥: {str(notify_err)}", exc_info=True)
            
        # Start monitor can kill self process if parent process is dead or exit
        try:
            logger.info("å¯åŠ¨çˆ¶è¿›ç¨‹ç›‘æ§çº¿ç¨‹...")
            monitor_thread = threading.Thread(target=monitor_parent, daemon=True)
            monitor_thread.start()
            logger.info("çˆ¶è¿›ç¨‹ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨")
        except Exception as monitor_err:
            logger.error(f"å¯åŠ¨çˆ¶è¿›ç¨‹ç›‘æ§çº¿ç¨‹å¤±è´¥: {str(monitor_err)}", exc_info=True)

        # é…ç½®è§£æåº“çš„è­¦å‘Šå’Œæ—¥å¿—çº§åˆ«
        try:
            from file_tagging_mgr import configure_parsing_warnings
            configure_parsing_warnings()
            logger.info("è§£æåº“æ—¥å¿—é…ç½®å·²åº”ç”¨")
        except Exception as parsing_config_err:
            logger.error(f"é…ç½®è§£æåº“æ—¥å¿—å¤±è´¥: {str(parsing_config_err)}", exc_info=True)

        # æ­£å¼å¼€å§‹æœåŠ¡
        logger.info("åº”ç”¨åˆå§‹åŒ–å®Œæˆï¼Œå¼€å§‹æä¾›æœåŠ¡...")
        yield
    except Exception as e:
        logger.critical(f"åº”ç”¨å¯åŠ¨è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}", exc_info=True)
        # ç¡®ä¿å¼‚å¸¸ä¼ æ’­ï¼Œè¿™æ ·FastAPIä¼šçŸ¥é“å¯åŠ¨å¤±è´¥
        raise
    finally:
        # é€€å‡ºå‰çš„æ¸…ç†å·¥ä½œ
        logger.info("åº”ç”¨å¼€å§‹å…³é—­...")
        
        try:
            if hasattr(app.state, "task_processor_thread") and app.state.task_processor_thread.is_alive():
                logger.info("æ­£åœ¨åœæ­¢åå°ä»»åŠ¡å¤„ç†çº¿ç¨‹...")
                app.state.task_processor_stop_event.set()
                app.state.task_processor_thread.join(timeout=5) # ç­‰å¾…5ç§’
                if app.state.task_processor_thread.is_alive():
                    logger.warning("åå°ä»»åŠ¡å¤„ç†çº¿ç¨‹åœ¨5ç§’å†…æœªåœæ­¢")
                else:
                    logger.info("åå°ä»»åŠ¡å¤„ç†çº¿ç¨‹å·²åœæ­¢")
        except Exception as e:
            logger.error(f"åœæ­¢åå°ä»»åŠ¡å¤„ç†çº¿ç¨‹å¤±è´¥: {e}", exc_info=True)
        
        # åœ¨åº”ç”¨å…³é—­æ—¶æ‰§è¡Œæ¸…ç†æ“ä½œ
        try:
            if hasattr(app.state, "engine") and app.state.engine is not None:
                logger.info("é‡Šæ”¾æ•°æ®åº“è¿æ¥æ± ...")
                app.state.engine.dispose()  # é‡Šæ”¾æ•°æ®åº“è¿æ¥æ± 
                logger.info("æ•°æ®åº“è¿æ¥æ± å·²é‡Šæ”¾")
        except Exception as db_close_err:
            logger.error(f"å…³é—­æ•°æ®åº“è¿æ¥å¤±è´¥: {str(db_close_err)}", exc_info=True)
        
        logger.info("åº”ç”¨å·²å®Œå…¨å…³é—­")

app = FastAPI(lifespan=lifespan)
origins = [
    "http://localhost:1420",  # Your Tauri dev server
    "tauri://localhost",      # Often used by Tauri in production
    "https://tauri.localhost" # Also used by Tauri in production
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,  # Allows specific origins
    # allow_origins=["*"], # Or, to allow all origins (less secure, use with caution)
    allow_credentials=True, # Allows cookies to be included in requests
    allow_methods=["*"],    # Allows all methods (GET, POST, PUT, DELETE, etc.)
    allow_headers=["*"],    # Allows all headers
)


# å‘¨æœŸæ€§æ£€æŸ¥æ–°é€šçŸ¥å¹¶å¹¿æ’­
# async def check_notifications():
#     while True:
#         # å¹¿æ’­æ¶ˆæ¯
#         # await manager.broadcast("New notification")
#         await asyncio.sleep(8)

def get_session():
    """FastAPIä¾èµ–å‡½æ•°ï¼Œç”¨äºè·å–æ•°æ®åº“ä¼šè¯"""
    if not hasattr(app.state, "engine") or app.state.engine is None:
        # ç¡®ä¿æ•°æ®åº“å¼•æ“å·²åˆå§‹åŒ–
        raise RuntimeError("æ•°æ®åº“å¼•æ“æœªåˆå§‹åŒ–")
    
    with Session(app.state.engine) as session:
        yield session


# # æœ¬åœ°å¤§æ¨¡å‹APIç«¯ç‚¹æ·»åŠ 
from models_api import get_router as get_models_router
models_router = get_models_router(external_get_session=get_session)
app.include_router(models_router, prefix="", tags=["local-models"])

# Add the new tagging API router
from tagging_api import get_router as get_tagging_router
tagging_router = get_tagging_router(get_session=get_session)
app.include_router(tagging_router, prefix="", tags=["tagging"])

# è·å– MyFilesManager çš„ä¾èµ–å‡½æ•°
def get_myfiles_manager(session: Session = Depends(get_session)):
    """è·å–æ–‡ä»¶/æ–‡ä»¶å¤¹ç®¡ç†å™¨å®ä¾‹"""
    return MyFilesManager(session)

# è·å–æ‰€æœ‰é…ç½®ä¿¡æ¯çš„APIç«¯ç‚¹
@app.get("/config/all")
async def get_all_configuration(
    session: Session = Depends(get_session),
    myfiles_mgr: MyFilesManager = Depends(get_myfiles_manager)
):
    """
    è·å–æ‰€æœ‰Rustç«¯è¿›è¡Œæ–‡ä»¶å¤„ç†æ‰€éœ€çš„é…ç½®ä¿¡æ¯ã€‚
    åŒ…æ‹¬æ–‡ä»¶åˆ†ç±»ã€ç²—ç­›è§„åˆ™ã€æ–‡ä»¶æ‰©å±•åæ˜ å°„ã€é¡¹ç›®è¯†åˆ«è§„åˆ™ä»¥åŠç›‘æ§çš„æ–‡ä»¶å¤¹åˆ—è¡¨ã€‚
    ç°åœ¨ä½¿ç”¨ç¼“å­˜æœºåˆ¶æé«˜æ€§èƒ½ï¼Œå‡å°‘æ•°æ®åº“æŸ¥è¯¢ã€‚
    """
    try:
        # ä½¿ç”¨å¼‚æ­¥è¶…æ—¶æ§åˆ¶
        try:
            return await asyncio.wait_for(
                _get_all_configuration_async(session, myfiles_mgr), 
                timeout=5.0  # è®¾ç½®5ç§’è¶…æ—¶
            )
        except asyncio.TimeoutError:
            logger.error("è·å–é…ç½®è¶…æ—¶ï¼Œè¿”å›ç¼“å­˜æ•°æ®æˆ–ç©ºç»“æœ")
            # å°è¯•ä»ç¼“å­˜è·å–
            hit, cached_value = config_cache.get("config_all")
            if hit:
                logger.info("ä½¿ç”¨ç¼“å­˜æ•°æ®å“åº”è¶…æ—¶è¯·æ±‚")
                return cached_value
            # æ²¡æœ‰ç¼“å­˜ï¼Œè¿”å›ç©ºç»“æœ
            raise Exception("è·å–é…ç½®è¶…æ—¶")
    except Exception as e:
        logger.error(f"Error fetching all configuration: {e}", exc_info=True)
        # Return a default structure in case of error to prevent client-side parsing issues.
        # The client can check for the presence of 'error_message' or if data arrays are empty.
        return {
            "file_categories": [],
            "file_filter_rules": [],
            "file_extension_maps": [],
            "project_recognition_rules": [],
            "monitored_folders": [],
            "full_disk_access": False,  # Default to false on error
            "error_message": f"Failed to fetch configuration: {str(e)}"
        }

async def _get_all_configuration_async(session: Session, myfiles_mgr: MyFilesManager):
    """å¼‚æ­¥åŒ…è£…ç¼“å­˜å‡½æ•°ï¼Œç”¨äºè¶…æ—¶æ§åˆ¶"""
    return _get_all_configuration_cached(session, myfiles_mgr)

@cached(config_cache, "config_all")
def _get_all_configuration_cached(session: Session, myfiles_mgr: MyFilesManager):
    """ç¼“å­˜ç‰ˆæœ¬çš„é…ç½®è·å–å‡½æ•°"""
    start_time = time.time()
    file_categories = session.exec(select(FileCategory)).all()
    file_filter_rules = session.exec(select(FileFilterRule)).all()
    file_extension_maps = session.exec(select(FileExtensionMap)).all()
    project_recognition_rules = session.exec(select(ProjectRecognitionRule)).all()
    monitored_folders = session.exec(select(MyFiles)).all()
    
    # æ£€æŸ¥å®Œå…¨ç£ç›˜è®¿é—®æƒé™çŠ¶æ€ 
    full_disk_access = False
    if sys.platform == "darwin":  # macOS
        access_status = myfiles_mgr.check_full_disk_access_status()
        full_disk_access = access_status.get("has_full_disk_access", False)
        logger.info(f"[CONFIG] Full disk access status: {full_disk_access}")
    
    elapsed = time.time() - start_time
    logger.info(f"[CONFIG] è·å–æ‰€æœ‰é…ç½®è€—æ—¶ {elapsed:.3f}s (ä»æ•°æ®åº“)")
    
    # è·å– bundle æ‰©å±•ååˆ—è¡¨ï¼ˆç›´æ¥ä»æ•°æ®åº“è·å–ï¼Œä¸ä½¿ç”¨æ­£åˆ™è§„åˆ™ï¼‰
    bundle_extensions = myfiles_mgr.get_bundle_extensions_for_rust()
    logger.info(f"[CONFIG] è·å–åˆ° {len(bundle_extensions)} ä¸ª bundle æ‰©å±•å")
    from file_tagging_mgr import PARSEABLE_EXTENSIONS  # ç¡®ä¿è§£æå™¨æ‰©å±•åå·²åŠ è½½
    return {
        "file_categories": file_categories,
        "file_filter_rules": file_filter_rules,
        "file_extension_maps": file_extension_maps,
        "project_recognition_rules": project_recognition_rules,
        "monitored_folders": monitored_folders,
        "parsable_extensions": PARSEABLE_EXTENSIONS,
        "full_disk_access": full_disk_access,  # å®Œå…¨ç£ç›˜è®¿é—®æƒé™çŠ¶æ€
        "bundle_extensions": bundle_extensions  # æ·»åŠ ç›´æ¥å¯ç”¨çš„ bundle æ‰©å±•ååˆ—è¡¨
    }

# ä»»åŠ¡å¤„ç†è€…
def task_processor(db_path: str, stop_event: threading.Event):
    """å¤„ç†ä»»åŠ¡çš„åå°å·¥ä½œçº¿ç¨‹"""
    logger.info(f"ä»»åŠ¡å¤„ç†çº¿ç¨‹å·²å¯åŠ¨")
    sqlite_url = f"sqlite:///{db_path}"
    engine = create_engine(
        sqlite_url, 
        echo=False, 
        connect_args={"check_same_thread": False, "timeout": 30}
    )
    db_directory = os.path.dirname(db_path)
    lancedb_mgr = LanceDBMgr(base_dir=db_directory)

    while not stop_event.is_set():
        try:
            with Session(engine) as session:
                task_mgr = TaskManager(session)
                task: Task = task_mgr.get_next_task()

                if task is None:
                    time.sleep(5) # æ²¡æœ‰ä»»åŠ¡æ—¶ï¼Œç­‰å¾…5ç§’
                    continue

                logger.info(f"ä»»åŠ¡å¤„ç†çº¿ç¨‹æ¥æ”¶ä»»åŠ¡: ID={task.id}, Name='{task.task_name}', Type='{task.task_type}', Priority={task.priority}")
                task_mgr.update_task_status(task.id, TaskStatus.RUNNING)

                models_mgr = ModelsMgr(session)
                file_tagging_mgr = FileTaggingMgr(session, lancedb_mgr, models_mgr)

                if task.task_type == TaskType.TAGGING.value:
                    # æ£€æŸ¥åŸºç¡€æ¨¡å‹å¯ç”¨æ€§
                    if not file_tagging_mgr.check_base_model_availability():
                        logger.error("åŸºç¡€æ¨¡å‹ä¸å¯ç”¨ï¼Œæ— æ³•å¤„ç†æ–‡ä»¶æ‰“æ ‡ç­¾ä»»åŠ¡")
                        return
                    # é«˜ä¼˜å…ˆçº§ä»»åŠ¡: é€šå¸¸æ˜¯å•ä¸ªæ–‡ä»¶å¤„ç†
                    if task.priority == TaskPriority.HIGH.value and task.extra_data and 'screening_result_id' in task.extra_data:
                        logger.info(f"å¼€å§‹å¤„ç†é«˜ä¼˜å…ˆçº§æ–‡ä»¶æ‰“æ ‡ç­¾ä»»åŠ¡ (Task ID: {task.id})")
                        success = file_tagging_mgr.process_single_file_task(task.extra_data['screening_result_id'])
                        if success:
                            task_mgr.update_task_status(task.id, TaskStatus.COMPLETED, result=TaskResult.SUCCESS)
                            
                            # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨è¡”æ¥MULTIVECTORä»»åŠ¡ï¼ˆä»…å½“æ–‡ä»¶è¢«pinæ—¶ï¼‰
                            _check_and_create_multivector_task(session, task_mgr, task.extra_data.get('screening_result_id'))
                        else:
                            task_mgr.update_task_status(task.id, TaskStatus.FAILED, result=TaskResult.FAILURE)
                    # ä½ä¼˜å…ˆçº§ä»»åŠ¡: æ‰¹é‡å¤„ç†
                    else:
                        logger.info(f"å¼€å§‹æ‰¹é‡æ–‡ä»¶æ‰“æ ‡ç­¾ä»»åŠ¡ (Task ID: {task.id})")
                        result_data = file_tagging_mgr.process_pending_batch(task_id=task.id, batch_size=10) # æ¯æ¬¡å¤„ç†10ä¸ª
                        
                        # æ— è®ºæ‰¹é‡ä»»åŠ¡å¤„ç†äº†å¤šå°‘æ–‡ä»¶ï¼Œéƒ½å°†è§¦å‘ä»»åŠ¡æ–‡ä»¶æ‰“æ ‡ç­¾ä¸ºå®Œæˆ
                        task_mgr.update_task_status(
                            task.id, 
                            TaskStatus.COMPLETED, 
                            result=TaskResult.SUCCESS, 
                            message=f"æ‰¹é‡å¤„ç†å®Œæˆ: å¤„ç†äº† {result_data.get('processed', 0)} ä¸ªæ–‡ä»¶ã€‚"
                        )
                
                elif task.task_type == TaskType.MULTIVECTOR.value:
                    from multivector_mgr import MultiVectorMgr
                    mv_mgr = MultiVectorMgr(session, lancedb_mgr, models_mgr)
                    if not mv_mgr.check_vision_embedding_model_availability():
                        logger.error("è§†è§‰æ¨¡å‹æˆ–å‘é‡æ¨¡å‹ä¸å¯ç”¨ï¼Œæ— æ³•å¤„ç†å¤šæ¨¡æ€å‘é‡åŒ–ä»»åŠ¡")
                        return
                    # é«˜ä¼˜å…ˆçº§ä»»åŠ¡: å•æ–‡ä»¶å¤„ç†ï¼ˆç”¨æˆ·pinæ“ä½œæˆ–æ–‡ä»¶å˜åŒ–è¡”æ¥ï¼‰
                    if task.priority == TaskPriority.HIGH.value and task.extra_data and 'file_path' in task.extra_data:
                        file_path = task.extra_data['file_path']
                        logger.info(f"å¼€å§‹å¤„ç†é«˜ä¼˜å…ˆçº§å¤šæ¨¡æ€å‘é‡åŒ–ä»»åŠ¡ (Task ID: {task.id}): {file_path}")
                        
                        try:
                            # ä¼ é€’task_idä»¥ä¾¿äº‹ä»¶è¿½è¸ª
                            success = mv_mgr.process_document(file_path, str(task.id))
                            if success:
                                task_mgr.update_task_status(
                                    task.id, 
                                    TaskStatus.COMPLETED, 
                                    result=TaskResult.SUCCESS,
                                    message=f"å¤šæ¨¡æ€å‘é‡åŒ–å®Œæˆ: {file_path}"
                                )
                                logger.info(f"å¤šæ¨¡æ€å‘é‡åŒ–æˆåŠŸå®Œæˆ: {file_path}")
                            else:
                                task_mgr.update_task_status(
                                    task.id, 
                                    TaskStatus.FAILED, 
                                    result=TaskResult.FAILURE,
                                    message=f"å¤šæ¨¡æ€å‘é‡åŒ–å¤±è´¥: {file_path}"
                                )
                                logger.error(f"å¤šæ¨¡æ€å‘é‡åŒ–å¤±è´¥: {file_path}")
                        except Exception as e:
                            error_msg = f"å¤šæ¨¡æ€å‘é‡åŒ–å¼‚å¸¸: {file_path} - {str(e)}"
                            task_mgr.update_task_status(
                                task.id, 
                                TaskStatus.FAILED, 
                                result=TaskResult.FAILURE,
                                message=error_msg
                            )
                            logger.error(error_msg, exc_info=True)
                    else:
                        # ä¸­ä½ä¼˜å…ˆçº§ä»»åŠ¡: æ‰¹é‡å¤„ç†ï¼ˆæœªæ¥æ”¯æŒï¼‰
                        logger.info(f"æ‰¹é‡å¤šæ¨¡æ€å‘é‡åŒ–ä»»åŠ¡æš‚æœªå®ç° (Task ID: {task.id})")
                        task_mgr.update_task_status(
                            task.id, 
                            TaskStatus.COMPLETED, 
                            result=TaskResult.SUCCESS,
                            message="æ‰¹é‡å¤šæ¨¡æ€å‘é‡åŒ–ä»»åŠ¡å·²è·³è¿‡"
                        )
                
                else:
                    logger.warning(f"æœªçŸ¥çš„ä»»åŠ¡ç±»å‹: {task.task_type} for task ID: {task.id}")
                    task_mgr.update_task_status(task.id, TaskStatus.FAILED, result=TaskResult.FAILURE, message=f"Unknown task type: {task.task_type}")

        except Exception as e:
            logger.error(f"ä»»åŠ¡å¤„ç†çº¿ç¨‹å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
            time.sleep(30)

    logger.info("ä»»åŠ¡å¤„ç†çº¿ç¨‹å·²åœæ­¢")

def _check_and_create_multivector_task(session: Session, task_mgr: TaskManager, screening_result_id: int):
    """
    æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å¤„äºpinçŠ¶æ€ï¼Œå¦‚æœæ˜¯åˆ™è‡ªåŠ¨åˆ›å»ºMULTIVECTORä»»åŠ¡
    
    Args:
        session: æ•°æ®åº“ä¼šè¯
        task_mgr: ä»»åŠ¡ç®¡ç†å™¨
        screening_result_id: ç²—ç­›ç»“æœID
    """
    if not screening_result_id:
        return
    
    try:
        from screening_mgr import ScreeningResult
        # è·å–ç²—ç­›ç»“æœï¼ŒåŒ…å«æ–‡ä»¶è·¯å¾„ä¿¡æ¯
        screening_result = session.get(ScreeningResult, screening_result_id)
        if not screening_result:
            logger.warning(f"æœªæ‰¾åˆ°screening_result_id: {screening_result_id}")
            return
        
        file_path = screening_result.file_path
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨æœ€è¿‘8å°æ—¶å†…è¢«pinè¿‡
        is_recently_pinned = _check_file_pin_status(file_path, session)
        
        if is_recently_pinned:
            logger.info(f"æ–‡ä»¶ {file_path} åœ¨æœ€è¿‘8å°æ—¶å†…è¢«pinè¿‡ï¼Œåˆ›å»ºMULTIVECTORä»»åŠ¡")
            task_mgr.add_task(
                task_name=f"å¤šæ¨¡æ€å‘é‡åŒ–: {Path(file_path).name}",
                task_type=TaskType.MULTIVECTOR,
                priority=TaskPriority.HIGH,
                extra_data={"file_path": file_path},
                target_file_path=file_path  # è®¾ç½®å†—ä½™å­—æ®µä¾¿äºæŸ¥è¯¢
            )
        else:
            logger.info(f"æ–‡ä»¶ {file_path} åœ¨æœ€è¿‘8å°æ—¶å†…æœªè¢«pinè¿‡ï¼Œè·³è¿‡MULTIVECTORä»»åŠ¡")
            
    except Exception as e:
        logger.error(f"æ£€æŸ¥å’Œåˆ›å»ºMULTIVECTORä»»åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)

def _check_file_pin_status(file_path: str, session: Session) -> bool:
    """
    æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨æœ€è¿‘8å°æ—¶å†…è¢«pinè¿‡ï¼ˆå³æœ‰æˆåŠŸçš„MULTIVECTORä»»åŠ¡ï¼‰
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        session: æ•°æ®åº“ä¼šè¯
        
    Returns:
        bool: æ–‡ä»¶æ˜¯å¦åœ¨æœ€è¿‘8å°æ—¶å†…è¢«pinè¿‡
    """
    try:
        task_mgr = TaskManager(session)
        return task_mgr.is_file_recently_pinned(file_path, hours=8)
    except Exception as e:
        logger.error(f"æ£€æŸ¥æ–‡ä»¶pinçŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return False
    return file_ext in important_extensions

# è·å– ScreeningManager çš„ä¾èµ–å‡½æ•°
def get_screening_manager(session: Session = Depends(get_session)):
    """è·å–æ–‡ä»¶ç²—ç­›ç»“æœç®¡ç†ç±»å®ä¾‹"""
    return ScreeningManager(session)

# è·å– FileTaggingMgr çš„ä¾èµ–å‡½æ•°
def get_file_tagging_manager(session: Session = Depends(get_session)):
    """è·å–æ–‡ä»¶è§£æç®¡ç†ç±»å®ä¾‹"""
    return FileTaggingMgr(session)

# è·å– TaskManager çš„ä¾èµ–å‡½æ•°
def get_task_manager(session: Session = Depends(get_session)):
    """è·å–ä»»åŠ¡ç®¡ç†å™¨å®ä¾‹"""
    return TaskManager(session)

def get_lancedb_manager():
    """è·å–LanceDBç®¡ç†å™¨å®ä¾‹"""
    if not hasattr(app.state, "engine") or app.state.engine is None:
        raise RuntimeError("æ•°æ®åº“å¼•æ“æœªåˆå§‹åŒ–")
    
    # ä»SQLiteæ•°æ®åº“è·¯å¾„æ¨å¯¼å‡ºbase_dir
    sqlite_url = str(app.state.engine.url)
    if sqlite_url.startswith('sqlite:///'):
        db_path = sqlite_url.replace('sqlite:///', '')
        db_directory = os.path.dirname(db_path)
        return LanceDBMgr(base_dir=db_directory)
    else:
        raise RuntimeError("æ— æ³•ä»æ•°æ®åº“URLæ¨å¯¼å‡ºLanceDBè·¯å¾„")

def get_models_manager(session: Session = Depends(get_session)):
    """è·å–æ¨¡å‹ç®¡ç†å™¨å®ä¾‹"""
    return ModelsMgr(session)

def get_search_manager(
    session: Session = Depends(get_session),
    lancedb_mgr: LanceDBMgr = Depends(get_lancedb_manager),
    models_mgr: ModelsMgr = Depends(get_models_manager)
):
    """è·å–æœç´¢ç®¡ç†å™¨å®ä¾‹"""
    return SearchManager(session, lancedb_mgr, models_mgr)

def get_multivector_manager(
    session: Session = Depends(get_session),
    lancedb_mgr: LanceDBMgr = Depends(get_lancedb_manager),
    models_mgr: ModelsMgr = Depends(get_models_manager)
):
    """è·å–å¤šæ¨¡æ€å‘é‡ç®¡ç†å™¨å®ä¾‹"""
    from multivector_mgr import MultiVectorMgr
    return MultiVectorMgr(session, lancedb_mgr, models_mgr)

@app.post("/pin-file")
def pin_file(
    request: Dict[str, Any] = Body(...),
    task_mgr: TaskManager = Depends(get_task_manager)
):
    """
    Pinä¸€ä¸ªæ–‡ä»¶ï¼Œåˆ›å»ºé«˜ä¼˜å…ˆçº§çš„å¤šæ¨¡æ€å‘é‡åŒ–ä»»åŠ¡
    
    å‚æ•°:
    - file_path: æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
    
    è¿”å›:
    - task_id: åˆ›å»ºçš„ä»»åŠ¡ID
    - message: çŠ¶æ€ä¿¡æ¯
    """
    try:
        file_path = request.get("file_path")
        if not file_path:
            return {"success": False, "error": "ç¼ºå°‘file_pathå‚æ•°"}
        
        # éªŒè¯æ–‡ä»¶è·¯å¾„
        if not os.path.exists(file_path):
            return {"success": False, "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}
        
        if not os.path.isfile(file_path):
            return {"success": False, "error": f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {file_path}"}
        
        # æ£€æŸ¥æ–‡ä»¶æƒé™
        if not os.access(file_path, os.R_OK):
            return {"success": False, "error": f"æ–‡ä»¶æ— è¯»å–æƒé™: {file_path}"}
        
        # åˆ›å»ºé«˜ä¼˜å…ˆçº§MULTIVECTORä»»åŠ¡
        file_name = os.path.basename(file_path)
        task = task_mgr.add_task(
            task_name=f"Pinæ–‡ä»¶å‘é‡åŒ–: {file_name}",
            task_type=TaskType.MULTIVECTOR,
            priority=TaskPriority.HIGH,
            extra_data={"file_path": file_path, "source": "user_pin"},
            target_file_path=file_path
        )
        
        logger.info(f"ç”¨æˆ·Pinæ–‡ä»¶æˆåŠŸï¼Œåˆ›å»ºä»»åŠ¡ID: {task.id}, æ–‡ä»¶: {file_path}")
        
        return {
            "success": True,
            "task_id": task.id,
            "message": f"æ–‡ä»¶PinæˆåŠŸï¼Œæ­£åœ¨å¤„ç†: {file_name}",
            "file_path": file_path
        }
        
    except Exception as e:
        logger.error(f"Pinæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return {"success": False, "error": f"Pinæ–‡ä»¶å¤±è´¥: {str(e)}"}

@app.get("/task/{task_id}")
def get_task_status(task_id: int, task_mgr: TaskManager = Depends(get_task_manager)):
    """
    è·å–ä»»åŠ¡çŠ¶æ€
    
    å‚æ•°:
    - task_id: ä»»åŠ¡ID
    
    è¿”å›:
    - ä»»åŠ¡è¯¦ç»†ä¿¡æ¯
    """
    try:
        task = task_mgr.get_task(task_id)
        if not task:
            return {"success": False, "error": f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}"}
        
        return {
            "success": True,
            "task": {
                "id": task.id,
                "task_name": task.task_name,
                "task_type": task.task_type,
                "priority": task.priority,
                "status": task.status,
                "result": task.result,
                "error_message": task.error_message,
                "created_at": task.created_at,
                "updated_at": task.updated_at,
                "start_time": task.start_time,
                "extra_data": task.extra_data,
                "target_file_path": task.target_file_path
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡çŠ¶æ€æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return {"success": False, "error": f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}"}

@app.get("/images/{image_filename}")
def get_image(image_filename: str, session: Session = Depends(get_session)):
    """
    è·å–å›¾ç‰‡æ–‡ä»¶å†…å®¹
    
    å‚æ•°:
    - image_filename: å›¾ç‰‡æ–‡ä»¶å (ä¾‹å¦‚: image_000000_hash.png)
    
    è¿”å›:
    - å›¾ç‰‡æ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹
    """
    try:
        from fastapi.responses import FileResponse
        from pathlib import Path
        import os
        
        # éªŒè¯æ–‡ä»¶åæ ¼å¼ï¼ˆå®‰å…¨æ£€æŸ¥ï¼‰
        if not image_filename.endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):
            return {"success": False, "error": "ä¸æ”¯æŒçš„å›¾ç‰‡æ ¼å¼"}
        
        if ".." in image_filename or "/" in image_filename or "\\" in image_filename:
            return {"success": False, "error": "æ— æ•ˆçš„æ–‡ä»¶å"}
        
        # è·å–doclingç¼“å­˜ç›®å½•
        try:
            # ä»æ•°æ®åº“å¼•æ“è·å–åŸºç¡€ç›®å½•
            sqlite_url = str(app.state.engine.url)
            if sqlite_url.startswith('sqlite:///'):
                db_path = sqlite_url.replace('sqlite:///', '')
                base_dir = Path(db_path).parent
            else:
                base_dir = Path.cwd()
            
            docling_cache_dir = base_dir / "docling_cache"
            image_path = docling_cache_dir / image_filename
            
        except Exception as e:
            logger.error(f"è·å–doclingç¼“å­˜ç›®å½•å¤±è´¥: {e}")
            return {"success": False, "error": "æ— æ³•ç¡®å®šå›¾ç‰‡å­˜å‚¨ä½ç½®"}
        
        # æ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not image_path.exists():
            logger.warning(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
            return {"success": False, "error": f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_filename}"}
        
        # éªŒè¯è¿™ä¸ªå›¾ç‰‡æ˜¯å¦å±äºæŸä¸ªå·²å¤„ç†çš„æ–‡æ¡£ï¼ˆå®‰å…¨æ£€æŸ¥ï¼‰
        from sqlmodel import select
        from db_mgr import ParentChunk
        
        # æŸ¥æ‰¾åŒ…å«æ­¤å›¾ç‰‡æ–‡ä»¶åçš„ParentChunkï¼ˆåœ¨metadataçš„image_file_pathä¸­æŸ¥æ‰¾ï¼‰
        stmt = select(ParentChunk).where(
            ParentChunk.chunk_type == "image",
            ParentChunk.metadata_json.contains(image_filename)
        )
        chunk = session.exec(stmt).first()
        
        if not chunk:
            logger.warning(f"å›¾ç‰‡æ–‡ä»¶æœªåœ¨æ•°æ®åº“ä¸­æ‰¾åˆ°å…³è”è®°å½•: {image_filename}")
            return {"success": False, "error": "å›¾ç‰‡æ–‡ä»¶æ— æ•ˆæˆ–å·²è¿‡æœŸ"}
        
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åç¡®å®šæ­£ç¡®çš„ MIME ç±»å‹
        file_ext = image_filename.lower().split('.')[-1]
        mime_type_map = {
            'png': 'image/png',
            'jpg': 'image/jpeg',
            'jpeg': 'image/jpeg',
            'gif': 'image/gif',
            'bmp': 'image/bmp',
            'webp': 'image/webp'
        }
        media_type = mime_type_map.get(file_ext, 'image/png')
        
        # è¿”å›å›¾ç‰‡æ–‡ä»¶
        return FileResponse(
            path=str(image_path),
            media_type=media_type,
            headers={"Content-Disposition": "inline"}  # è®©æµè§ˆå™¨ç›´æ¥æ˜¾ç¤ºè€Œä¸æ˜¯ä¸‹è½½
        )
        
    except Exception as e:
        logger.error(f"è·å–å›¾ç‰‡æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return {"success": False, "error": f"è·å–å›¾ç‰‡å¤±è´¥: {str(e)}"}

@app.get("/images/by-chunk/{parent_chunk_id}")
def get_image_by_chunk(parent_chunk_id: int, session: Session = Depends(get_session)):
    """
    é€šè¿‡ParentChunk IDè·å–å…³è”çš„å›¾ç‰‡
    
    å‚æ•°:
    - parent_chunk_id: çˆ¶å—ID
    
    è¿”å›:
    - å›¾ç‰‡æ–‡ä»¶çš„äºŒè¿›åˆ¶å†…å®¹ï¼Œæˆ–é‡å®šå‘åˆ°å›¾ç‰‡ç«¯ç‚¹
    """
    try:
        from fastapi.responses import RedirectResponse
        from pathlib import Path
        from sqlmodel import select
        from db_mgr import ParentChunk
        
        # æŸ¥æ‰¾æŒ‡å®šçš„ParentChunk
        stmt = select(ParentChunk).where(
            ParentChunk.id == parent_chunk_id,
            ParentChunk.chunk_type == "image"
        )
        chunk = session.exec(stmt).first()
        
        if not chunk:
            return {"success": False, "error": f"å›¾ç‰‡å—ä¸å­˜åœ¨: {parent_chunk_id}"}
        
        # ä»chunkä¸­æå–å›¾ç‰‡æ–‡ä»¶è·¯å¾„
        image_filename = None
        
        # ä»metadataä¸­è·å–image_file_path
        try:
            metadata = json.loads(chunk.metadata_json)
            image_file_path = metadata.get("image_file_path")
            
            if image_file_path and os.path.exists(image_file_path):
                # metadataä¸­æœ‰å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
                image_path = Path(image_file_path)
                image_filename = image_path.name
                logger.info(f"Found image file from metadata: {image_filename}")
            else:
                logger.warning(f"Image file path not found or file does not exist: {image_file_path}")
                        
        except Exception as e:
            logger.warning(f"æ— æ³•ä»metadataæå–å›¾ç‰‡è·¯å¾„: {e}")
        
        if not image_filename:
            return {"success": False, "error": "æ— æ³•ç¡®å®šå›¾ç‰‡æ–‡ä»¶è·¯å¾„"}
        
        # é‡å®šå‘åˆ°å›¾ç‰‡è·å–ç«¯ç‚¹
        return RedirectResponse(url=f"/images/{image_filename}")
        
    except Exception as e:
        logger.error(f"é€šè¿‡chunkè·å–å›¾ç‰‡æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return {"success": False, "error": f"è·å–å›¾ç‰‡å¤±è´¥: {str(e)}"}

@app.get("/documents/{document_id}/images")
def get_document_images(document_id: int, session: Session = Depends(get_session)):
    """
    è·å–æ–‡æ¡£ä¸­çš„æ‰€æœ‰å›¾ç‰‡åˆ—è¡¨
    
    å‚æ•°:
    - document_id: æ–‡æ¡£ID
    
    è¿”å›:
    - å›¾ç‰‡åˆ—è¡¨ï¼ŒåŒ…å«chunk_idã€æ–‡ä»¶åã€æè¿°ç­‰ä¿¡æ¯
    """
    try:
        from sqlmodel import select
        from db_mgr import ParentChunk
        from pathlib import Path
        import json
        
        # æŸ¥æ‰¾æ–‡æ¡£ä¸­æ‰€æœ‰çš„å›¾ç‰‡å—
        stmt = select(ParentChunk).where(
            ParentChunk.document_id == document_id,
            ParentChunk.chunk_type == "image"
        )
        image_chunks = session.exec(stmt).all()
        
        images = []
        for chunk in image_chunks:
            try:
                # æå–å›¾ç‰‡æ–‡ä»¶å - ä»metadataä¸­è·å–
                image_filename = None
                
                # ä»metadataä¸­è·å–image_file_path
                try:
                    metadata = json.loads(chunk.metadata_json)
                    image_file_path = metadata.get("image_file_path")
                    
                    if image_file_path and os.path.exists(image_file_path):
                        # metadataä¸­æœ‰å®Œæ•´çš„æ–‡ä»¶è·¯å¾„
                        image_path = Path(image_file_path)
                        image_filename = image_path.name
                    else:
                        logger.warning(f"Image file path not found or file does not exist for chunk {chunk.id}: {image_file_path}")
                                
                except Exception as e:
                    logger.warning(f"å¤„ç†å›¾ç‰‡å— {chunk.id} metadataæ—¶å‡ºé”™: {e}")
                
                # å¦‚æœæ— æ³•ç¡®å®šæ–‡ä»¶åï¼Œè·³è¿‡è¿™ä¸ªå›¾ç‰‡å—
                if not image_filename:
                    logger.warning(f"æ— æ³•ç¡®å®šå›¾ç‰‡å— {chunk.id} çš„æ–‡ä»¶åï¼Œè·³è¿‡")
                    continue
                
                # è·å–å›¾ç‰‡æè¿° - ç°åœ¨ç›´æ¥ä»contentå­—æ®µè·å–
                image_description = chunk.content if chunk.content else ""
                
                images.append({
                    "chunk_id": chunk.id,
                    "filename": image_filename,
                    "description": image_description,
                    "image_url": f"/images/{image_filename}",
                    "chunk_url": f"/images/by-chunk/{chunk.id}"
                })
                
            except Exception as e:
                logger.warning(f"å¤„ç†å›¾ç‰‡å— {chunk.id} æ—¶å‡ºé”™: {e}")
                continue
        
        return {
            "success": True,
            "document_id": document_id,
            "images": images,
            "total_count": len(images)
        }
        
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£å›¾ç‰‡åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return {"success": False, "error": f"è·å–å›¾ç‰‡åˆ—è¡¨å¤±è´¥: {str(e)}"}

# =============================================================================
# ğŸ“Š å‘é‡å†…å®¹æœç´¢APIç«¯ç‚¹ - P0æ ¸å¿ƒåŠŸèƒ½
# =============================================================================

@app.post("/search/content")
def search_document_content(
    request: Dict[str, Any] = Body(...),
    search_mgr: SearchManager = Depends(get_search_manager)
):
    """
    æ–‡æ¡£å†…å®¹çš„è‡ªç„¶è¯­è¨€æ£€ç´¢ - P0æ ¸å¿ƒAPI
    
    å‚æ•°:
    - query: è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ–‡æœ¬
    - top_k: è¿”å›çš„æœ€å¤§ç»“æœæ•° (å¯é€‰ï¼Œé»˜è®¤10)
    - document_ids: æ–‡æ¡£IDè¿‡æ»¤åˆ—è¡¨ (å¯é€‰)
    - distance_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ (å¯é€‰)
    
    è¿”å›:
    - success: æ˜¯å¦æˆåŠŸ
    - results: æ ¼å¼åŒ–çš„æ£€ç´¢ç»“æœ
    - query_info: æŸ¥è¯¢å…ƒä¿¡æ¯
    """
    try:
        # æå–å‚æ•°
        query = request.get("query", "").strip()
        top_k = request.get("top_k", 10)
        document_ids = request.get("document_ids")
        distance_threshold = request.get("distance_threshold")
        
        logger.info(f"[SEARCH API] Content search request: '{query[:50]}...'")
        
        # åŸºç¡€éªŒè¯
        if not query:
            return {
                "success": False,
                "error": "æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º",
                "results": None
            }
        
        # æ‰§è¡Œæœç´¢
        search_result = search_mgr.search_documents(
            query=query,
            top_k=top_k,
            document_ids=document_ids,
            distance_threshold=distance_threshold
        )
        
        # è¿”å›ç»“æœ
        logger.info(f"[SEARCH API] Search completed with {search_result.get('success', False)} status")
        return search_result
        
    except Exception as e:
        logger.error(f"[SEARCH API] Content search failed: {e}")
        return {
            "success": False,
            "error": f"æœç´¢å¤±è´¥: {str(e)}",
            "results": None
        }

@app.post("/documents/{document_id}/search/content")  
def search_document_content_by_id(
    document_id: int,
    request: Dict[str, Any] = Body(...),
    search_mgr: SearchManager = Depends(get_search_manager)
):
    """
    åœ¨æŒ‡å®šæ–‡æ¡£å†…è¿›è¡Œå‘é‡å†…å®¹æ£€ç´¢
    
    å‚æ•°:
    - document_id: æ–‡æ¡£ID
    - query: è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ–‡æœ¬
    - top_k: è¿”å›çš„æœ€å¤§ç»“æœæ•° (å¯é€‰ï¼Œé»˜è®¤10)
    - distance_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ (å¯é€‰)
    
    è¿”å›:
    - success: æ˜¯å¦æˆåŠŸ
    - results: æ ¼å¼åŒ–çš„æ£€ç´¢ç»“æœ
    - query_info: æŸ¥è¯¢å…ƒä¿¡æ¯
    """
    try:
        # æå–å‚æ•°
        query = request.get("query", "").strip()
        top_k = request.get("top_k", 10)
        distance_threshold = request.get("distance_threshold")
        
        logger.info(f"[SEARCH API] Document {document_id} content search: '{query[:50]}...'")
        
        # åŸºç¡€éªŒè¯
        if not query:
            return {
                "success": False,
                "error": "æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º",
                "results": None
            }
        
        # æ‰§è¡Œæœç´¢ï¼ˆé™åˆ¶åœ¨æŒ‡å®šæ–‡æ¡£ï¼‰
        search_result = search_mgr.search_documents(
            query=query,
            top_k=top_k,
            document_ids=[document_id],  # é™åˆ¶åœ¨æŒ‡å®šæ–‡æ¡£
            distance_threshold=distance_threshold
        )
        
        # æ·»åŠ æ–‡æ¡£IDä¿¡æ¯åˆ°ç»“æœä¸­
        if search_result.get("success", False):
            if "query_info" not in search_result:
                search_result["query_info"] = {}
            search_result["query_info"]["target_document_id"] = document_id
        
        logger.info(f"[SEARCH API] Document {document_id} search completed")
        return search_result
        
    except Exception as e:
        logger.error(f"[SEARCH API] Document {document_id} content search failed: {e}")
        return {
            "success": False,
            "error": f"æ–‡æ¡£å†…æœç´¢å¤±è´¥: {str(e)}",
            "results": None
        }

@app.post("/file-screening/batch")
def add_batch_file_screening_results(
    request: Dict[str, Any] = Body(...), 
    screening_mgr: ScreeningManager = Depends(get_screening_manager),
    task_mgr: TaskManager = Depends(get_task_manager)
):
    """æ‰¹é‡æ·»åŠ æ–‡ä»¶ç²—ç­›ç»“æœ
    
    å‚æ•°:
    - data_list: æ–‡ä»¶ç²—ç­›ç»“æœåˆ—è¡¨
    """
    try:
        # ä»è¯·æ±‚ä½“ä¸­æå–æ•°æ®å’Œå‚æ•°
        logger.info(f"æ¥æ”¶åˆ°æ‰¹é‡æ–‡ä»¶ç²—ç­›ç»“æœï¼Œè¯·æ±‚ä½“é”®å: {list(request.keys())}")
        
        # é€‚é…Rustå®¢æˆ·ç«¯å‘é€çš„æ ¼å¼: {data_list: [...], auto_create_tasks: true}
        if "data_list" in request:
            data_list = request.get("data_list", [])
        elif isinstance(request, dict):
            data_list = request.get("files", [])
        else:
            # å‡è®¾è¯·æ±‚ä½“æœ¬èº«å°±æ˜¯åˆ—è¡¨
            data_list = request
            
        if not data_list:
            return {"success": True, "processed_count": 0, "failed_count": 0, "message": "æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶"}

        # é¢„å¤„ç†æ¯ä¸ªæ–‡ä»¶è®°å½•ä¸­çš„æ—¶é—´æˆ³ï¼Œè½¬æ¢ä¸ºPython datetimeå¯¹è±¡
        for data in data_list:
            # å¤„ç†Unixæ—¶é—´æˆ³çš„è½¬æ¢ (ä»Rustå‘é€çš„ç§’æ•°è½¬æ¢ä¸ºPython datetime)
            if "created_time" in data and isinstance(data["created_time"], (int, float)):
                data["created_time"] = datetime.fromtimestamp(data["created_time"])
                
            if "modified_time" in data and isinstance(data["modified_time"], (int, float)):
                data["modified_time"] = datetime.fromtimestamp(data["modified_time"])
                
            if "accessed_time" in data and isinstance(data["accessed_time"], (int, float)):
                data["accessed_time"] = datetime.fromtimestamp(data["accessed_time"])
        
        # å¤„ç†å­—ç¬¦ä¸²æ ¼å¼çš„æ—¶é—´å­—æ®µï¼ˆå¤„ç†ä¹‹å‰å·²ç»å…ˆå¤„ç†äº†æ•´æ•°æ—¶é—´æˆ³ï¼‰
        for data in data_list:
            for time_field in ["created_time", "modified_time", "accessed_time"]:
                # åªå¤„ç†ä»ç„¶æ˜¯å­—ç¬¦ä¸²æ ¼å¼çš„æ—¶é—´å­—æ®µï¼ˆæ•´æ•°æ—¶é—´æˆ³å·²åœ¨å‰ä¸€æ­¥è½¬æ¢ï¼‰
                if time_field in data and isinstance(data[time_field], str):
                    try:
                        data[time_field] = datetime.fromisoformat(data[time_field].replace("Z", "+00:00"))
                    except Exception as e:
                        logger.warning(f"è½¬æ¢å­—ç¬¦ä¸²æ—¶é—´å­—æ®µ {time_field} å¤±è´¥: {str(e)}")
                        # å¦‚æœæ˜¯ä¿®æ”¹æ—¶é—´å­—æ®µè½¬æ¢å¤±è´¥ï¼Œè®¾ç½®ä¸ºå½“å‰æ—¶é—´
                        if time_field == "modified_time":
                            data[time_field] = datetime.now()
                
                # ç¡®ä¿æ¯ä¸ªæ—¶é—´å­—æ®µéƒ½æœ‰å€¼ï¼Œå¯¹äºå¿…å¡«å­—æ®µ
                if time_field == "modified_time" and (time_field not in data or data[time_field] is None):
                    logger.warning(f"ç¼ºå°‘å¿…å¡«æ—¶é—´å­—æ®µ {time_field}ï¼Œä½¿ç”¨å½“å‰æ—¶é—´")
                    data[time_field] = datetime.now()
                            
            # Ensure 'extra_metadata' is used, but allow 'metadata' for backward compatibility from client
            if "metadata" in data and "extra_metadata" not in data:
                data["extra_metadata"] = data.pop("metadata")

        # 1. å…ˆåˆ›å»ºä»»åŠ¡ï¼Œè·å– task_id
        task_name = f"æ‰¹é‡å¤„ç†æ–‡ä»¶: {len(data_list)} ä¸ªæ–‡ä»¶"
        task: Task = task_mgr.add_task(
            task_name=task_name,
            task_type=TaskType.TAGGING,
            priority=TaskPriority.MEDIUM,
            extra_data={"file_count": len(data_list)}
        )
        logger.info(f"å·²åˆ›å»ºæ ‡è®°ä»»åŠ¡ ID: {task.id}ï¼Œå‡†å¤‡å¤„ç† {len(data_list)} ä¸ªæ–‡ä»¶")

        # 2. æ‰¹é‡æ·»åŠ ç²—ç­›ç»“æœï¼Œå¹¶å…³è” task_id
        result = screening_mgr.add_batch_screening_results(data_list, task_id=task.id)
        
        # 3. è¿”å›ç»“æœ
        if result["success"] > 0:
            message = f"å·²ä¸º {result['success']} ä¸ªæ–‡ä»¶åˆ›å»ºå¤„ç†ä»»åŠ¡ï¼Œå¤±è´¥ {result['failed']} ä¸ª"
        else:
            message = f"æœªèƒ½å¤„ç†ä»»ä½•æ–‡ä»¶ï¼Œå¤±è´¥ {result['failed']} ä¸ª"

        return {
            "success": result["success"] > 0,
            "processed_count": result["success"],
            "failed_count": result["failed"],
            "errors": result.get("errors"),
            "task_id": task.id,
            "message": message
        }
        
    except Exception as e:
        logger.error(f"æ‰¹é‡å¤„ç†æ–‡ä»¶ç²—ç­›ç»“æœå¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"æ‰¹é‡å¤„ç†å¤±è´¥: {str(e)}"
        }

@app.get("/file-screening/results")
def get_file_screening_results(
    limit: int = 1000,
    category_id: int = None,
    time_range: str = None,
    screening_mgr: ScreeningManager = Depends(get_screening_manager)
):
    """è·å–æ–‡ä»¶ç²—ç­›ç»“æœåˆ—è¡¨ï¼Œæ”¯æŒæŒ‰åˆ†ç±»å’Œæ—¶é—´èŒƒå›´ç­›é€‰
    
    å‚æ•°:
    - limit: æœ€å¤§è¿”å›ç»“æœæ•°
    - category_id: å¯é€‰ï¼ŒæŒ‰æ–‡ä»¶åˆ†ç±»IDè¿‡æ»¤
    - time_range: å¯é€‰ï¼ŒæŒ‰æ—¶é—´èŒƒå›´è¿‡æ»¤ ("today", "last7days", "last30days")
    """
    try:
        from datetime import datetime, timedelta
        
        # åŸºç¡€æŸ¥è¯¢
        results = screening_mgr.get_all_results(limit)
        
        # å¦‚æœç»“æœä¸ºç©ºï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨ï¼Œé˜²æ­¢åç»­å¤„ç†å‡ºé”™
        if not results:
            return {
                "success": True,
                "count": 0,
                "data": []
            }
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–å­—å…¸åˆ—è¡¨
        results_dict = [result.model_dump() for result in results]
        
        # è¿‡æ»¤é€»è¾‘
        filtered_results = results_dict
        
        # æŒ‰åˆ†ç±»è¿‡æ»¤
        if (category_id is not None):
            filtered_results = [r for r in filtered_results if r.get('category_id') == category_id]
        
        # æŒ‰æ—¶é—´èŒƒå›´è¿‡æ»¤
        if time_range:
            now = datetime.now()
            # Ensure modified_time is a string before parsing
            date_format = "%Y-%m-%d %H:%M:%S" # Define the correct format

            if time_range == "today":
                today = datetime(now.year, now.month, now.day)
                filtered_results = [r for r in filtered_results if r.get('modified_time') and datetime.strptime(r.get('modified_time'), date_format) >= today]
            elif time_range == "last7days":
                week_ago = now - timedelta(days=7)
                filtered_results = [r for r in filtered_results if r.get('modified_time') and datetime.strptime(r.get('modified_time'), date_format) >= week_ago]
            elif time_range == "last30days":
                month_ago = now - timedelta(days=30)
                filtered_results = [r for r in filtered_results if r.get('modified_time') and datetime.strptime(r.get('modified_time'), date_format) >= month_ago]
        
        return {
            "success": True,
            "count": len(filtered_results),
            "data": filtered_results
        }
        
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶ç²—ç­›ç»“æœåˆ—è¡¨å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            "success": False,
            "message": f"è·å–å¤±è´¥: {str(e)}"
        }
@app.get("/file-screening/results/search")
def search_files_by_path_substring(
    substring: str,
    limit: int = 100,
    screening_mgr: ScreeningManager = Depends(get_screening_manager)
):
    """æ ¹æ®è·¯å¾„å­å­—ç¬¦ä¸²æœç´¢æ–‡ä»¶ç²—ç­›ç»“æœ
    
    å‚æ•°:
    - substring: è¦æœç´¢çš„è·¯å¾„å­å­—ç¬¦ä¸²
    - limit: æœ€å¤§è¿”å›ç»“æœæ•°
    """
    try:
        # ä½¿ç”¨ ScreeningManager çš„æœç´¢æ–¹æ³•ï¼Œç°åœ¨è¿”å›å­—å…¸åˆ—è¡¨
        results_dict = screening_mgr.search_files_by_path_substring(substring, limit)
        
        return {
            "success": True,
            "count": len(results_dict),
            "data": results_dict
        }
        
    except Exception as e:
        logger.error(f"æ ¹æ®è·¯å¾„å­å­—ç¬¦ä¸²æœç´¢æ–‡ä»¶ç²—ç­›ç»“æœå¤±è´¥: {str(e)}")
        return {
            "success": False,
            "message": f"æœç´¢å¤±è´¥: {str(e)}"
        }

@app.get("/")
def read_root():
    # ç°åœ¨å¯ä»¥åœ¨ä»»ä½•è·¯ç”±ä¸­ä½¿ç”¨ app.state.db_path
    return {"Hello": "World", "db_path": app.state.db_path}

# æ·»åŠ å¥åº·æ£€æŸ¥ç«¯ç‚¹
@app.get("/health")
def health_check():
    """APIå¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼Œç”¨äºéªŒè¯APIæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ"""
    return {
        "status": "ok", 
        "timestamp": datetime.now().isoformat(),
        "cache_stats": {
            "config": config_cache.get_stats(),
            "folders": folder_hierarchy_cache.get_stats()
        }
    }

# æ·»åŠ æ–‡ä»¶å¤¹ç®¡ç†ç›¸å…³API
@app.get("/directories")
def get_directories(
    myfiles_mgr: MyFilesManager = Depends(get_myfiles_manager)
):
    try:
        # æ ¹æ®ç³»ç»Ÿå¹³å°è®¾ç½® full_disk_access çŠ¶æ€
        # åªåœ¨ macOS ä¸Šæ‰æœ‰æ„ä¹‰
        fda_status = False
        if sys.platform == "darwin":  # macOS
            # åœ¨ macOS ä¸Šï¼Œæ£€æŸ¥åº”ç”¨æ˜¯å¦æœ‰å®Œå…¨ç£ç›˜è®¿é—®æƒé™
            access_status = myfiles_mgr.check_full_disk_access_status()
            fda_status = access_status.get("has_full_disk_access", False)
            logger.info(f"[API DEBUG] Full disk access status: {fda_status}, details: {access_status}")

        # ä½¿ç”¨ select è¯­å¥ä»æ•°æ®åº“è·å–æ‰€æœ‰ç›‘æ§çš„ç›®å½•
        stmt = select(MyFiles)
        directories_from_db = myfiles_mgr.session.exec(stmt).all()
        
        processed_dirs = []
        for d in directories_from_db:
            dir_dict = {
                "id": getattr(d, 'id', None),
                "path": getattr(d, 'path', None),
                "alias": getattr(d, 'alias', None),
                "is_blacklist": getattr(d, 'is_blacklist', False),
                "created_at": d.created_at.isoformat() if getattr(d, 'created_at', None) else None,
                "updated_at": d.updated_at.isoformat() if getattr(d, 'updated_at', None) else None,
            }
            processed_dirs.append(dir_dict)
        
        logger.info(f"[API DEBUG] /directories returning: fda_status={fda_status}, num_dirs={len(processed_dirs)}")
        return {"status": "success", "full_disk_access": fda_status, "data": processed_dirs}
    except Exception as e:
        logger.error(f"Error in get_directories: {e}", exc_info=True)
        return {"status": "error", "full_disk_access": False, "data": [], "message": str(e)}

@app.post("/directories")
def add_directory(
    data: Dict[str, Any] = Body(...),
    myfiles_mgr: MyFilesManager = Depends(get_myfiles_manager)
):
    """æ·»åŠ æ–°æ–‡ä»¶å¤¹"""
    try:
        path = data.get("path", "")
        alias = data.get("alias", "")
        is_blacklist = data.get("is_blacklist", False)
        
        if not path: # ä¿®æ­£ï¼šä¹‹å‰æ˜¯ if nameæˆ–not path:
            return {"status": "error", "message": "è·¯å¾„ä¸èƒ½ä¸ºç©º"}
        
        success, message_or_dir = myfiles_mgr.add_directory(path, alias, is_blacklist)
        
        if success:
            # æ¸…é™¤ç›¸å…³ç¼“å­˜
            invalidate_config_caches()
            logger.info(f"[CACHE] å·²æ¸…é™¤ç¼“å­˜ï¼Œå› ä¸ºæ·»åŠ äº†æ–°æ–‡ä»¶å¤¹: {path}")
            
            # æ£€æŸ¥è¿”å›å€¼æ˜¯å¦æ˜¯å­—ç¬¦ä¸²æˆ–MyFileså¯¹è±¡
            if isinstance(message_or_dir, str):
                return {"status": "success", "message": message_or_dir}
            else:                
                # å¦‚æœä¸æ˜¯é»‘åå•ï¼Œå‰ç«¯ä¼šç«‹å³å¯åŠ¨Rustç›‘æ§
                if not is_blacklist:
                    # æ·»åŠ Rustç›‘æ§çš„è§¦å‘ä¿¡å·
                    # æ­¤å¤„æ—¥å¿—è®°å½•å³å¯ï¼Œå®é™…ç›‘æ§ç”±å‰ç«¯Taurié€šè¿‡fetch_and_store_all_configè·å–æœ€æ–°é…ç½®
                    logger.info(f"[MONITOR] æ–°æ–‡ä»¶å¤¹å·²æ·»åŠ ï¼Œéœ€è¦ç«‹å³å¯åŠ¨ç›‘æ§: {path}")
                    
                return {"status": "success", "data": message_or_dir.model_dump(), "message": "æ–‡ä»¶å¤¹æ·»åŠ æˆåŠŸ"}
        else:
            return {"status": "error", "message": message_or_dir}
    except Exception as e:
        logger.error(f"æ·»åŠ æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")
        return {"status": "error", "message": f"æ·»åŠ æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}"}

@app.put("/directories/{directory_id}")
def update_directory(
    directory_id: int,
    data: Dict[str, Any] = Body(...),
    myfiles_mgr: MyFilesManager = Depends(get_myfiles_manager)
):
    """æ›´æ–°æ–‡ä»¶å¤¹çš„ä¿¡æ¯"""
    try:
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´æ–°æ–‡ä»¶å¤¹å…¶ä»–ä¿¡æ¯çš„é€»è¾‘
        return {"status": "success", "message": "æ–‡ä»¶å¤¹ä¿¡æ¯æ›´æ–°æˆåŠŸ"}
    except Exception as e:
        logger.error(f"æ›´æ–°æ–‡ä»¶å¤¹ä¿¡æ¯å¤±è´¥: {directory_id}, {str(e)}")
        return {"status": "error", "message": f"æ›´æ–°æ–‡ä»¶å¤¹ä¿¡æ¯å¤±è´¥: {str(e)}"}

@app.put("/directories/{directory_id}/blacklist")
def toggle_directory_blacklist(
    directory_id: int,
    data: Dict[str, Any] = Body(...), # åŒ…å« is_blacklist: bool
    myfiles_mgr: MyFilesManager = Depends(get_myfiles_manager)
):
    """åˆ‡æ¢æ–‡ä»¶å¤¹çš„é»‘åå•çŠ¶æ€"""
    try:
        is_blacklist = data.get("is_blacklist")
        if not isinstance(is_blacklist, bool):
            return {"status": "error", "message": "æ— æ•ˆçš„é»‘åå•çŠ¶æ€å‚æ•°"}

        success, message_or_dir = myfiles_mgr.toggle_blacklist(directory_id, is_blacklist)
        if success:
            # æ¸…é™¤ç›¸å…³ç¼“å­˜
            invalidate_config_caches()
            logger.info(f"[CACHE] å·²æ¸…é™¤ç¼“å­˜ï¼Œå› ä¸ºåˆ‡æ¢äº†æ–‡ä»¶å¤¹ {directory_id} çš„é»‘åå•çŠ¶æ€ä¸º {is_blacklist}")
            return {"status": "success", "data": message_or_dir.model_dump(), "message": "é»‘åå•çŠ¶æ€æ›´æ–°æˆåŠŸ"}
        else:
            return {"status": "error", "message": message_or_dir}
    except Exception as e:
        logger.error(f"åˆ‡æ¢æ–‡ä»¶å¤¹é»‘åå•çŠ¶æ€å¤±è´¥: {directory_id}, {str(e)}")
        return {"status": "error", "message": f"åˆ‡æ¢æ–‡ä»¶å¤¹é»‘åå•çŠ¶æ€å¤±è´¥: {str(e)}"}

@app.delete("/directories/{directory_id}")
def delete_directory(
    directory_id: int,
    myfiles_mgr: MyFilesManager = Depends(get_myfiles_manager)
):
    """åˆ é™¤æ–‡ä»¶å¤¹"""
    try:
        success, message = myfiles_mgr.remove_directory(directory_id)
        if success:
            # æ¸…é™¤ç›¸å…³ç¼“å­˜
            invalidate_config_caches()
            logger.info(f"[CACHE] å·²æ¸…é™¤ç¼“å­˜ï¼Œå› ä¸ºåˆ é™¤äº†æ–‡ä»¶å¤¹ {directory_id}")
            return {"status": "success", "message": "æ–‡ä»¶å¤¹åˆ é™¤æˆåŠŸ"}
        else:
            return {"status": "error", "message": message}
    except Exception as e:
        logger.error(f"åˆ é™¤æ–‡ä»¶å¤¹å¤±è´¥: {directory_id}, {str(e)}")
        return {"status": "error", "message": f"åˆ é™¤æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}"}

@app.put("/directories/{directory_id}/alias")
def update_directory_alias(
    directory_id: int,
    data: Dict[str, Any] = Body(...), # åŒ…å« alias: str
    myfiles_mgr: MyFilesManager = Depends(get_myfiles_manager)
):
    """æ›´æ–°æ–‡ä»¶å¤¹çš„åˆ«å"""
    try:
        alias = data.get("alias")
        if alias is None: # å…è®¸ç©ºå­—ç¬¦ä¸²ä½œä¸ºåˆ«åï¼Œä½†ä¸å…è®¸None
            return {"status": "error", "message": "åˆ«åä¸èƒ½ä¸ºç©º"}

        success, message_or_dir = myfiles_mgr.update_alias(directory_id, alias)
        if success:
            return {"status": "success", "data": message_or_dir.model_dump(), "message": "åˆ«åæ›´æ–°æˆåŠŸ"}
        else:
            return {"status": "error", "message": message_or_dir}
    except Exception as e:
        logger.error(f"æ›´æ–°æ–‡ä»¶å¤¹åˆ«åå¤±è´¥: {directory_id}, {str(e)}")
        return {"status": "error", "message": f"æ›´æ–°æ–‡ä»¶å¤¹åˆ«åå¤±è´¥: {str(e)}"}

# åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ ä»¥ä¸‹ç«¯ç‚¹ï¼Œç”¨äºåˆå§‹åŒ–é»˜è®¤æ–‡ä»¶å¤¹å’Œè·å–æƒé™æç¤º
@app.get("/directories/default")
def initialize_default_directories_endpoint(myfiles_mgr: MyFilesManager = Depends(get_myfiles_manager)):
    """åˆå§‹åŒ–é»˜è®¤ç³»ç»Ÿæ–‡ä»¶å¤¹"""
    try:
        count = myfiles_mgr.initialize_default_directories()
        return {"status": "success", "message": f"æˆåŠŸåˆå§‹åŒ–/æ£€æŸ¥äº† {count} ä¸ªé»˜è®¤æ–‡ä»¶å¤¹ã€‚"}
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–é»˜è®¤æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")
        return {"status": "error", "message": f"åˆå§‹åŒ–é»˜è®¤æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}"}

@app.get("/directories/default-list")
def get_default_directories_list(myfiles_mgr: MyFilesManager = Depends(get_myfiles_manager)):
    """è·å–é»˜è®¤ç³»ç»Ÿæ–‡ä»¶å¤¹åˆ—è¡¨ï¼ˆä¸è¿›è¡Œæ•°æ®åº“æ“ä½œï¼‰"""
    try:
        directories = myfiles_mgr.get_default_directories()
        return {"status": "success", "data": directories}
    except Exception as e:
        logger.error(f"è·å–é»˜è®¤æ–‡ä»¶å¤¹åˆ—è¡¨å¤±è´¥: {str(e)}")
        return {"status": "error", "message": f"è·å–é»˜è®¤æ–‡ä»¶å¤¹åˆ—è¡¨å¤±è´¥: {str(e)}"}

@app.get("/macos-permissions-hint")
def get_macos_permissions_hint_endpoint(myfiles_mgr: MyFilesManager = Depends(get_myfiles_manager)):
    """è·å– macOS æƒé™æç¤º"""
    try:
        hint = myfiles_mgr.get_macOS_permissions_hint()
        return {"status": "success", "data": hint}
    except Exception as e:
        logger.error(f"è·å– macOS æƒé™æç¤ºå¤±è´¥: {str(e)}")
        return {"status": "error", "message": f"è·å– macOS æƒé™æç¤ºå¤±è´¥: {str(e)}"}

@app.post("/directories/{directory_id}/request-access")
def request_directory_access(
    directory_id: int,
    myfiles_mgr: MyFilesManager = Depends(get_myfiles_manager)
):
    """å°è¯•è¯»å–ç›®å½•ä»¥è§¦å‘ç³»ç»Ÿæˆæƒå¯¹è¯æ¡†"""
    try:
        success, message = myfiles_mgr.test_directory_access(directory_id)
        if success:
            return {"status": "success", "message": message}
        else:
            return {"status": "error", "message": message}
    except Exception as e:
        logger.error(f"è¯·æ±‚ç›®å½•è®¿é—®å¤±è´¥: {directory_id}, {str(e)}")
        return {"status": "error", "message": f"è¯·æ±‚ç›®å½•è®¿é—®å¤±è´¥: {str(e)}"}

@app.get("/directories/{directory_id}/access-status")
def check_directory_access_status(
    directory_id: int,
    myfiles_mgr: MyFilesManager = Depends(get_myfiles_manager)
):
    """æ£€æŸ¥ç›®å½•çš„è®¿é—®æƒé™çŠ¶æ€"""
    try:
        success, result = myfiles_mgr.check_directory_access_status(directory_id)
        if success:
            return {"status": "success", "data": result}
        else:
            return {"status": "error", "message": result.get("message", "æ£€æŸ¥è®¿é—®çŠ¶æ€å¤±è´¥")}
    except Exception as e:
        logger.error(f"æ£€æŸ¥ç›®å½•è®¿é—®çŠ¶æ€å¤±è´¥: {directory_id}, {str(e)}")
        return {"status": "error", "message": f"æ£€æŸ¥ç›®å½•è®¿é—®çŠ¶æ€å¤±è´¥: {str(e)}"}

# ========== Bundleæ‰©å±•åç®¡ç†ç«¯ç‚¹ ==========
@app.get("/bundle-extensions")
def get_bundle_extensions(
    active_only: bool = True,
    myfiles_mgr: MyFilesManager = Depends(get_myfiles_manager)
):
    """è·å–Bundleæ‰©å±•ååˆ—è¡¨"""
    try:
        extensions = myfiles_mgr.get_bundle_extensions(active_only=active_only)
        extensions_data = []
        for ext in extensions:
            extensions_data.append({
                "id": ext.id,
                "extension": ext.extension,
                "description": ext.description,
                "is_active": ext.is_active,
                "created_at": ext.created_at.isoformat() if ext.created_at else None,
                "updated_at": ext.updated_at.isoformat() if ext.updated_at else None,
            })
        
        return {
            "status": "success",
            "data": extensions_data,
            "count": len(extensions_data),
            "message": f"æˆåŠŸè·å– {len(extensions_data)} ä¸ªBundleæ‰©å±•å"
        }
    except Exception as e:
        logger.error(f"è·å–Bundleæ‰©å±•åå¤±è´¥: {str(e)}")
        return {"status": "error", "message": f"è·å–Bundleæ‰©å±•åå¤±è´¥: {str(e)}"}

@app.post("/bundle-extensions")
def add_bundle_extension(
    data: Dict[str, Any] = Body(...),
    myfiles_mgr: MyFilesManager = Depends(get_myfiles_manager)
):
    """æ·»åŠ æ–°çš„Bundleæ‰©å±•å"""
    try:
        extension = data.get("extension", "").strip()
        description = data.get("description", "").strip()
        
        if not extension:
            return {"status": "error", "message": "æ‰©å±•åä¸èƒ½ä¸ºç©º"}
        
        success, result = myfiles_mgr.add_bundle_extension(extension, description)
        
        if success:
            return {
                "status": "success",
                "data": {
                    "id": result.id,
                    "extension": result.extension,
                    "description": result.description,
                    "is_active": result.is_active,
                    "created_at": result.created_at.isoformat() if result.created_at else None,
                    "updated_at": result.updated_at.isoformat() if result.updated_at else None,
                },
                "message": f"æˆåŠŸæ·»åŠ Bundleæ‰©å±•å: {result.extension}"
            }
        else:
            return {"status": "error", "message": result}
            
    except Exception as e:
        logger.error(f"æ·»åŠ Bundleæ‰©å±•åå¤±è´¥: {str(e)}")
        return {"status": "error", "message": f"æ·»åŠ Bundleæ‰©å±•åå¤±è´¥: {str(e)}"}

@app.delete("/bundle-extensions/{ext_id}")
def remove_bundle_extension(
    ext_id: int,
    myfiles_mgr: MyFilesManager = Depends(get_myfiles_manager)
):
    """åˆ é™¤Bundleæ‰©å±•åï¼ˆè®¾ä¸ºä¸æ´»è·ƒï¼‰"""
    try:
        success, message = myfiles_mgr.remove_bundle_extension(ext_id)
        
        if success:
            return {"status": "success", "message": message}
        else:
            return {"status": "error", "message": message}
            
    except Exception as e:
        logger.error(f"åˆ é™¤Bundleæ‰©å±•åå¤±è´¥: {str(e)}")
        return {"status": "error", "message": f"åˆ é™¤Bundleæ‰©å±•åå¤±è´¥: {str(e)}"}

@app.get("/bundle-extensions/for-rust")
def get_bundle_extensions_for_rust(
    myfiles_mgr: MyFilesManager = Depends(get_myfiles_manager)
):
    """è·å–ç”¨äºRustç«¯çš„Bundleæ‰©å±•ååˆ—è¡¨"""
    try:
        # æ·»åŠ æ—¥å¿—ï¼Œæç¤ºåº”è¯¥ä½¿ç”¨æ–°çš„æ¥å£
        logger.warning("[API_DEPRECATED] /bundle-extensions/for-rust æ¥å£å·²è¢«å¼ƒç”¨ï¼Œå»ºè®®ä½¿ç”¨ /config/all æ¥å£è·å– bundle_extensions å­—æ®µ")
        
        extensions = myfiles_mgr.get_bundle_extensions_for_rust()
        return {
            "status": "success",
            "data": extensions,
            "count": len(extensions),
            "message": f"æˆåŠŸè·å– {len(extensions)} ä¸ªRustç«¯Bundleæ‰©å±•å"
        }
    except Exception as e:
        logger.error(f"è·å–Rustç«¯Bundleæ‰©å±•åå¤±è´¥: {str(e)}")
        return {"status": "error", "data": [], "message": f"è·å–å¤±è´¥: {str(e)}"}

# ========== å±‚çº§æ–‡ä»¶å¤¹ç®¡ç†ç«¯ç‚¹ ==========
@app.post("/folders/blacklist/{parent_id}")
def add_blacklist_folder_under_parent(
    parent_id: int,
    data: Dict[str, Any] = Body(...),
    myfiles_mgr: MyFilesManager = Depends(get_myfiles_manager),
    screening_mgr: ScreeningManager = Depends(get_screening_manager)
):
    """åœ¨æŒ‡å®šçš„ç™½åå•çˆ¶æ–‡ä»¶å¤¹ä¸‹æ·»åŠ é»‘åå•å­æ–‡ä»¶å¤¹"""
    try:
        folder_path = data.get("path", "").strip()
        folder_alias = data.get("alias", "").strip() or None
        
        if not folder_path:
            return {"status": "error", "message": "æ–‡ä»¶å¤¹è·¯å¾„ä¸èƒ½ä¸ºç©º"}
        
        success, result = myfiles_mgr.add_blacklist_folder(parent_id, folder_path, folder_alias)
        
        if success:
            # æ¸…é™¤ç›¸å…³ç¼“å­˜
            invalidate_config_caches()
            logger.info(f"[CACHE] å·²æ¸…é™¤ç¼“å­˜ï¼Œå› ä¸ºæ·»åŠ äº†é»‘åå•æ–‡ä»¶å¤¹: {folder_path}")
            
            # å½“æ–‡ä»¶å¤¹å˜ä¸ºé»‘åå•æ—¶ï¼Œæ¸…ç†ç›¸å…³çš„ç²—ç­›ç»“æœæ•°æ®
            deleted_count = screening_mgr.delete_screening_results_by_folder(folder_path)
            
            return {
                "status": "success",
                "data": {
                    "id": result.id,
                    "path": result.path,
                    "alias": result.alias,
                    "is_blacklist": result.is_blacklist,
                    "parent_id": result.parent_id,
                    "created_at": result.created_at.isoformat() if result.created_at else None,
                    "updated_at": result.updated_at.isoformat() if result.updated_at else None,
                },
                "message": f"æˆåŠŸæ·»åŠ é»‘åå•æ–‡ä»¶å¤¹: {result.path}ï¼Œæ¸…ç†äº† {deleted_count} æ¡ç›¸å…³ç²—ç­›ç»“æœ"
            }
        else:
            return {"status": "error", "message": result}
            
    except Exception as e:
        logger.error(f"æ·»åŠ é»‘åå•æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}")
        return {"status": "error", "message": f"æ·»åŠ é»‘åå•æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}"}

@app.get("/folders/hierarchy")
async def get_folder_hierarchy(
    myfiles_mgr: MyFilesManager = Depends(get_myfiles_manager)
):
    """è·å–æ–‡ä»¶å¤¹å±‚çº§å…³ç³»ï¼ˆç™½åå•+å…¶ä¸‹çš„é»‘åå•ï¼‰"""
    try:
        # ä½¿ç”¨å¼‚æ­¥è¶…æ—¶æ§åˆ¶
        try:
            return await asyncio.wait_for(
                _get_folder_hierarchy_async(myfiles_mgr), 
                timeout=3.0  # è®¾ç½®3ç§’è¶…æ—¶
            )
        except asyncio.TimeoutError:
            logger.error("è·å–æ–‡ä»¶å¤¹å±‚çº§å…³ç³»è¶…æ—¶")
            # å°è¯•ä»ç¼“å­˜è·å–
            hit, cached_value = folder_hierarchy_cache.get("folder_hierarchy")
            if hit:
                logger.info("ä½¿ç”¨ç¼“å­˜æ•°æ®å“åº”è¶…æ—¶è¯·æ±‚")
                return cached_value
            # æ²¡æœ‰ç¼“å­˜ï¼Œè¿”å›é”™è¯¯
            return {"status": "error", "message": "è·å–æ–‡ä»¶å¤¹å±‚çº§å…³ç³»è¶…æ—¶"}
    except Exception as e:
        logger.error(f"è·å–æ–‡ä»¶å¤¹å±‚çº§å…³ç³»å¤±è´¥: {str(e)}")
        return {"status": "error", "message": f"è·å–æ–‡ä»¶å¤¹å±‚çº§å…³ç³»å¤±è´¥: {str(e)}"}
        
async def _get_folder_hierarchy_async(myfiles_mgr: MyFilesManager):
    """å¼‚æ­¥åŒ…è£…ç¼“å­˜å‡½æ•°ï¼Œç”¨äºè¶…æ—¶æ§åˆ¶"""
    return _get_folder_hierarchy_cached(myfiles_mgr)

@cached(folder_hierarchy_cache, "folder_hierarchy")
def _get_folder_hierarchy_cached(myfiles_mgr: MyFilesManager):
    """ç¼“å­˜ç‰ˆæœ¬çš„æ–‡ä»¶å¤¹å±‚çº§å…³ç³»è·å–å‡½æ•°"""
    start_time = time.time()
    hierarchy = myfiles_mgr.get_folder_hierarchy()
    elapsed = time.time() - start_time
    logger.info(f"[FOLDERS] è·å–æ–‡ä»¶å¤¹å±‚çº§å…³ç³»è€—æ—¶ {elapsed:.3f}s (ä»æ•°æ®åº“)")
    
    return {
        "status": "success",
        "data": hierarchy,
        "count": len(hierarchy),
        "message": f"æˆåŠŸè·å– {len(hierarchy)} ä¸ªçˆ¶æ–‡ä»¶å¤¹çš„å±‚çº§å…³ç³»"
    }

@app.post("/screening/clean-by-path")
def clean_screening_results_by_path(
    data: Dict[str, Any] = Body(...),
    screening_mgr: ScreeningManager = Depends(get_screening_manager)
):
    """æ‰‹åŠ¨æ¸…ç†æŒ‡å®šè·¯å¾„ä¸‹çš„ç²—ç­›ç»“æœï¼ˆç”¨äºæ·»åŠ é»‘åå•å­æ–‡ä»¶å¤¹æ—¶ï¼‰
    
    å‰ç«¯å¯ä»¥ä½¿ç”¨æ­¤ç«¯ç‚¹åœ¨ç”¨æˆ·åœ¨ç™½åå•ä¸‹æ·»åŠ é»‘åå•å­æ–‡ä»¶å¤¹åæ¸…ç†æ•°æ®ï¼Œ
    ç›¸å½“äºåœ¨é›†åˆä¸­æ‰£å‡ºä¸€ä¸ªå­é›†æ¥åˆ æ‰ã€‚
    """
    try:
        folder_path = data.get("path", "").strip()
        
        if not folder_path:
            return {"status": "error", "message": "æ–‡ä»¶å¤¹è·¯å¾„ä¸èƒ½ä¸ºç©º"}
        
        # ä½¿ç”¨ delete_screening_results_by_path_prefix æ–¹æ³•ï¼Œç”¨äºåœ¨ç™½åå•ä¸‹æ·»åŠ é»‘åå•å­æ–‡ä»¶å¤¹
        deleted_count = screening_mgr.delete_screening_results_by_path_prefix(folder_path)
        return {
            "status": "success", 
            "deleted": deleted_count,
            "message": f"å·²æ¸…ç† {deleted_count} æ¡ä¸è·¯å¾„å‰ç¼€ '{folder_path}' ç›¸å…³çš„ç²—ç­›ç»“æœ"
        }
            
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨æ¸…ç†ç²—ç­›ç»“æœå¤±è´¥: {str(e)}")
        return {"status": "error", "message": f"æ¸…ç†å¤±è´¥: {str(e)}"}

def invalidate_config_caches():
    """ä½¿æ‰€æœ‰é…ç½®ç›¸å…³ç¼“å­˜å¤±æ•ˆ"""
    config_cache.clear()
    folder_hierarchy_cache.clear()
    logger.info("[CACHE] æ‰€æœ‰é…ç½®ç¼“å­˜å·²æ¸…é™¤")

@app.post("/screening/delete-by-path")
def delete_screening_by_path(
    data: Dict[str, Any] = Body(...),
    screening_mgr: ScreeningManager = Depends(get_screening_manager)
):
    """åˆ é™¤æŒ‡å®šè·¯å¾„çš„æ–‡ä»¶ç²—ç­›è®°å½•
    
    å½“å®¢æˆ·ç«¯æ£€æµ‹åˆ°æ–‡ä»¶åˆ é™¤äº‹ä»¶æ—¶ï¼Œè°ƒç”¨æ­¤APIç«¯ç‚¹åˆ é™¤å¯¹åº”çš„ç²—ç­›è®°å½•ã€‚
    
    è¯·æ±‚ä½“:
    - file_path: è¦åˆ é™¤çš„æ–‡ä»¶è·¯å¾„
    
    è¿”å›:
    - success: æ“ä½œæ˜¯å¦æˆåŠŸ
    - deleted_count: åˆ é™¤çš„è®°å½•æ•°é‡
    - message: æ“ä½œç»“æœæ¶ˆæ¯
    """
    try:
        file_path = data.get("file_path")
        
        if not file_path:
            logger.warning("åˆ é™¤ç²—ç­›è®°å½•è¯·æ±‚ä¸­æœªæä¾›æ–‡ä»¶è·¯å¾„")
            return {
                "success": False,
                "deleted_count": 0,
                "message": "æ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º"
            }
        
        # å¯¹äºå•ä¸ªæ–‡ä»¶åˆ é™¤ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿è·¯å¾„æ˜¯ç²¾ç¡®åŒ¹é…çš„
        # æˆ‘ä»¬å¯ä»¥ä½¿ç”¨delete_screening_results_by_path_prefixæ–¹æ³•ï¼Œä½†éœ€è¦ç¡®ä¿åªåˆ é™¤è¿™ä¸ªç¡®åˆ‡è·¯å¾„
        # é€šå¸¸æƒ…å†µä¸‹ï¼Œè¿™ä¸ªè·¯å¾„åº”è¯¥æ˜¯ä¸€ä¸ªæ–‡ä»¶è·¯å¾„ï¼Œä¸ä¼šåŒ¹é…åˆ°å…¶ä»–æ–‡ä»¶
        
        # æ ‡å‡†åŒ–è·¯å¾„
        normalized_path = os.path.normpath(file_path).replace("\\", "/")
        
        # æ‰§è¡Œåˆ é™¤æ“ä½œ
        deleted_count = screening_mgr.delete_screening_results_by_path_prefix(normalized_path)
        
        # è®°å½•æ“ä½œç»“æœ
        if deleted_count > 0:
            logger.info(f"æˆåŠŸåˆ é™¤æ–‡ä»¶ '{normalized_path}' çš„ç²—ç­›è®°å½•ï¼Œå…± {deleted_count} æ¡")
        else:
            logger.info(f"æœªæ‰¾åˆ°æ–‡ä»¶ '{normalized_path}' çš„ç²—ç­›è®°å½•ï¼Œæ— éœ€åˆ é™¤")
        
        return {
            "success": True,
            "deleted_count": deleted_count,
            "message": f"æˆåŠŸåˆ é™¤æ–‡ä»¶ '{normalized_path}' çš„ç²—ç­›è®°å½•ï¼Œå…± {deleted_count} æ¡"
        }
        
    except Exception as e:
        logger.error(f"åˆ é™¤æ–‡ä»¶ç²—ç­›è®°å½•å¤±è´¥: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            "success": False,
            "deleted_count": 0,
            "message": f"åˆ é™¤å¤±è´¥: {str(e)}"
        }

@app.post("/pin-file")
async def pin_file(
    data: Dict[str, Any] = Body(...),
    task_mgr: TaskManager = Depends(get_task_manager)
):
    """Pinæ–‡ä»¶å¹¶åˆ›å»ºå¤šæ¨¡æ€å‘é‡åŒ–ä»»åŠ¡
    
    ç”¨æˆ·pinæ–‡ä»¶æ—¶è°ƒç”¨æ­¤ç«¯ç‚¹ï¼Œç«‹å³åˆ›å»ºHIGHä¼˜å…ˆçº§çš„MULTIVECTORä»»åŠ¡
    
    è¯·æ±‚ä½“:
    - file_path: è¦pinçš„æ–‡ä»¶ç»å¯¹è·¯å¾„
    
    è¿”å›:
    - success: æ“ä½œæ˜¯å¦æˆåŠŸ
    - task_id: åˆ›å»ºçš„ä»»åŠ¡ID
    - message: æ“ä½œç»“æœæ¶ˆæ¯
    """
    try:
        file_path = data.get("file_path")
        
        if not file_path:
            logger.warning("Pinæ–‡ä»¶è¯·æ±‚ä¸­æœªæä¾›æ–‡ä»¶è·¯å¾„")
            return {
                "success": False,
                "task_id": None,
                "message": "æ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©º"
            }
        
        # éªŒè¯æ–‡ä»¶è·¯å¾„å’Œæƒé™
        if not os.path.exists(file_path):
            logger.warning(f"Pinæ–‡ä»¶å¤±è´¥ï¼Œæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return {
                "success": False,
                "task_id": None,
                "message": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
            }
        
        if not os.access(file_path, os.R_OK):
            logger.warning(f"Pinæ–‡ä»¶å¤±è´¥ï¼Œæ–‡ä»¶æ— è¯»å–æƒé™: {file_path}")
            return {
                "success": False,
                "task_id": None,
                "message": f"æ–‡ä»¶æ— è¯»å–æƒé™: {file_path}"
            }
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹æ˜¯å¦æ”¯æŒ
        supported_extensions = {'.pdf', '.docx', '.pptx', '.doc', '.ppt'}
        file_ext = Path(file_path).suffix.lower()
        if file_ext not in supported_extensions:
            logger.warning(f"Pinæ–‡ä»¶å¤±è´¥ï¼Œä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}")
            return {
                "success": False,
                "task_id": None,
                "message": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}ï¼Œæ”¯æŒçš„ç±»å‹: {supported_extensions}"
            }
        
        # åˆ›å»ºHIGHä¼˜å…ˆçº§MULTIVECTORä»»åŠ¡
        task = task_mgr.add_task(
            task_name=f"Pinæ–‡ä»¶å¤šæ¨¡æ€å‘é‡åŒ–: {Path(file_path).name}",
            task_type=TaskType.MULTIVECTOR,
            priority=TaskPriority.HIGH,
            extra_data={"file_path": file_path}
        )
        
        logger.info(f"æˆåŠŸåˆ›å»ºPinæ–‡ä»¶çš„å¤šæ¨¡æ€å‘é‡åŒ–ä»»åŠ¡: {file_path} (Task ID: {task.id})")
        
        return {
            "success": True,
            "task_id": task.id,
            "message": f"å·²åˆ›å»ºå¤šæ¨¡æ€å‘é‡åŒ–ä»»åŠ¡ï¼ŒTask ID: {task.id}"
        }
        
    except Exception as e:
        logger.error(f"Pinæ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
        return {
            "success": False,
            "task_id": None,
            "message": f"Pinæ–‡ä»¶å¤±è´¥: {str(e)}"
        }

@app.get("/test-bridge-stdout")
def test_bridge_stdout():
    """æµ‹è¯•æ¡¥æ¥äº‹ä»¶çš„stdoutè¾“å‡ºèƒ½åŠ›"""
    from test_bridge_stdout import test_bridge_stdout_main
    test_bridge_stdout_main()
    return {"status": "ok"}


if __name__ == "__main__":
    try:
        parser = argparse.ArgumentParser()
        parser.add_argument("--port", type=int, default=60315, help="APIæœåŠ¡ç›‘å¬ç«¯å£")
        parser.add_argument("--host", type=str, default="127.0.0.1", help="APIæœåŠ¡ç›‘å¬åœ°å€")
        parser.add_argument("--db-path", type=str, default="knowledge-focus.db", help="æ•°æ®åº“æ–‡ä»¶è·¯å¾„")
        parser.add_argument("--mode", type=str, default="dev", help="æ ‡è®°æ˜¯å¼€å‘ç¯å¢ƒè¿˜æ˜¯ç”Ÿäº§ç¯å¢ƒ")
        args = parser.parse_args()

        # æ£€æŸ¥æ•°æ®åº“è·¯å¾„æ˜¯å¦å­˜åœ¨
        db_dir = os.path.dirname(os.path.abspath(args.db_path))
        if db_dir and not os.path.exists(db_dir):
            os.makedirs(db_dir, exist_ok=True)

        if args.mode is not None and args.mode == "dev":
            setup_logging()
        else:
            logging_dir = os.path.join(db_dir, "logs")
            setup_logging(logging_dir)

        logger = logging.getLogger(__name__)
        logger.info("APIæœåŠ¡ç¨‹åºå¯åŠ¨")
        logger.info(f"å‘½ä»¤è¡Œå‚æ•°: port={args.port}, host={args.host}, db_path={args.db_path}, mode={args.mode}")

        # æ£€æŸ¥ç«¯å£æ˜¯å¦è¢«å ç”¨ï¼Œå¦‚æœè¢«å ç”¨åˆ™ç»ˆæ­¢å ç”¨è¿›ç¨‹
        try:
            logger.info(f"æ£€æŸ¥ç«¯å£ {args.port} æ˜¯å¦è¢«å ç”¨...")
            kill_process_on_port(args.port)
            time.sleep(2)  # ç­‰å¾…ç«¯å£é‡Šæ”¾
            logger.info(f"ç«¯å£ {args.port} å·²é‡Šæ”¾æˆ–æœ¬æ¥å°±æ²¡è¢«å ç”¨")
        except Exception as e:
            logger.error(f"é‡Šæ”¾ç«¯å£ {args.port} å¤±è´¥: {str(e)}", exc_info=True)
            # ç»§ç»­æ‰§è¡Œï¼Œç«¯å£å¯èƒ½æœ¬æ¥å°±æ²¡æœ‰è¢«å ç”¨
        
        # è®¾ç½®æ•°æ®åº“è·¯å¾„
        app.state.db_path = args.db_path
        logger.info(f"è®¾ç½®æ•°æ®åº“è·¯å¾„: {args.db_path}")
        
        # å¯åŠ¨æœåŠ¡å™¨
        logger.info(f"APIæœåŠ¡å¯åŠ¨åœ¨: http://{args.host}:{args.port}")
        
        # # å¼€å‘æ¨¡å¼å¯ç”¨çƒ­é‡è½½
        # if args.mode is not None and args.mode == "dev":
        #     logger.info("å¼€å‘æ¨¡å¼ï¼šå¯ç”¨çƒ­é‡è½½åŠŸèƒ½")
        #     uvicorn.run(
        #         "main:app", 
        #         host=args.host, 
        #         port=args.port, 
        #         log_level="info",
        #         reload=True,
        #         reload_dirs=["./"]
        #     )
        # else:
        #     logger.info("ç”Ÿäº§æ¨¡å¼ï¼šç¦ç”¨çƒ­é‡è½½åŠŸèƒ½")
        uvicorn.run(app, host=args.host, port=args.port, log_level="info")
    except Exception as e:
        logger.critical(f"APIæœåŠ¡å¯åŠ¨å¤±è´¥: {str(e)}", exc_info=True)

        # è¿”å›é€€å‡ºç 2ï¼Œè¡¨ç¤ºå‘ç”Ÿé”™è¯¯
        sys.exit(2)
